{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    return {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroDLCnn(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ZeroDLCnn, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # kernel_size:畳み込む行列のサイズ\n",
    "            # stride:スライドするときのサイズ\n",
    "            # padding:上下左右に追加する空白\n",
    "            # 1*28*28 のデータ\n",
    "            nn.Conv2d(1, 30, kernel_size=5, stride=1, padding=0), # 1*28*28 => 30*24*24\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #30*24*24 => 30*12*12\n",
    "        )\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(30*12*12, 30*12*12),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(30*12*12, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20\n",
    "history = {\n",
    "    'train_loss':[],\n",
    "    'test_loss':[],\n",
    "    'test_acc':[]\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = ZeroDLCnn().to(device)\n",
    "loaders = load_MNIST()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 2.3028454780578613\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 1.9706215858459473\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 1.8114593029022217\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 1.7521766424179077\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 1.7253754138946533\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 1.7517757415771484\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 1.6752127408981323\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 1.6969773769378662\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 1.755406141281128\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 1.6106853485107422\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 1.666837453842163\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 1.638575553894043\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 1.5992403030395508\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 1.607192039489746\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 1.6265372037887573\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 1.6292754411697388\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 1.626425862312317\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 1.576538324356079\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 1.580243706703186\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 1.5649311542510986\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 1.6298068761825562\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 1.5932068824768066\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 1.5741978883743286\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 1.5753958225250244\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 1.5681066513061523\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 1.5795327425003052\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 1.6025774478912354\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 1.5594843626022339\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 1.5123194456100464\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 1.5312793254852295\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 1.506226897239685\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 1.491325855255127\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 1.5004290342330933\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 1.4854037761688232\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 1.5057032108306885\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 1.4820448160171509\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 1.4861797094345093\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 1.492931604385376\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 1.525955319404602\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 1.4907877445220947\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 1.4947932958602905\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 1.4925487041473389\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 1.486519455909729\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 1.5043468475341797\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 1.5160093307495117\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 1.4775962829589844\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 1.468625783920288\n",
      "Test loss (avg): 0.011788892793655395, Accuracy: 0.9714\n",
      "Training log: 2 epoch (128 / 60000 train. data). Loss: 1.483596682548523\n",
      "Training log: 2 epoch (1408 / 60000 train. data). Loss: 1.477189540863037\n",
      "Training log: 2 epoch (2688 / 60000 train. data). Loss: 1.5057945251464844\n",
      "Training log: 2 epoch (3968 / 60000 train. data). Loss: 1.488991141319275\n",
      "Training log: 2 epoch (5248 / 60000 train. data). Loss: 1.4945333003997803\n",
      "Training log: 2 epoch (6528 / 60000 train. data). Loss: 1.4832834005355835\n",
      "Training log: 2 epoch (7808 / 60000 train. data). Loss: 1.4715335369110107\n",
      "Training log: 2 epoch (9088 / 60000 train. data). Loss: 1.4936720132827759\n",
      "Training log: 2 epoch (10368 / 60000 train. data). Loss: 1.4723880290985107\n",
      "Training log: 2 epoch (11648 / 60000 train. data). Loss: 1.4920448064804077\n",
      "Training log: 2 epoch (12928 / 60000 train. data). Loss: 1.5021440982818604\n",
      "Training log: 2 epoch (14208 / 60000 train. data). Loss: 1.512221336364746\n",
      "Training log: 2 epoch (15488 / 60000 train. data). Loss: 1.471622109413147\n",
      "Training log: 2 epoch (16768 / 60000 train. data). Loss: 1.4651484489440918\n",
      "Training log: 2 epoch (18048 / 60000 train. data). Loss: 1.4744006395339966\n",
      "Training log: 2 epoch (19328 / 60000 train. data). Loss: 1.4918766021728516\n",
      "Training log: 2 epoch (20608 / 60000 train. data). Loss: 1.491363763809204\n",
      "Training log: 2 epoch (21888 / 60000 train. data). Loss: 1.4712345600128174\n",
      "Training log: 2 epoch (23168 / 60000 train. data). Loss: 1.482019066810608\n",
      "Training log: 2 epoch (24448 / 60000 train. data). Loss: 1.4866786003112793\n",
      "Training log: 2 epoch (25728 / 60000 train. data). Loss: 1.465918779373169\n",
      "Training log: 2 epoch (27008 / 60000 train. data). Loss: 1.4815716743469238\n",
      "Training log: 2 epoch (28288 / 60000 train. data). Loss: 1.498511791229248\n",
      "Training log: 2 epoch (29568 / 60000 train. data). Loss: 1.48537015914917\n",
      "Training log: 2 epoch (30848 / 60000 train. data). Loss: 1.4691662788391113\n",
      "Training log: 2 epoch (32128 / 60000 train. data). Loss: 1.4801030158996582\n",
      "Training log: 2 epoch (33408 / 60000 train. data). Loss: 1.4716697931289673\n",
      "Training log: 2 epoch (34688 / 60000 train. data). Loss: 1.4859349727630615\n",
      "Training log: 2 epoch (35968 / 60000 train. data). Loss: 1.4929394721984863\n",
      "Training log: 2 epoch (37248 / 60000 train. data). Loss: 1.4862403869628906\n",
      "Training log: 2 epoch (38528 / 60000 train. data). Loss: 1.4736274480819702\n",
      "Training log: 2 epoch (39808 / 60000 train. data). Loss: 1.4856399297714233\n",
      "Training log: 2 epoch (41088 / 60000 train. data). Loss: 1.4977478981018066\n",
      "Training log: 2 epoch (42368 / 60000 train. data). Loss: 1.4722204208374023\n",
      "Training log: 2 epoch (43648 / 60000 train. data). Loss: 1.4864473342895508\n",
      "Training log: 2 epoch (44928 / 60000 train. data). Loss: 1.5094915628433228\n",
      "Training log: 2 epoch (46208 / 60000 train. data). Loss: 1.4947805404663086\n",
      "Training log: 2 epoch (47488 / 60000 train. data). Loss: 1.4865411520004272\n",
      "Training log: 2 epoch (48768 / 60000 train. data). Loss: 1.4858067035675049\n",
      "Training log: 2 epoch (50048 / 60000 train. data). Loss: 1.481716513633728\n",
      "Training log: 2 epoch (51328 / 60000 train. data). Loss: 1.4748706817626953\n",
      "Training log: 2 epoch (52608 / 60000 train. data). Loss: 1.4842004776000977\n",
      "Training log: 2 epoch (53888 / 60000 train. data). Loss: 1.4778246879577637\n",
      "Training log: 2 epoch (55168 / 60000 train. data). Loss: 1.5106101036071777\n",
      "Training log: 2 epoch (56448 / 60000 train. data). Loss: 1.4891674518585205\n",
      "Training log: 2 epoch (57728 / 60000 train. data). Loss: 1.4842969179153442\n",
      "Training log: 2 epoch (59008 / 60000 train. data). Loss: 1.5115578174591064\n",
      "Test loss (avg): 0.011711004602909088, Accuracy: 0.9803\n",
      "Training log: 3 epoch (128 / 60000 train. data). Loss: 1.4832582473754883\n",
      "Training log: 3 epoch (1408 / 60000 train. data). Loss: 1.4811055660247803\n",
      "Training log: 3 epoch (2688 / 60000 train. data). Loss: 1.4943255186080933\n",
      "Training log: 3 epoch (3968 / 60000 train. data). Loss: 1.4893977642059326\n",
      "Training log: 3 epoch (5248 / 60000 train. data). Loss: 1.464752435684204\n",
      "Training log: 3 epoch (6528 / 60000 train. data). Loss: 1.4730437994003296\n",
      "Training log: 3 epoch (7808 / 60000 train. data). Loss: 1.4843684434890747\n",
      "Training log: 3 epoch (9088 / 60000 train. data). Loss: 1.483156442642212\n",
      "Training log: 3 epoch (10368 / 60000 train. data). Loss: 1.4924030303955078\n",
      "Training log: 3 epoch (11648 / 60000 train. data). Loss: 1.4800292253494263\n",
      "Training log: 3 epoch (12928 / 60000 train. data). Loss: 1.465354084968567\n",
      "Training log: 3 epoch (14208 / 60000 train. data). Loss: 1.4793179035186768\n",
      "Training log: 3 epoch (15488 / 60000 train. data). Loss: 1.4698753356933594\n",
      "Training log: 3 epoch (16768 / 60000 train. data). Loss: 1.4768462181091309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 3 epoch (18048 / 60000 train. data). Loss: 1.470085620880127\n",
      "Training log: 3 epoch (19328 / 60000 train. data). Loss: 1.485600471496582\n",
      "Training log: 3 epoch (20608 / 60000 train. data). Loss: 1.4735212326049805\n",
      "Training log: 3 epoch (21888 / 60000 train. data). Loss: 1.4769343137741089\n",
      "Training log: 3 epoch (23168 / 60000 train. data). Loss: 1.4815514087677002\n",
      "Training log: 3 epoch (24448 / 60000 train. data). Loss: 1.4711551666259766\n",
      "Training log: 3 epoch (25728 / 60000 train. data). Loss: 1.493294596672058\n",
      "Training log: 3 epoch (27008 / 60000 train. data). Loss: 1.4837238788604736\n",
      "Training log: 3 epoch (28288 / 60000 train. data). Loss: 1.4892480373382568\n",
      "Training log: 3 epoch (29568 / 60000 train. data). Loss: 1.474858283996582\n",
      "Training log: 3 epoch (30848 / 60000 train. data). Loss: 1.4830831289291382\n",
      "Training log: 3 epoch (32128 / 60000 train. data). Loss: 1.473541498184204\n",
      "Training log: 3 epoch (33408 / 60000 train. data). Loss: 1.4883242845535278\n",
      "Training log: 3 epoch (34688 / 60000 train. data). Loss: 1.4729515314102173\n",
      "Training log: 3 epoch (35968 / 60000 train. data). Loss: 1.4621894359588623\n",
      "Training log: 3 epoch (37248 / 60000 train. data). Loss: 1.468418836593628\n",
      "Training log: 3 epoch (38528 / 60000 train. data). Loss: 1.475307583808899\n",
      "Training log: 3 epoch (39808 / 60000 train. data). Loss: 1.4792582988739014\n",
      "Training log: 3 epoch (41088 / 60000 train. data). Loss: 1.4617018699645996\n",
      "Training log: 3 epoch (42368 / 60000 train. data). Loss: 1.4871598482131958\n",
      "Training log: 3 epoch (43648 / 60000 train. data). Loss: 1.4789327383041382\n",
      "Training log: 3 epoch (44928 / 60000 train. data). Loss: 1.470122218132019\n",
      "Training log: 3 epoch (46208 / 60000 train. data). Loss: 1.468282699584961\n",
      "Training log: 3 epoch (47488 / 60000 train. data). Loss: 1.482444405555725\n",
      "Training log: 3 epoch (48768 / 60000 train. data). Loss: 1.4696242809295654\n",
      "Training log: 3 epoch (50048 / 60000 train. data). Loss: 1.4768531322479248\n",
      "Training log: 3 epoch (51328 / 60000 train. data). Loss: 1.4630675315856934\n",
      "Training log: 3 epoch (52608 / 60000 train. data). Loss: 1.4694006443023682\n",
      "Training log: 3 epoch (53888 / 60000 train. data). Loss: 1.4744964838027954\n",
      "Training log: 3 epoch (55168 / 60000 train. data). Loss: 1.4889222383499146\n",
      "Training log: 3 epoch (56448 / 60000 train. data). Loss: 1.483430027961731\n",
      "Training log: 3 epoch (57728 / 60000 train. data). Loss: 1.4719431400299072\n",
      "Training log: 3 epoch (59008 / 60000 train. data). Loss: 1.4681800603866577\n",
      "Test loss (avg): 0.011660100686550141, Accuracy: 0.9855\n",
      "Training log: 4 epoch (128 / 60000 train. data). Loss: 1.4690382480621338\n",
      "Training log: 4 epoch (1408 / 60000 train. data). Loss: 1.4630627632141113\n",
      "Training log: 4 epoch (2688 / 60000 train. data). Loss: 1.48872971534729\n",
      "Training log: 4 epoch (3968 / 60000 train. data). Loss: 1.4612773656845093\n",
      "Training log: 4 epoch (5248 / 60000 train. data). Loss: 1.471530795097351\n",
      "Training log: 4 epoch (6528 / 60000 train. data). Loss: 1.4744572639465332\n",
      "Training log: 4 epoch (7808 / 60000 train. data). Loss: 1.4996087551116943\n",
      "Training log: 4 epoch (9088 / 60000 train. data). Loss: 1.4641010761260986\n",
      "Training log: 4 epoch (10368 / 60000 train. data). Loss: 1.4789340496063232\n",
      "Training log: 4 epoch (11648 / 60000 train. data). Loss: 1.4774162769317627\n",
      "Training log: 4 epoch (12928 / 60000 train. data). Loss: 1.4794954061508179\n",
      "Training log: 4 epoch (14208 / 60000 train. data). Loss: 1.4690438508987427\n",
      "Training log: 4 epoch (15488 / 60000 train. data). Loss: 1.4611867666244507\n",
      "Training log: 4 epoch (16768 / 60000 train. data). Loss: 1.4835952520370483\n",
      "Training log: 4 epoch (18048 / 60000 train. data). Loss: 1.463536024093628\n",
      "Training log: 4 epoch (19328 / 60000 train. data). Loss: 1.467851161956787\n",
      "Training log: 4 epoch (20608 / 60000 train. data). Loss: 1.4612928628921509\n",
      "Training log: 4 epoch (21888 / 60000 train. data). Loss: 1.477021336555481\n",
      "Training log: 4 epoch (23168 / 60000 train. data). Loss: 1.4654682874679565\n",
      "Training log: 4 epoch (24448 / 60000 train. data). Loss: 1.462100863456726\n",
      "Training log: 4 epoch (25728 / 60000 train. data). Loss: 1.4691414833068848\n",
      "Training log: 4 epoch (27008 / 60000 train. data). Loss: 1.4780102968215942\n",
      "Training log: 4 epoch (28288 / 60000 train. data). Loss: 1.468975305557251\n",
      "Training log: 4 epoch (29568 / 60000 train. data). Loss: 1.4722265005111694\n",
      "Training log: 4 epoch (30848 / 60000 train. data). Loss: 1.4669538736343384\n",
      "Training log: 4 epoch (32128 / 60000 train. data). Loss: 1.468839168548584\n",
      "Training log: 4 epoch (33408 / 60000 train. data). Loss: 1.4656527042388916\n",
      "Training log: 4 epoch (34688 / 60000 train. data). Loss: 1.4650391340255737\n",
      "Training log: 4 epoch (35968 / 60000 train. data). Loss: 1.4782012701034546\n",
      "Training log: 4 epoch (37248 / 60000 train. data). Loss: 1.496972680091858\n",
      "Training log: 4 epoch (38528 / 60000 train. data). Loss: 1.4689186811447144\n",
      "Training log: 4 epoch (39808 / 60000 train. data). Loss: 1.4686505794525146\n",
      "Training log: 4 epoch (41088 / 60000 train. data). Loss: 1.4652661085128784\n",
      "Training log: 4 epoch (42368 / 60000 train. data). Loss: 1.4706169366836548\n",
      "Training log: 4 epoch (43648 / 60000 train. data). Loss: 1.4759799242019653\n",
      "Training log: 4 epoch (44928 / 60000 train. data). Loss: 1.482114553451538\n",
      "Training log: 4 epoch (46208 / 60000 train. data). Loss: 1.4738225936889648\n",
      "Training log: 4 epoch (47488 / 60000 train. data). Loss: 1.4784265756607056\n",
      "Training log: 4 epoch (48768 / 60000 train. data). Loss: 1.4759632349014282\n",
      "Training log: 4 epoch (50048 / 60000 train. data). Loss: 1.4806742668151855\n",
      "Training log: 4 epoch (51328 / 60000 train. data). Loss: 1.4690290689468384\n",
      "Training log: 4 epoch (52608 / 60000 train. data). Loss: 1.4612663984298706\n",
      "Training log: 4 epoch (53888 / 60000 train. data). Loss: 1.4757483005523682\n",
      "Training log: 4 epoch (55168 / 60000 train. data). Loss: 1.4758802652359009\n",
      "Training log: 4 epoch (56448 / 60000 train. data). Loss: 1.472393274307251\n",
      "Training log: 4 epoch (57728 / 60000 train. data). Loss: 1.468499779701233\n",
      "Training log: 4 epoch (59008 / 60000 train. data). Loss: 1.4770889282226562\n",
      "Test loss (avg): 0.011666324365139008, Accuracy: 0.9845\n",
      "Training log: 5 epoch (128 / 60000 train. data). Loss: 1.4615490436553955\n",
      "Training log: 5 epoch (1408 / 60000 train. data). Loss: 1.4781074523925781\n",
      "Training log: 5 epoch (2688 / 60000 train. data). Loss: 1.4614670276641846\n",
      "Training log: 5 epoch (3968 / 60000 train. data). Loss: 1.4611847400665283\n",
      "Training log: 5 epoch (5248 / 60000 train. data). Loss: 1.4693479537963867\n",
      "Training log: 5 epoch (6528 / 60000 train. data). Loss: 1.4684619903564453\n",
      "Training log: 5 epoch (7808 / 60000 train. data). Loss: 1.4772931337356567\n",
      "Training log: 5 epoch (9088 / 60000 train. data). Loss: 1.4679296016693115\n",
      "Training log: 5 epoch (10368 / 60000 train. data). Loss: 1.4691576957702637\n",
      "Training log: 5 epoch (11648 / 60000 train. data). Loss: 1.4755955934524536\n",
      "Training log: 5 epoch (12928 / 60000 train. data). Loss: 1.4685158729553223\n",
      "Training log: 5 epoch (14208 / 60000 train. data). Loss: 1.4726612567901611\n",
      "Training log: 5 epoch (15488 / 60000 train. data). Loss: 1.4687877893447876\n",
      "Training log: 5 epoch (16768 / 60000 train. data). Loss: 1.474115014076233\n",
      "Training log: 5 epoch (18048 / 60000 train. data). Loss: 1.4863977432250977\n",
      "Training log: 5 epoch (19328 / 60000 train. data). Loss: 1.4760253429412842\n",
      "Training log: 5 epoch (20608 / 60000 train. data). Loss: 1.4689280986785889\n",
      "Training log: 5 epoch (21888 / 60000 train. data). Loss: 1.4884363412857056\n",
      "Training log: 5 epoch (23168 / 60000 train. data). Loss: 1.4855692386627197\n",
      "Training log: 5 epoch (24448 / 60000 train. data). Loss: 1.4848408699035645\n",
      "Training log: 5 epoch (25728 / 60000 train. data). Loss: 1.4776712656021118\n",
      "Training log: 5 epoch (27008 / 60000 train. data). Loss: 1.4643326997756958\n",
      "Training log: 5 epoch (28288 / 60000 train. data). Loss: 1.483283281326294\n",
      "Training log: 5 epoch (29568 / 60000 train. data). Loss: 1.4708950519561768\n",
      "Training log: 5 epoch (30848 / 60000 train. data). Loss: 1.4621021747589111\n",
      "Training log: 5 epoch (32128 / 60000 train. data). Loss: 1.4902904033660889\n",
      "Training log: 5 epoch (33408 / 60000 train. data). Loss: 1.4842736721038818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 5 epoch (34688 / 60000 train. data). Loss: 1.4796512126922607\n",
      "Training log: 5 epoch (35968 / 60000 train. data). Loss: 1.4698057174682617\n",
      "Training log: 5 epoch (37248 / 60000 train. data). Loss: 1.5074025392532349\n",
      "Training log: 5 epoch (38528 / 60000 train. data). Loss: 1.4704586267471313\n",
      "Training log: 5 epoch (39808 / 60000 train. data). Loss: 1.468669056892395\n",
      "Training log: 5 epoch (41088 / 60000 train. data). Loss: 1.468674898147583\n",
      "Training log: 5 epoch (42368 / 60000 train. data). Loss: 1.4774123430252075\n",
      "Training log: 5 epoch (43648 / 60000 train. data). Loss: 1.47038733959198\n",
      "Training log: 5 epoch (44928 / 60000 train. data). Loss: 1.4690706729888916\n",
      "Training log: 5 epoch (46208 / 60000 train. data). Loss: 1.4696643352508545\n",
      "Training log: 5 epoch (47488 / 60000 train. data). Loss: 1.4899685382843018\n",
      "Training log: 5 epoch (48768 / 60000 train. data). Loss: 1.4819477796554565\n",
      "Training log: 5 epoch (50048 / 60000 train. data). Loss: 1.4683398008346558\n",
      "Training log: 5 epoch (51328 / 60000 train. data). Loss: 1.4687546491622925\n",
      "Training log: 5 epoch (52608 / 60000 train. data). Loss: 1.4906330108642578\n",
      "Training log: 5 epoch (53888 / 60000 train. data). Loss: 1.4687448740005493\n",
      "Training log: 5 epoch (55168 / 60000 train. data). Loss: 1.4778324365615845\n",
      "Training log: 5 epoch (56448 / 60000 train. data). Loss: 1.4817694425582886\n",
      "Training log: 5 epoch (57728 / 60000 train. data). Loss: 1.461310863494873\n",
      "Training log: 5 epoch (59008 / 60000 train. data). Loss: 1.462195634841919\n",
      "Test loss (avg): 0.011649852967262267, Accuracy: 0.9866\n",
      "Training log: 6 epoch (128 / 60000 train. data). Loss: 1.4698833227157593\n",
      "Training log: 6 epoch (1408 / 60000 train. data). Loss: 1.4775539636611938\n",
      "Training log: 6 epoch (2688 / 60000 train. data). Loss: 1.4688745737075806\n",
      "Training log: 6 epoch (3968 / 60000 train. data). Loss: 1.4612420797348022\n",
      "Training log: 6 epoch (5248 / 60000 train. data). Loss: 1.4769234657287598\n",
      "Training log: 6 epoch (6528 / 60000 train. data). Loss: 1.4755885601043701\n",
      "Training log: 6 epoch (7808 / 60000 train. data). Loss: 1.4611517190933228\n",
      "Training log: 6 epoch (9088 / 60000 train. data). Loss: 1.4698586463928223\n",
      "Training log: 6 epoch (10368 / 60000 train. data). Loss: 1.4695732593536377\n",
      "Training log: 6 epoch (11648 / 60000 train. data). Loss: 1.479413390159607\n",
      "Training log: 6 epoch (12928 / 60000 train. data). Loss: 1.4680964946746826\n",
      "Training log: 6 epoch (14208 / 60000 train. data). Loss: 1.4702043533325195\n",
      "Training log: 6 epoch (15488 / 60000 train. data). Loss: 1.4612911939620972\n",
      "Training log: 6 epoch (16768 / 60000 train. data). Loss: 1.4700126647949219\n",
      "Training log: 6 epoch (18048 / 60000 train. data). Loss: 1.4611526727676392\n",
      "Training log: 6 epoch (19328 / 60000 train. data). Loss: 1.4611626863479614\n",
      "Training log: 6 epoch (20608 / 60000 train. data). Loss: 1.4660251140594482\n",
      "Training log: 6 epoch (21888 / 60000 train. data). Loss: 1.4770245552062988\n",
      "Training log: 6 epoch (23168 / 60000 train. data). Loss: 1.4684733152389526\n",
      "Training log: 6 epoch (24448 / 60000 train. data). Loss: 1.4614331722259521\n",
      "Training log: 6 epoch (25728 / 60000 train. data). Loss: 1.4750491380691528\n",
      "Training log: 6 epoch (27008 / 60000 train. data). Loss: 1.484732985496521\n",
      "Training log: 6 epoch (28288 / 60000 train. data). Loss: 1.4612091779708862\n",
      "Training log: 6 epoch (29568 / 60000 train. data). Loss: 1.4826041460037231\n",
      "Training log: 6 epoch (30848 / 60000 train. data). Loss: 1.4729995727539062\n",
      "Training log: 6 epoch (32128 / 60000 train. data). Loss: 1.4766058921813965\n",
      "Training log: 6 epoch (33408 / 60000 train. data). Loss: 1.4696029424667358\n",
      "Training log: 6 epoch (34688 / 60000 train. data). Loss: 1.4833405017852783\n",
      "Training log: 6 epoch (35968 / 60000 train. data). Loss: 1.476893663406372\n",
      "Training log: 6 epoch (37248 / 60000 train. data). Loss: 1.4851049184799194\n",
      "Training log: 6 epoch (38528 / 60000 train. data). Loss: 1.4837620258331299\n",
      "Training log: 6 epoch (39808 / 60000 train. data). Loss: 1.4689984321594238\n",
      "Training log: 6 epoch (41088 / 60000 train. data). Loss: 1.4764840602874756\n",
      "Training log: 6 epoch (42368 / 60000 train. data). Loss: 1.4617815017700195\n",
      "Training log: 6 epoch (43648 / 60000 train. data). Loss: 1.4766944646835327\n",
      "Training log: 6 epoch (44928 / 60000 train. data). Loss: 1.4689851999282837\n",
      "Training log: 6 epoch (46208 / 60000 train. data). Loss: 1.4641258716583252\n",
      "Training log: 6 epoch (47488 / 60000 train. data). Loss: 1.4680976867675781\n",
      "Training log: 6 epoch (48768 / 60000 train. data). Loss: 1.469517469406128\n",
      "Training log: 6 epoch (50048 / 60000 train. data). Loss: 1.4965039491653442\n",
      "Training log: 6 epoch (51328 / 60000 train. data). Loss: 1.478026032447815\n",
      "Training log: 6 epoch (52608 / 60000 train. data). Loss: 1.4647479057312012\n",
      "Training log: 6 epoch (53888 / 60000 train. data). Loss: 1.4833263158798218\n",
      "Training log: 6 epoch (55168 / 60000 train. data). Loss: 1.469017505645752\n",
      "Training log: 6 epoch (56448 / 60000 train. data). Loss: 1.4632093906402588\n",
      "Training log: 6 epoch (57728 / 60000 train. data). Loss: 1.4760487079620361\n",
      "Training log: 6 epoch (59008 / 60000 train. data). Loss: 1.4725520610809326\n",
      "Test loss (avg): 0.011662299585342407, Accuracy: 0.9848\n",
      "Training log: 7 epoch (128 / 60000 train. data). Loss: 1.479804277420044\n",
      "Training log: 7 epoch (1408 / 60000 train. data). Loss: 1.4762027263641357\n",
      "Training log: 7 epoch (2688 / 60000 train. data). Loss: 1.4767756462097168\n",
      "Training log: 7 epoch (3968 / 60000 train. data). Loss: 1.4690687656402588\n",
      "Training log: 7 epoch (5248 / 60000 train. data). Loss: 1.4765135049819946\n",
      "Training log: 7 epoch (6528 / 60000 train. data). Loss: 1.4797484874725342\n",
      "Training log: 7 epoch (7808 / 60000 train. data). Loss: 1.4760825634002686\n",
      "Training log: 7 epoch (9088 / 60000 train. data). Loss: 1.4611690044403076\n",
      "Training log: 7 epoch (10368 / 60000 train. data). Loss: 1.4611592292785645\n",
      "Training log: 7 epoch (11648 / 60000 train. data). Loss: 1.4622677564620972\n",
      "Training log: 7 epoch (12928 / 60000 train. data). Loss: 1.4672236442565918\n",
      "Training log: 7 epoch (14208 / 60000 train. data). Loss: 1.4689748287200928\n",
      "Training log: 7 epoch (15488 / 60000 train. data). Loss: 1.4704581499099731\n",
      "Training log: 7 epoch (16768 / 60000 train. data). Loss: 1.46501624584198\n",
      "Training log: 7 epoch (18048 / 60000 train. data). Loss: 1.4612029790878296\n",
      "Training log: 7 epoch (19328 / 60000 train. data). Loss: 1.4611594676971436\n",
      "Training log: 7 epoch (20608 / 60000 train. data). Loss: 1.4635945558547974\n",
      "Training log: 7 epoch (21888 / 60000 train. data). Loss: 1.477903127670288\n",
      "Training log: 7 epoch (23168 / 60000 train. data). Loss: 1.4613265991210938\n",
      "Training log: 7 epoch (24448 / 60000 train. data). Loss: 1.4612152576446533\n",
      "Training log: 7 epoch (25728 / 60000 train. data). Loss: 1.4613333940505981\n",
      "Training log: 7 epoch (27008 / 60000 train. data). Loss: 1.461176872253418\n",
      "Training log: 7 epoch (28288 / 60000 train. data). Loss: 1.479599952697754\n",
      "Training log: 7 epoch (29568 / 60000 train. data). Loss: 1.4627480506896973\n",
      "Training log: 7 epoch (30848 / 60000 train. data). Loss: 1.4611514806747437\n",
      "Training log: 7 epoch (32128 / 60000 train. data). Loss: 1.4784611463546753\n",
      "Training log: 7 epoch (33408 / 60000 train. data). Loss: 1.462546944618225\n",
      "Training log: 7 epoch (34688 / 60000 train. data). Loss: 1.4638705253601074\n",
      "Training log: 7 epoch (35968 / 60000 train. data). Loss: 1.4767221212387085\n",
      "Training log: 7 epoch (37248 / 60000 train. data). Loss: 1.4611517190933228\n",
      "Training log: 7 epoch (38528 / 60000 train. data). Loss: 1.465936303138733\n",
      "Training log: 7 epoch (39808 / 60000 train. data). Loss: 1.4774832725524902\n",
      "Training log: 7 epoch (41088 / 60000 train. data). Loss: 1.4774458408355713\n",
      "Training log: 7 epoch (42368 / 60000 train. data). Loss: 1.4798685312271118\n",
      "Training log: 7 epoch (43648 / 60000 train. data). Loss: 1.4705716371536255\n",
      "Training log: 7 epoch (44928 / 60000 train. data). Loss: 1.4929020404815674\n",
      "Training log: 7 epoch (46208 / 60000 train. data). Loss: 1.4725537300109863\n",
      "Training log: 7 epoch (47488 / 60000 train. data). Loss: 1.4635429382324219\n",
      "Training log: 7 epoch (48768 / 60000 train. data). Loss: 1.4709590673446655\n",
      "Training log: 7 epoch (50048 / 60000 train. data). Loss: 1.481754183769226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 7 epoch (51328 / 60000 train. data). Loss: 1.4611531496047974\n",
      "Training log: 7 epoch (52608 / 60000 train. data). Loss: 1.4689972400665283\n",
      "Training log: 7 epoch (53888 / 60000 train. data). Loss: 1.4767639636993408\n",
      "Training log: 7 epoch (55168 / 60000 train. data). Loss: 1.4704238176345825\n",
      "Training log: 7 epoch (56448 / 60000 train. data). Loss: 1.4611541032791138\n",
      "Training log: 7 epoch (57728 / 60000 train. data). Loss: 1.4757914543151855\n",
      "Training log: 7 epoch (59008 / 60000 train. data). Loss: 1.4620795249938965\n",
      "Test loss (avg): 0.011646606171131134, Accuracy: 0.9872\n",
      "Training log: 8 epoch (128 / 60000 train. data). Loss: 1.461151361465454\n",
      "Training log: 8 epoch (1408 / 60000 train. data). Loss: 1.4613784551620483\n",
      "Training log: 8 epoch (2688 / 60000 train. data). Loss: 1.46894109249115\n",
      "Training log: 8 epoch (3968 / 60000 train. data). Loss: 1.4612032175064087\n",
      "Training log: 8 epoch (5248 / 60000 train. data). Loss: 1.469003438949585\n",
      "Training log: 8 epoch (6528 / 60000 train. data). Loss: 1.4770108461380005\n",
      "Training log: 8 epoch (7808 / 60000 train. data). Loss: 1.4611543416976929\n",
      "Training log: 8 epoch (9088 / 60000 train. data). Loss: 1.4611598253250122\n",
      "Training log: 8 epoch (10368 / 60000 train. data). Loss: 1.461761713027954\n",
      "Training log: 8 epoch (11648 / 60000 train. data). Loss: 1.4611812829971313\n",
      "Training log: 8 epoch (12928 / 60000 train. data). Loss: 1.4773728847503662\n",
      "Training log: 8 epoch (14208 / 60000 train. data). Loss: 1.470651626586914\n",
      "Training log: 8 epoch (15488 / 60000 train. data). Loss: 1.4859288930892944\n",
      "Training log: 8 epoch (16768 / 60000 train. data). Loss: 1.4611607789993286\n",
      "Training log: 8 epoch (18048 / 60000 train. data). Loss: 1.4676297903060913\n",
      "Training log: 8 epoch (19328 / 60000 train. data). Loss: 1.4689823389053345\n",
      "Training log: 8 epoch (20608 / 60000 train. data). Loss: 1.4612940549850464\n",
      "Training log: 8 epoch (21888 / 60000 train. data). Loss: 1.4719158411026\n",
      "Training log: 8 epoch (23168 / 60000 train. data). Loss: 1.4689120054244995\n",
      "Training log: 8 epoch (24448 / 60000 train. data). Loss: 1.461544156074524\n",
      "Training log: 8 epoch (25728 / 60000 train. data). Loss: 1.4846071004867554\n",
      "Training log: 8 epoch (27008 / 60000 train. data). Loss: 1.4693101644515991\n",
      "Training log: 8 epoch (28288 / 60000 train. data). Loss: 1.4658596515655518\n",
      "Training log: 8 epoch (29568 / 60000 train. data). Loss: 1.468843698501587\n",
      "Training log: 8 epoch (30848 / 60000 train. data). Loss: 1.464642882347107\n",
      "Training log: 8 epoch (32128 / 60000 train. data). Loss: 1.4774013757705688\n",
      "Training log: 8 epoch (33408 / 60000 train. data). Loss: 1.4685742855072021\n",
      "Training log: 8 epoch (34688 / 60000 train. data). Loss: 1.4697458744049072\n",
      "Training log: 8 epoch (35968 / 60000 train. data). Loss: 1.4612715244293213\n",
      "Training log: 8 epoch (37248 / 60000 train. data). Loss: 1.4689645767211914\n",
      "Training log: 8 epoch (38528 / 60000 train. data). Loss: 1.4625945091247559\n",
      "Training log: 8 epoch (39808 / 60000 train. data). Loss: 1.4612468481063843\n",
      "Training log: 8 epoch (41088 / 60000 train. data). Loss: 1.4619626998901367\n",
      "Training log: 8 epoch (42368 / 60000 train. data). Loss: 1.4696342945098877\n",
      "Training log: 8 epoch (43648 / 60000 train. data). Loss: 1.4762203693389893\n",
      "Training log: 8 epoch (44928 / 60000 train. data). Loss: 1.4689768552780151\n",
      "Training log: 8 epoch (46208 / 60000 train. data). Loss: 1.4674643278121948\n",
      "Training log: 8 epoch (47488 / 60000 train. data). Loss: 1.4656962156295776\n",
      "Training log: 8 epoch (48768 / 60000 train. data). Loss: 1.469336748123169\n",
      "Training log: 8 epoch (50048 / 60000 train. data). Loss: 1.4678630828857422\n",
      "Training log: 8 epoch (51328 / 60000 train. data). Loss: 1.4683129787445068\n",
      "Training log: 8 epoch (52608 / 60000 train. data). Loss: 1.468974232673645\n",
      "Training log: 8 epoch (53888 / 60000 train. data). Loss: 1.491537094116211\n",
      "Training log: 8 epoch (55168 / 60000 train. data). Loss: 1.4679956436157227\n",
      "Training log: 8 epoch (56448 / 60000 train. data). Loss: 1.4617000818252563\n",
      "Training log: 8 epoch (57728 / 60000 train. data). Loss: 1.4836257696151733\n",
      "Training log: 8 epoch (59008 / 60000 train. data). Loss: 1.4646879434585571\n",
      "Test loss (avg): 0.01163369289636612, Accuracy: 0.9885\n",
      "Training log: 9 epoch (128 / 60000 train. data). Loss: 1.4657464027404785\n",
      "Training log: 9 epoch (1408 / 60000 train. data). Loss: 1.4661654233932495\n",
      "Training log: 9 epoch (2688 / 60000 train. data). Loss: 1.461694598197937\n",
      "Training log: 9 epoch (3968 / 60000 train. data). Loss: 1.4690370559692383\n",
      "Training log: 9 epoch (5248 / 60000 train. data). Loss: 1.4612103700637817\n",
      "Training log: 9 epoch (6528 / 60000 train. data). Loss: 1.4799000024795532\n",
      "Training log: 9 epoch (7808 / 60000 train. data). Loss: 1.4714922904968262\n",
      "Training log: 9 epoch (9088 / 60000 train. data). Loss: 1.46901273727417\n",
      "Training log: 9 epoch (10368 / 60000 train. data). Loss: 1.4766029119491577\n",
      "Training log: 9 epoch (11648 / 60000 train. data). Loss: 1.4616855382919312\n",
      "Training log: 9 epoch (12928 / 60000 train. data). Loss: 1.4730234146118164\n",
      "Training log: 9 epoch (14208 / 60000 train. data). Loss: 1.4615845680236816\n",
      "Training log: 9 epoch (15488 / 60000 train. data). Loss: 1.461151361465454\n",
      "Training log: 9 epoch (16768 / 60000 train. data). Loss: 1.4612103700637817\n",
      "Training log: 9 epoch (18048 / 60000 train. data). Loss: 1.4824559688568115\n",
      "Training log: 9 epoch (19328 / 60000 train. data). Loss: 1.4686379432678223\n",
      "Training log: 9 epoch (20608 / 60000 train. data). Loss: 1.4729793071746826\n",
      "Training log: 9 epoch (21888 / 60000 train. data). Loss: 1.4735008478164673\n",
      "Training log: 9 epoch (23168 / 60000 train. data). Loss: 1.4654953479766846\n",
      "Training log: 9 epoch (24448 / 60000 train. data). Loss: 1.4611761569976807\n",
      "Training log: 9 epoch (25728 / 60000 train. data). Loss: 1.484882116317749\n",
      "Training log: 9 epoch (27008 / 60000 train. data). Loss: 1.47320556640625\n",
      "Training log: 9 epoch (28288 / 60000 train. data). Loss: 1.4616352319717407\n",
      "Training log: 9 epoch (29568 / 60000 train. data). Loss: 1.4630801677703857\n",
      "Training log: 9 epoch (30848 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 9 epoch (32128 / 60000 train. data). Loss: 1.4689679145812988\n",
      "Training log: 9 epoch (33408 / 60000 train. data). Loss: 1.461151361465454\n",
      "Training log: 9 epoch (34688 / 60000 train. data). Loss: 1.4788339138031006\n",
      "Training log: 9 epoch (35968 / 60000 train. data). Loss: 1.4635497331619263\n",
      "Training log: 9 epoch (37248 / 60000 train. data). Loss: 1.4765952825546265\n",
      "Training log: 9 epoch (38528 / 60000 train. data). Loss: 1.4650558233261108\n",
      "Training log: 9 epoch (39808 / 60000 train. data). Loss: 1.4628562927246094\n",
      "Training log: 9 epoch (41088 / 60000 train. data). Loss: 1.4681308269500732\n",
      "Training log: 9 epoch (42368 / 60000 train. data). Loss: 1.4700615406036377\n",
      "Training log: 9 epoch (43648 / 60000 train. data). Loss: 1.462345838546753\n",
      "Training log: 9 epoch (44928 / 60000 train. data). Loss: 1.4614773988723755\n",
      "Training log: 9 epoch (46208 / 60000 train. data). Loss: 1.4612475633621216\n",
      "Training log: 9 epoch (47488 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 9 epoch (48768 / 60000 train. data). Loss: 1.4611517190933228\n",
      "Training log: 9 epoch (50048 / 60000 train. data). Loss: 1.4689619541168213\n",
      "Training log: 9 epoch (51328 / 60000 train. data). Loss: 1.4714106321334839\n",
      "Training log: 9 epoch (52608 / 60000 train. data). Loss: 1.4691340923309326\n",
      "Training log: 9 epoch (53888 / 60000 train. data). Loss: 1.469254970550537\n",
      "Training log: 9 epoch (55168 / 60000 train. data). Loss: 1.4891945123672485\n",
      "Training log: 9 epoch (56448 / 60000 train. data). Loss: 1.468971848487854\n",
      "Training log: 9 epoch (57728 / 60000 train. data). Loss: 1.467529535293579\n",
      "Training log: 9 epoch (59008 / 60000 train. data). Loss: 1.4611527919769287\n",
      "Test loss (avg): 0.01162231525182724, Accuracy: 0.9895\n",
      "Training log: 10 epoch (128 / 60000 train. data). Loss: 1.468961477279663\n",
      "Training log: 10 epoch (1408 / 60000 train. data). Loss: 1.4766261577606201\n",
      "Training log: 10 epoch (2688 / 60000 train. data). Loss: 1.4613070487976074\n",
      "Training log: 10 epoch (3968 / 60000 train. data). Loss: 1.4620846509933472\n",
      "Training log: 10 epoch (5248 / 60000 train. data). Loss: 1.4691165685653687\n",
      "Training log: 10 epoch (6528 / 60000 train. data). Loss: 1.4624792337417603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 10 epoch (7808 / 60000 train. data). Loss: 1.4686193466186523\n",
      "Training log: 10 epoch (9088 / 60000 train. data). Loss: 1.471354603767395\n",
      "Training log: 10 epoch (10368 / 60000 train. data). Loss: 1.4698835611343384\n",
      "Training log: 10 epoch (11648 / 60000 train. data). Loss: 1.468919277191162\n",
      "Training log: 10 epoch (12928 / 60000 train. data). Loss: 1.4680036306381226\n",
      "Training log: 10 epoch (14208 / 60000 train. data). Loss: 1.4751436710357666\n",
      "Training log: 10 epoch (15488 / 60000 train. data). Loss: 1.4621775150299072\n",
      "Training log: 10 epoch (16768 / 60000 train. data). Loss: 1.462120532989502\n",
      "Training log: 10 epoch (18048 / 60000 train. data). Loss: 1.483266830444336\n",
      "Training log: 10 epoch (19328 / 60000 train. data). Loss: 1.4611577987670898\n",
      "Training log: 10 epoch (20608 / 60000 train. data). Loss: 1.4877843856811523\n",
      "Training log: 10 epoch (21888 / 60000 train. data). Loss: 1.462991714477539\n",
      "Training log: 10 epoch (23168 / 60000 train. data). Loss: 1.4771534204483032\n",
      "Training log: 10 epoch (24448 / 60000 train. data). Loss: 1.4611624479293823\n",
      "Training log: 10 epoch (25728 / 60000 train. data). Loss: 1.4611927270889282\n",
      "Training log: 10 epoch (27008 / 60000 train. data). Loss: 1.4716289043426514\n",
      "Training log: 10 epoch (28288 / 60000 train. data). Loss: 1.4611749649047852\n",
      "Training log: 10 epoch (29568 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 10 epoch (30848 / 60000 train. data). Loss: 1.461150884628296\n",
      "Training log: 10 epoch (32128 / 60000 train. data). Loss: 1.4689875841140747\n",
      "Training log: 10 epoch (33408 / 60000 train. data). Loss: 1.468914270401001\n",
      "Training log: 10 epoch (34688 / 60000 train. data). Loss: 1.4664076566696167\n",
      "Training log: 10 epoch (35968 / 60000 train. data). Loss: 1.4689604043960571\n",
      "Training log: 10 epoch (37248 / 60000 train. data). Loss: 1.4768060445785522\n",
      "Training log: 10 epoch (38528 / 60000 train. data). Loss: 1.461194634437561\n",
      "Training log: 10 epoch (39808 / 60000 train. data). Loss: 1.4769130945205688\n",
      "Training log: 10 epoch (41088 / 60000 train. data). Loss: 1.461150884628296\n",
      "Training log: 10 epoch (42368 / 60000 train. data). Loss: 1.4643031358718872\n",
      "Training log: 10 epoch (43648 / 60000 train. data). Loss: 1.471487283706665\n",
      "Training log: 10 epoch (44928 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 10 epoch (46208 / 60000 train. data). Loss: 1.462321162223816\n",
      "Training log: 10 epoch (47488 / 60000 train. data). Loss: 1.4768342971801758\n",
      "Training log: 10 epoch (48768 / 60000 train. data). Loss: 1.4690308570861816\n",
      "Training log: 10 epoch (50048 / 60000 train. data). Loss: 1.462633728981018\n",
      "Training log: 10 epoch (51328 / 60000 train. data). Loss: 1.461154580116272\n",
      "Training log: 10 epoch (52608 / 60000 train. data). Loss: 1.4759345054626465\n",
      "Training log: 10 epoch (53888 / 60000 train. data). Loss: 1.4767783880233765\n",
      "Training log: 10 epoch (55168 / 60000 train. data). Loss: 1.468963384628296\n",
      "Training log: 10 epoch (56448 / 60000 train. data). Loss: 1.468964695930481\n",
      "Training log: 10 epoch (57728 / 60000 train. data). Loss: 1.4700798988342285\n",
      "Training log: 10 epoch (59008 / 60000 train. data). Loss: 1.4613380432128906\n",
      "Test loss (avg): 0.011644085478782654, Accuracy: 0.9878\n",
      "Training log: 11 epoch (128 / 60000 train. data). Loss: 1.4645471572875977\n",
      "Training log: 11 epoch (1408 / 60000 train. data). Loss: 1.4689022302627563\n",
      "Training log: 11 epoch (2688 / 60000 train. data). Loss: 1.4744935035705566\n",
      "Training log: 11 epoch (3968 / 60000 train. data). Loss: 1.4765597581863403\n",
      "Training log: 11 epoch (5248 / 60000 train. data). Loss: 1.4695329666137695\n",
      "Training log: 11 epoch (6528 / 60000 train. data). Loss: 1.4663522243499756\n",
      "Training log: 11 epoch (7808 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 11 epoch (9088 / 60000 train. data). Loss: 1.4611515998840332\n",
      "Training log: 11 epoch (10368 / 60000 train. data). Loss: 1.4687384366989136\n",
      "Training log: 11 epoch (11648 / 60000 train. data). Loss: 1.4856412410736084\n",
      "Training log: 11 epoch (12928 / 60000 train. data). Loss: 1.468936800956726\n",
      "Training log: 11 epoch (14208 / 60000 train. data). Loss: 1.468978762626648\n",
      "Training log: 11 epoch (15488 / 60000 train. data). Loss: 1.4711954593658447\n",
      "Training log: 11 epoch (16768 / 60000 train. data). Loss: 1.4654732942581177\n",
      "Training log: 11 epoch (18048 / 60000 train. data). Loss: 1.4692480564117432\n",
      "Training log: 11 epoch (19328 / 60000 train. data). Loss: 1.461180567741394\n",
      "Training log: 11 epoch (20608 / 60000 train. data). Loss: 1.4689284563064575\n",
      "Training log: 11 epoch (21888 / 60000 train. data). Loss: 1.4619519710540771\n",
      "Training log: 11 epoch (23168 / 60000 train. data). Loss: 1.4814342260360718\n",
      "Training log: 11 epoch (24448 / 60000 train. data). Loss: 1.4689286947250366\n",
      "Training log: 11 epoch (25728 / 60000 train. data). Loss: 1.4611552953720093\n",
      "Training log: 11 epoch (27008 / 60000 train. data). Loss: 1.461186408996582\n",
      "Training log: 11 epoch (28288 / 60000 train. data). Loss: 1.4611523151397705\n",
      "Training log: 11 epoch (29568 / 60000 train. data). Loss: 1.4682543277740479\n",
      "Training log: 11 epoch (30848 / 60000 train. data). Loss: 1.461790919303894\n",
      "Training log: 11 epoch (32128 / 60000 train. data). Loss: 1.4759317636489868\n",
      "Training log: 11 epoch (33408 / 60000 train. data). Loss: 1.461227536201477\n",
      "Training log: 11 epoch (34688 / 60000 train. data). Loss: 1.4628467559814453\n",
      "Training log: 11 epoch (35968 / 60000 train. data). Loss: 1.4768508672714233\n",
      "Training log: 11 epoch (37248 / 60000 train. data). Loss: 1.468977451324463\n",
      "Training log: 11 epoch (38528 / 60000 train. data). Loss: 1.4611549377441406\n",
      "Training log: 11 epoch (39808 / 60000 train. data). Loss: 1.4839329719543457\n",
      "Training log: 11 epoch (41088 / 60000 train. data). Loss: 1.4611529111862183\n",
      "Training log: 11 epoch (42368 / 60000 train. data). Loss: 1.4689186811447144\n",
      "Training log: 11 epoch (43648 / 60000 train. data). Loss: 1.468946933746338\n",
      "Training log: 11 epoch (44928 / 60000 train. data). Loss: 1.4768157005310059\n",
      "Training log: 11 epoch (46208 / 60000 train. data). Loss: 1.4767757654190063\n",
      "Training log: 11 epoch (47488 / 60000 train. data). Loss: 1.4612090587615967\n",
      "Training log: 11 epoch (48768 / 60000 train. data). Loss: 1.4620587825775146\n",
      "Training log: 11 epoch (50048 / 60000 train. data). Loss: 1.4689384698867798\n",
      "Training log: 11 epoch (51328 / 60000 train. data). Loss: 1.461156964302063\n",
      "Training log: 11 epoch (52608 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 11 epoch (53888 / 60000 train. data). Loss: 1.4611520767211914\n",
      "Training log: 11 epoch (55168 / 60000 train. data). Loss: 1.4689688682556152\n",
      "Training log: 11 epoch (56448 / 60000 train. data). Loss: 1.4611566066741943\n",
      "Training log: 11 epoch (57728 / 60000 train. data). Loss: 1.4611520767211914\n",
      "Training log: 11 epoch (59008 / 60000 train. data). Loss: 1.468962550163269\n",
      "Test loss (avg): 0.01165306967496872, Accuracy: 0.9857\n",
      "Training log: 12 epoch (128 / 60000 train. data). Loss: 1.4695757627487183\n",
      "Training log: 12 epoch (1408 / 60000 train. data). Loss: 1.4613392353057861\n",
      "Training log: 12 epoch (2688 / 60000 train. data). Loss: 1.4615662097930908\n",
      "Training log: 12 epoch (3968 / 60000 train. data). Loss: 1.4738775491714478\n",
      "Training log: 12 epoch (5248 / 60000 train. data). Loss: 1.4751781225204468\n",
      "Training log: 12 epoch (6528 / 60000 train. data). Loss: 1.4613871574401855\n",
      "Training log: 12 epoch (7808 / 60000 train. data). Loss: 1.475743293762207\n",
      "Training log: 12 epoch (9088 / 60000 train. data). Loss: 1.4711915254592896\n",
      "Training log: 12 epoch (10368 / 60000 train. data). Loss: 1.465973973274231\n",
      "Training log: 12 epoch (11648 / 60000 train. data). Loss: 1.4639972448349\n",
      "Training log: 12 epoch (12928 / 60000 train. data). Loss: 1.4611977338790894\n",
      "Training log: 12 epoch (14208 / 60000 train. data). Loss: 1.4616087675094604\n",
      "Training log: 12 epoch (15488 / 60000 train. data). Loss: 1.478147029876709\n",
      "Training log: 12 epoch (16768 / 60000 train. data). Loss: 1.4611510038375854\n",
      "Training log: 12 epoch (18048 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 12 epoch (19328 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 12 epoch (20608 / 60000 train. data). Loss: 1.4612032175064087\n",
      "Training log: 12 epoch (21888 / 60000 train. data). Loss: 1.4651139974594116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 12 epoch (23168 / 60000 train. data). Loss: 1.4830989837646484\n",
      "Training log: 12 epoch (24448 / 60000 train. data). Loss: 1.46132493019104\n",
      "Training log: 12 epoch (25728 / 60000 train. data). Loss: 1.471467137336731\n",
      "Training log: 12 epoch (27008 / 60000 train. data). Loss: 1.4689644575119019\n",
      "Training log: 12 epoch (28288 / 60000 train. data). Loss: 1.476303219795227\n",
      "Training log: 12 epoch (29568 / 60000 train. data). Loss: 1.467045783996582\n",
      "Training log: 12 epoch (30848 / 60000 train. data). Loss: 1.4845528602600098\n",
      "Training log: 12 epoch (32128 / 60000 train. data). Loss: 1.4705098867416382\n",
      "Training log: 12 epoch (33408 / 60000 train. data). Loss: 1.468806505203247\n",
      "Training log: 12 epoch (34688 / 60000 train. data). Loss: 1.476833462715149\n",
      "Training log: 12 epoch (35968 / 60000 train. data). Loss: 1.4639543294906616\n",
      "Training log: 12 epoch (37248 / 60000 train. data). Loss: 1.4611518383026123\n",
      "Training log: 12 epoch (38528 / 60000 train. data). Loss: 1.4711859226226807\n",
      "Training log: 12 epoch (39808 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 12 epoch (41088 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 12 epoch (42368 / 60000 train. data). Loss: 1.4689635038375854\n",
      "Training log: 12 epoch (43648 / 60000 train. data). Loss: 1.4623135328292847\n",
      "Training log: 12 epoch (44928 / 60000 train. data). Loss: 1.4802523851394653\n",
      "Training log: 12 epoch (46208 / 60000 train. data). Loss: 1.4611512422561646\n",
      "Training log: 12 epoch (47488 / 60000 train. data). Loss: 1.4630190134048462\n",
      "Training log: 12 epoch (48768 / 60000 train. data). Loss: 1.461228609085083\n",
      "Training log: 12 epoch (50048 / 60000 train. data). Loss: 1.4807403087615967\n",
      "Training log: 12 epoch (51328 / 60000 train. data). Loss: 1.4611549377441406\n",
      "Training log: 12 epoch (52608 / 60000 train. data). Loss: 1.4819847345352173\n",
      "Training log: 12 epoch (53888 / 60000 train. data). Loss: 1.4828237295150757\n",
      "Training log: 12 epoch (55168 / 60000 train. data). Loss: 1.4689688682556152\n",
      "Training log: 12 epoch (56448 / 60000 train. data). Loss: 1.4624422788619995\n",
      "Training log: 12 epoch (57728 / 60000 train. data). Loss: 1.4699070453643799\n",
      "Training log: 12 epoch (59008 / 60000 train. data). Loss: 1.4622124433517456\n",
      "Test loss (avg): 0.011665533518791198, Accuracy: 0.9846\n",
      "Training log: 13 epoch (128 / 60000 train. data). Loss: 1.4611520767211914\n",
      "Training log: 13 epoch (1408 / 60000 train. data). Loss: 1.4611650705337524\n",
      "Training log: 13 epoch (2688 / 60000 train. data). Loss: 1.468932032585144\n",
      "Training log: 13 epoch (3968 / 60000 train. data). Loss: 1.4611687660217285\n",
      "Training log: 13 epoch (5248 / 60000 train. data). Loss: 1.4687377214431763\n",
      "Training log: 13 epoch (6528 / 60000 train. data). Loss: 1.4681987762451172\n",
      "Training log: 13 epoch (7808 / 60000 train. data). Loss: 1.4611517190933228\n",
      "Training log: 13 epoch (9088 / 60000 train. data). Loss: 1.4689764976501465\n",
      "Training log: 13 epoch (10368 / 60000 train. data). Loss: 1.4756416082382202\n",
      "Training log: 13 epoch (11648 / 60000 train. data). Loss: 1.4693844318389893\n",
      "Training log: 13 epoch (12928 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 13 epoch (14208 / 60000 train. data). Loss: 1.4611512422561646\n",
      "Training log: 13 epoch (15488 / 60000 train. data). Loss: 1.4616470336914062\n",
      "Training log: 13 epoch (16768 / 60000 train. data). Loss: 1.4611515998840332\n",
      "Training log: 13 epoch (18048 / 60000 train. data). Loss: 1.475948691368103\n",
      "Training log: 13 epoch (19328 / 60000 train. data). Loss: 1.4764885902404785\n",
      "Training log: 13 epoch (20608 / 60000 train. data). Loss: 1.4740594625473022\n",
      "Training log: 13 epoch (21888 / 60000 train. data). Loss: 1.4762239456176758\n",
      "Training log: 13 epoch (23168 / 60000 train. data). Loss: 1.4849492311477661\n",
      "Training log: 13 epoch (24448 / 60000 train. data). Loss: 1.4625167846679688\n",
      "Training log: 13 epoch (25728 / 60000 train. data). Loss: 1.4690310955047607\n",
      "Training log: 13 epoch (27008 / 60000 train. data). Loss: 1.4768916368484497\n",
      "Training log: 13 epoch (28288 / 60000 train. data). Loss: 1.4611538648605347\n",
      "Training log: 13 epoch (29568 / 60000 train. data). Loss: 1.4689635038375854\n",
      "Training log: 13 epoch (30848 / 60000 train. data). Loss: 1.4611512422561646\n",
      "Training log: 13 epoch (32128 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 13 epoch (33408 / 60000 train. data). Loss: 1.4806371927261353\n",
      "Training log: 13 epoch (34688 / 60000 train. data). Loss: 1.4621597528457642\n",
      "Training log: 13 epoch (35968 / 60000 train. data). Loss: 1.471334457397461\n",
      "Training log: 13 epoch (37248 / 60000 train. data). Loss: 1.4834887981414795\n",
      "Training log: 13 epoch (38528 / 60000 train. data). Loss: 1.4659039974212646\n",
      "Training log: 13 epoch (39808 / 60000 train. data). Loss: 1.461562991142273\n",
      "Training log: 13 epoch (41088 / 60000 train. data). Loss: 1.4700754880905151\n",
      "Training log: 13 epoch (42368 / 60000 train. data). Loss: 1.4611543416976929\n",
      "Training log: 13 epoch (43648 / 60000 train. data). Loss: 1.4760409593582153\n",
      "Training log: 13 epoch (44928 / 60000 train. data). Loss: 1.4686803817749023\n",
      "Training log: 13 epoch (46208 / 60000 train. data). Loss: 1.4611514806747437\n",
      "Training log: 13 epoch (47488 / 60000 train. data). Loss: 1.4613397121429443\n",
      "Training log: 13 epoch (48768 / 60000 train. data). Loss: 1.4612252712249756\n",
      "Training log: 13 epoch (50048 / 60000 train. data). Loss: 1.469748854637146\n",
      "Training log: 13 epoch (51328 / 60000 train. data). Loss: 1.4612399339675903\n",
      "Training log: 13 epoch (52608 / 60000 train. data). Loss: 1.4611510038375854\n",
      "Training log: 13 epoch (53888 / 60000 train. data). Loss: 1.4611561298370361\n",
      "Training log: 13 epoch (55168 / 60000 train. data). Loss: 1.4611955881118774\n",
      "Training log: 13 epoch (56448 / 60000 train. data). Loss: 1.4693143367767334\n",
      "Training log: 13 epoch (57728 / 60000 train. data). Loss: 1.4713008403778076\n",
      "Training log: 13 epoch (59008 / 60000 train. data). Loss: 1.468263864517212\n",
      "Test loss (avg): 0.011630427920818328, Accuracy: 0.989\n",
      "Training log: 14 epoch (128 / 60000 train. data). Loss: 1.476775884628296\n",
      "Training log: 14 epoch (1408 / 60000 train. data). Loss: 1.4611592292785645\n",
      "Training log: 14 epoch (2688 / 60000 train. data). Loss: 1.4689635038375854\n",
      "Training log: 14 epoch (3968 / 60000 train. data). Loss: 1.461350679397583\n",
      "Training log: 14 epoch (5248 / 60000 train. data). Loss: 1.461151123046875\n",
      "Training log: 14 epoch (6528 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 14 epoch (7808 / 60000 train. data). Loss: 1.4611549377441406\n",
      "Training log: 14 epoch (9088 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 14 epoch (10368 / 60000 train. data). Loss: 1.4628256559371948\n",
      "Training log: 14 epoch (11648 / 60000 train. data). Loss: 1.469037652015686\n",
      "Training log: 14 epoch (12928 / 60000 train. data). Loss: 1.4611537456512451\n",
      "Training log: 14 epoch (14208 / 60000 train. data). Loss: 1.4611519575119019\n",
      "Training log: 14 epoch (15488 / 60000 train. data). Loss: 1.4679328203201294\n",
      "Training log: 14 epoch (16768 / 60000 train. data). Loss: 1.46267831325531\n",
      "Training log: 14 epoch (18048 / 60000 train. data). Loss: 1.4690661430358887\n",
      "Training log: 14 epoch (19328 / 60000 train. data). Loss: 1.4777315855026245\n",
      "Training log: 14 epoch (20608 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 14 epoch (21888 / 60000 train. data). Loss: 1.4767417907714844\n",
      "Training log: 14 epoch (23168 / 60000 train. data). Loss: 1.4681309461593628\n",
      "Training log: 14 epoch (24448 / 60000 train. data). Loss: 1.4752912521362305\n",
      "Training log: 14 epoch (25728 / 60000 train. data). Loss: 1.4611517190933228\n",
      "Training log: 14 epoch (27008 / 60000 train. data). Loss: 1.4611775875091553\n",
      "Training log: 14 epoch (28288 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 14 epoch (29568 / 60000 train. data). Loss: 1.4689667224884033\n",
      "Training log: 14 epoch (30848 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 14 epoch (32128 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 14 epoch (33408 / 60000 train. data). Loss: 1.468096137046814\n",
      "Training log: 14 epoch (34688 / 60000 train. data). Loss: 1.4768002033233643\n",
      "Training log: 14 epoch (35968 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 14 epoch (37248 / 60000 train. data). Loss: 1.4611749649047852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 14 epoch (38528 / 60000 train. data). Loss: 1.4639580249786377\n",
      "Training log: 14 epoch (39808 / 60000 train. data). Loss: 1.4677749872207642\n",
      "Training log: 14 epoch (41088 / 60000 train. data). Loss: 1.4621386528015137\n",
      "Training log: 14 epoch (42368 / 60000 train. data). Loss: 1.4615402221679688\n",
      "Training log: 14 epoch (43648 / 60000 train. data). Loss: 1.474478006362915\n",
      "Training log: 14 epoch (44928 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 14 epoch (46208 / 60000 train. data). Loss: 1.4641741514205933\n",
      "Training log: 14 epoch (47488 / 60000 train. data). Loss: 1.4611554145812988\n",
      "Training log: 14 epoch (48768 / 60000 train. data). Loss: 1.4689648151397705\n",
      "Training log: 14 epoch (50048 / 60000 train. data). Loss: 1.4613611698150635\n",
      "Training log: 14 epoch (51328 / 60000 train. data). Loss: 1.4713462591171265\n",
      "Training log: 14 epoch (52608 / 60000 train. data). Loss: 1.4614038467407227\n",
      "Training log: 14 epoch (53888 / 60000 train. data). Loss: 1.461188793182373\n",
      "Training log: 14 epoch (55168 / 60000 train. data). Loss: 1.477506160736084\n",
      "Training log: 14 epoch (56448 / 60000 train. data). Loss: 1.468955636024475\n",
      "Training log: 14 epoch (57728 / 60000 train. data). Loss: 1.471417784690857\n",
      "Training log: 14 epoch (59008 / 60000 train. data). Loss: 1.46896493434906\n",
      "Test loss (avg): 0.011632091116905212, Accuracy: 0.9884\n",
      "Training log: 15 epoch (128 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 15 epoch (1408 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 15 epoch (2688 / 60000 train. data). Loss: 1.4611735343933105\n",
      "Training log: 15 epoch (3968 / 60000 train. data). Loss: 1.4690054655075073\n",
      "Training log: 15 epoch (5248 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 15 epoch (6528 / 60000 train. data). Loss: 1.4699527025222778\n",
      "Training log: 15 epoch (7808 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 15 epoch (9088 / 60000 train. data). Loss: 1.4612090587615967\n",
      "Training log: 15 epoch (10368 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 15 epoch (11648 / 60000 train. data). Loss: 1.469284176826477\n",
      "Training log: 15 epoch (12928 / 60000 train. data). Loss: 1.4611608982086182\n",
      "Training log: 15 epoch (14208 / 60000 train. data). Loss: 1.461239218711853\n",
      "Training log: 15 epoch (15488 / 60000 train. data). Loss: 1.468962550163269\n",
      "Training log: 15 epoch (16768 / 60000 train. data). Loss: 1.467659831047058\n",
      "Training log: 15 epoch (18048 / 60000 train. data). Loss: 1.4612048864364624\n",
      "Training log: 15 epoch (19328 / 60000 train. data). Loss: 1.4677025079727173\n",
      "Training log: 15 epoch (20608 / 60000 train. data). Loss: 1.4765291213989258\n",
      "Training log: 15 epoch (21888 / 60000 train. data). Loss: 1.46151864528656\n",
      "Training log: 15 epoch (23168 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 15 epoch (24448 / 60000 train. data). Loss: 1.461151123046875\n",
      "Training log: 15 epoch (25728 / 60000 train. data). Loss: 1.4747051000595093\n",
      "Training log: 15 epoch (27008 / 60000 train. data). Loss: 1.4770545959472656\n",
      "Training log: 15 epoch (28288 / 60000 train. data). Loss: 1.4612828493118286\n",
      "Training log: 15 epoch (29568 / 60000 train. data). Loss: 1.4748048782348633\n",
      "Training log: 15 epoch (30848 / 60000 train. data). Loss: 1.476780652999878\n",
      "Training log: 15 epoch (32128 / 60000 train. data). Loss: 1.4753684997558594\n",
      "Training log: 15 epoch (33408 / 60000 train. data). Loss: 1.462648630142212\n",
      "Training log: 15 epoch (34688 / 60000 train. data). Loss: 1.4635546207427979\n",
      "Training log: 15 epoch (35968 / 60000 train. data). Loss: 1.468538522720337\n",
      "Training log: 15 epoch (37248 / 60000 train. data). Loss: 1.4679902791976929\n",
      "Training log: 15 epoch (38528 / 60000 train. data). Loss: 1.4635475873947144\n",
      "Training log: 15 epoch (39808 / 60000 train. data). Loss: 1.4686158895492554\n",
      "Training log: 15 epoch (41088 / 60000 train. data). Loss: 1.4767636060714722\n",
      "Training log: 15 epoch (42368 / 60000 train. data). Loss: 1.4701169729232788\n",
      "Training log: 15 epoch (43648 / 60000 train. data). Loss: 1.461151361465454\n",
      "Training log: 15 epoch (44928 / 60000 train. data). Loss: 1.4691975116729736\n",
      "Training log: 15 epoch (46208 / 60000 train. data). Loss: 1.4691497087478638\n",
      "Training log: 15 epoch (47488 / 60000 train. data). Loss: 1.4680640697479248\n",
      "Training log: 15 epoch (48768 / 60000 train. data). Loss: 1.4799460172653198\n",
      "Training log: 15 epoch (50048 / 60000 train. data). Loss: 1.4638680219650269\n",
      "Training log: 15 epoch (51328 / 60000 train. data). Loss: 1.4654656648635864\n",
      "Training log: 15 epoch (52608 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 15 epoch (53888 / 60000 train. data). Loss: 1.466517448425293\n",
      "Training log: 15 epoch (55168 / 60000 train. data). Loss: 1.461167573928833\n",
      "Training log: 15 epoch (56448 / 60000 train. data). Loss: 1.4611515998840332\n",
      "Training log: 15 epoch (57728 / 60000 train. data). Loss: 1.4767800569534302\n",
      "Training log: 15 epoch (59008 / 60000 train. data). Loss: 1.4689626693725586\n",
      "Test loss (avg): 0.011633664107322693, Accuracy: 0.9883\n",
      "Training log: 16 epoch (128 / 60000 train. data). Loss: 1.4689319133758545\n",
      "Training log: 16 epoch (1408 / 60000 train. data). Loss: 1.474573016166687\n",
      "Training log: 16 epoch (2688 / 60000 train. data). Loss: 1.4768249988555908\n",
      "Training log: 16 epoch (3968 / 60000 train. data). Loss: 1.4612661600112915\n",
      "Training log: 16 epoch (5248 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 16 epoch (6528 / 60000 train. data). Loss: 1.476470947265625\n",
      "Training log: 16 epoch (7808 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 16 epoch (9088 / 60000 train. data). Loss: 1.4689685106277466\n",
      "Training log: 16 epoch (10368 / 60000 train. data). Loss: 1.4612765312194824\n",
      "Training log: 16 epoch (11648 / 60000 train. data). Loss: 1.4625343084335327\n",
      "Training log: 16 epoch (12928 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 16 epoch (14208 / 60000 train. data). Loss: 1.4807137250900269\n",
      "Training log: 16 epoch (15488 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 16 epoch (16768 / 60000 train. data). Loss: 1.4612400531768799\n",
      "Training log: 16 epoch (18048 / 60000 train. data). Loss: 1.4612458944320679\n",
      "Training log: 16 epoch (19328 / 60000 train. data). Loss: 1.4611550569534302\n",
      "Training log: 16 epoch (20608 / 60000 train. data). Loss: 1.4687763452529907\n",
      "Training log: 16 epoch (21888 / 60000 train. data). Loss: 1.4679780006408691\n",
      "Training log: 16 epoch (23168 / 60000 train. data). Loss: 1.4687418937683105\n",
      "Training log: 16 epoch (24448 / 60000 train. data). Loss: 1.4611575603485107\n",
      "Training log: 16 epoch (25728 / 60000 train. data). Loss: 1.465083122253418\n",
      "Training log: 16 epoch (27008 / 60000 train. data). Loss: 1.4611519575119019\n",
      "Training log: 16 epoch (28288 / 60000 train. data). Loss: 1.473725438117981\n",
      "Training log: 16 epoch (29568 / 60000 train. data). Loss: 1.477220058441162\n",
      "Training log: 16 epoch (30848 / 60000 train. data). Loss: 1.468221664428711\n",
      "Training log: 16 epoch (32128 / 60000 train. data). Loss: 1.4611613750457764\n",
      "Training log: 16 epoch (33408 / 60000 train. data). Loss: 1.4795390367507935\n",
      "Training log: 16 epoch (34688 / 60000 train. data). Loss: 1.4668223857879639\n",
      "Training log: 16 epoch (35968 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 16 epoch (37248 / 60000 train. data). Loss: 1.464004635810852\n",
      "Training log: 16 epoch (38528 / 60000 train. data). Loss: 1.4742997884750366\n",
      "Training log: 16 epoch (39808 / 60000 train. data). Loss: 1.4659548997879028\n",
      "Training log: 16 epoch (41088 / 60000 train. data). Loss: 1.4673128128051758\n",
      "Training log: 16 epoch (42368 / 60000 train. data). Loss: 1.4611523151397705\n",
      "Training log: 16 epoch (43648 / 60000 train. data). Loss: 1.4767491817474365\n",
      "Training log: 16 epoch (44928 / 60000 train. data). Loss: 1.46882963180542\n",
      "Training log: 16 epoch (46208 / 60000 train. data). Loss: 1.4689631462097168\n",
      "Training log: 16 epoch (47488 / 60000 train. data). Loss: 1.4690325260162354\n",
      "Training log: 16 epoch (48768 / 60000 train. data). Loss: 1.4751265048980713\n",
      "Training log: 16 epoch (50048 / 60000 train. data). Loss: 1.4611550569534302\n",
      "Training log: 16 epoch (51328 / 60000 train. data). Loss: 1.4703036546707153\n",
      "Training log: 16 epoch (52608 / 60000 train. data). Loss: 1.4611754417419434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 16 epoch (53888 / 60000 train. data). Loss: 1.46159029006958\n",
      "Training log: 16 epoch (55168 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 16 epoch (56448 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 16 epoch (57728 / 60000 train. data). Loss: 1.4767643213272095\n",
      "Training log: 16 epoch (59008 / 60000 train. data). Loss: 1.4671745300292969\n",
      "Test loss (avg): 0.011663127517700196, Accuracy: 0.9845\n",
      "Training log: 17 epoch (128 / 60000 train. data). Loss: 1.476001501083374\n",
      "Training log: 17 epoch (1408 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (2688 / 60000 train. data). Loss: 1.4611530303955078\n",
      "Training log: 17 epoch (3968 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (5248 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 17 epoch (6528 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (7808 / 60000 train. data). Loss: 1.4611530303955078\n",
      "Training log: 17 epoch (9088 / 60000 train. data). Loss: 1.4693913459777832\n",
      "Training log: 17 epoch (10368 / 60000 train. data). Loss: 1.4611878395080566\n",
      "Training log: 17 epoch (11648 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (12928 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (14208 / 60000 train. data). Loss: 1.4688878059387207\n",
      "Training log: 17 epoch (15488 / 60000 train. data). Loss: 1.4689607620239258\n",
      "Training log: 17 epoch (16768 / 60000 train. data). Loss: 1.4612312316894531\n",
      "Training log: 17 epoch (18048 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (19328 / 60000 train. data). Loss: 1.4623645544052124\n",
      "Training log: 17 epoch (20608 / 60000 train. data). Loss: 1.468961238861084\n",
      "Training log: 17 epoch (21888 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (23168 / 60000 train. data). Loss: 1.4773114919662476\n",
      "Training log: 17 epoch (24448 / 60000 train. data). Loss: 1.4746365547180176\n",
      "Training log: 17 epoch (25728 / 60000 train. data). Loss: 1.4763143062591553\n",
      "Training log: 17 epoch (27008 / 60000 train. data). Loss: 1.4689762592315674\n",
      "Training log: 17 epoch (28288 / 60000 train. data). Loss: 1.461150884628296\n",
      "Training log: 17 epoch (29568 / 60000 train. data). Loss: 1.4617706537246704\n",
      "Training log: 17 epoch (30848 / 60000 train. data). Loss: 1.461744785308838\n",
      "Training log: 17 epoch (32128 / 60000 train. data). Loss: 1.4611635208129883\n",
      "Training log: 17 epoch (33408 / 60000 train. data). Loss: 1.4611563682556152\n",
      "Training log: 17 epoch (34688 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (35968 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (37248 / 60000 train. data). Loss: 1.4611698389053345\n",
      "Training log: 17 epoch (38528 / 60000 train. data). Loss: 1.4689605236053467\n",
      "Training log: 17 epoch (39808 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 17 epoch (41088 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 17 epoch (42368 / 60000 train. data). Loss: 1.4612478017807007\n",
      "Training log: 17 epoch (43648 / 60000 train. data). Loss: 1.4689640998840332\n",
      "Training log: 17 epoch (44928 / 60000 train. data). Loss: 1.467411756515503\n",
      "Training log: 17 epoch (46208 / 60000 train. data). Loss: 1.469051718711853\n",
      "Training log: 17 epoch (47488 / 60000 train. data). Loss: 1.4781862497329712\n",
      "Training log: 17 epoch (48768 / 60000 train. data). Loss: 1.4620976448059082\n",
      "Training log: 17 epoch (50048 / 60000 train. data). Loss: 1.4689663648605347\n",
      "Training log: 17 epoch (51328 / 60000 train. data). Loss: 1.4613386392593384\n",
      "Training log: 17 epoch (52608 / 60000 train. data). Loss: 1.4758087396621704\n",
      "Training log: 17 epoch (53888 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (55168 / 60000 train. data). Loss: 1.4611743688583374\n",
      "Training log: 17 epoch (56448 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 17 epoch (57728 / 60000 train. data). Loss: 1.4732648134231567\n",
      "Training log: 17 epoch (59008 / 60000 train. data). Loss: 1.4845820665359497\n",
      "Test loss (avg): 0.011635925495624541, Accuracy: 0.9883\n",
      "Training log: 18 epoch (128 / 60000 train. data). Loss: 1.4768028259277344\n",
      "Training log: 18 epoch (1408 / 60000 train. data). Loss: 1.4611563682556152\n",
      "Training log: 18 epoch (2688 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 18 epoch (3968 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 18 epoch (5248 / 60000 train. data). Loss: 1.4690076112747192\n",
      "Training log: 18 epoch (6528 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 18 epoch (7808 / 60000 train. data). Loss: 1.4719772338867188\n",
      "Training log: 18 epoch (9088 / 60000 train. data). Loss: 1.461150884628296\n",
      "Training log: 18 epoch (10368 / 60000 train. data). Loss: 1.4696094989776611\n",
      "Training log: 18 epoch (11648 / 60000 train. data). Loss: 1.468275547027588\n",
      "Training log: 18 epoch (12928 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 18 epoch (14208 / 60000 train. data). Loss: 1.4700101613998413\n",
      "Training log: 18 epoch (15488 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 18 epoch (16768 / 60000 train. data). Loss: 1.468245029449463\n",
      "Training log: 18 epoch (18048 / 60000 train. data). Loss: 1.46115243434906\n",
      "Training log: 18 epoch (19328 / 60000 train. data). Loss: 1.468964695930481\n",
      "Training log: 18 epoch (20608 / 60000 train. data). Loss: 1.4611567258834839\n",
      "Training log: 18 epoch (21888 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 18 epoch (23168 / 60000 train. data). Loss: 1.4611550569534302\n",
      "Training log: 18 epoch (24448 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 18 epoch (25728 / 60000 train. data). Loss: 1.461158037185669\n",
      "Training log: 18 epoch (27008 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 18 epoch (28288 / 60000 train. data). Loss: 1.4767757654190063\n",
      "Training log: 18 epoch (29568 / 60000 train. data). Loss: 1.4620447158813477\n",
      "Training log: 18 epoch (30848 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 18 epoch (32128 / 60000 train. data). Loss: 1.4611510038375854\n",
      "Training log: 18 epoch (33408 / 60000 train. data). Loss: 1.468737244606018\n",
      "Training log: 18 epoch (34688 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 18 epoch (35968 / 60000 train. data). Loss: 1.461150884628296\n",
      "Training log: 18 epoch (37248 / 60000 train. data). Loss: 1.461460828781128\n",
      "Training log: 18 epoch (38528 / 60000 train. data). Loss: 1.4689826965332031\n",
      "Training log: 18 epoch (39808 / 60000 train. data). Loss: 1.4668595790863037\n",
      "Training log: 18 epoch (41088 / 60000 train. data). Loss: 1.4754526615142822\n",
      "Training log: 18 epoch (42368 / 60000 train. data). Loss: 1.4662373065948486\n",
      "Training log: 18 epoch (43648 / 60000 train. data). Loss: 1.468872308731079\n",
      "Training log: 18 epoch (44928 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 18 epoch (46208 / 60000 train. data). Loss: 1.461154818534851\n",
      "Training log: 18 epoch (47488 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 18 epoch (48768 / 60000 train. data). Loss: 1.4636647701263428\n",
      "Training log: 18 epoch (50048 / 60000 train. data). Loss: 1.4897977113723755\n",
      "Training log: 18 epoch (51328 / 60000 train. data). Loss: 1.461165428161621\n",
      "Training log: 18 epoch (52608 / 60000 train. data). Loss: 1.4700028896331787\n",
      "Training log: 18 epoch (53888 / 60000 train. data). Loss: 1.46115243434906\n",
      "Training log: 18 epoch (55168 / 60000 train. data). Loss: 1.4611510038375854\n",
      "Training log: 18 epoch (56448 / 60000 train. data). Loss: 1.4676799774169922\n",
      "Training log: 18 epoch (57728 / 60000 train. data). Loss: 1.4643970727920532\n",
      "Training log: 18 epoch (59008 / 60000 train. data). Loss: 1.4612252712249756\n",
      "Test loss (avg): 0.011641063368320465, Accuracy: 0.9872\n",
      "Training log: 19 epoch (128 / 60000 train. data). Loss: 1.4691263437271118\n",
      "Training log: 19 epoch (1408 / 60000 train. data). Loss: 1.4642421007156372\n",
      "Training log: 19 epoch (2688 / 60000 train. data). Loss: 1.4767756462097168\n",
      "Training log: 19 epoch (3968 / 60000 train. data). Loss: 1.4611544609069824\n",
      "Training log: 19 epoch (5248 / 60000 train. data). Loss: 1.4688881635665894\n",
      "Training log: 19 epoch (6528 / 60000 train. data). Loss: 1.461164951324463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 19 epoch (7808 / 60000 train. data). Loss: 1.4611598253250122\n",
      "Training log: 19 epoch (9088 / 60000 train. data). Loss: 1.461150884628296\n",
      "Training log: 19 epoch (10368 / 60000 train. data). Loss: 1.462211012840271\n",
      "Training log: 19 epoch (11648 / 60000 train. data). Loss: 1.4611510038375854\n",
      "Training log: 19 epoch (12928 / 60000 train. data). Loss: 1.4689642190933228\n",
      "Training log: 19 epoch (14208 / 60000 train. data). Loss: 1.4680237770080566\n",
      "Training log: 19 epoch (15488 / 60000 train. data). Loss: 1.4764125347137451\n",
      "Training log: 19 epoch (16768 / 60000 train. data). Loss: 1.4611750841140747\n",
      "Training log: 19 epoch (18048 / 60000 train. data). Loss: 1.4690555334091187\n",
      "Training log: 19 epoch (19328 / 60000 train. data). Loss: 1.4611549377441406\n",
      "Training log: 19 epoch (20608 / 60000 train. data). Loss: 1.470973253250122\n",
      "Training log: 19 epoch (21888 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 19 epoch (23168 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 19 epoch (24448 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 19 epoch (25728 / 60000 train. data). Loss: 1.461423397064209\n",
      "Training log: 19 epoch (27008 / 60000 train. data). Loss: 1.4740872383117676\n",
      "Training log: 19 epoch (28288 / 60000 train. data). Loss: 1.4612095355987549\n",
      "Training log: 19 epoch (29568 / 60000 train. data). Loss: 1.461151123046875\n",
      "Training log: 19 epoch (30848 / 60000 train. data). Loss: 1.4965064525604248\n",
      "Training log: 19 epoch (32128 / 60000 train. data). Loss: 1.4687328338623047\n",
      "Training log: 19 epoch (33408 / 60000 train. data). Loss: 1.4611512422561646\n",
      "Training log: 19 epoch (34688 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 19 epoch (35968 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 19 epoch (37248 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 19 epoch (38528 / 60000 train. data). Loss: 1.462287425994873\n",
      "Training log: 19 epoch (39808 / 60000 train. data). Loss: 1.4617094993591309\n",
      "Training log: 19 epoch (41088 / 60000 train. data). Loss: 1.4618927240371704\n",
      "Training log: 19 epoch (42368 / 60000 train. data). Loss: 1.4612163305282593\n",
      "Training log: 19 epoch (43648 / 60000 train. data). Loss: 1.4611574411392212\n",
      "Training log: 19 epoch (44928 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 19 epoch (46208 / 60000 train. data). Loss: 1.4770822525024414\n",
      "Training log: 19 epoch (47488 / 60000 train. data). Loss: 1.4618425369262695\n",
      "Training log: 19 epoch (48768 / 60000 train. data). Loss: 1.4618443250656128\n",
      "Training log: 19 epoch (50048 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 19 epoch (51328 / 60000 train. data). Loss: 1.476775050163269\n",
      "Training log: 19 epoch (52608 / 60000 train. data). Loss: 1.4643501043319702\n",
      "Training log: 19 epoch (53888 / 60000 train. data). Loss: 1.461150884628296\n",
      "Training log: 19 epoch (55168 / 60000 train. data). Loss: 1.4611550569534302\n",
      "Training log: 19 epoch (56448 / 60000 train. data). Loss: 1.4611835479736328\n",
      "Training log: 19 epoch (57728 / 60000 train. data). Loss: 1.4641408920288086\n",
      "Training log: 19 epoch (59008 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Test loss (avg): 0.01162559494972229, Accuracy: 0.9894\n",
      "Training log: 20 epoch (128 / 60000 train. data). Loss: 1.4611514806747437\n",
      "Training log: 20 epoch (1408 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (2688 / 60000 train. data). Loss: 1.461155652999878\n",
      "Training log: 20 epoch (3968 / 60000 train. data). Loss: 1.461150884628296\n",
      "Training log: 20 epoch (5248 / 60000 train. data). Loss: 1.4767756462097168\n",
      "Training log: 20 epoch (6528 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (7808 / 60000 train. data). Loss: 1.4766474962234497\n",
      "Training log: 20 epoch (9088 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (10368 / 60000 train. data). Loss: 1.4611512422561646\n",
      "Training log: 20 epoch (11648 / 60000 train. data). Loss: 1.4639270305633545\n",
      "Training log: 20 epoch (12928 / 60000 train. data). Loss: 1.461152195930481\n",
      "Training log: 20 epoch (14208 / 60000 train. data). Loss: 1.4611525535583496\n",
      "Training log: 20 epoch (15488 / 60000 train. data). Loss: 1.4689631462097168\n",
      "Training log: 20 epoch (16768 / 60000 train. data). Loss: 1.472039818763733\n",
      "Training log: 20 epoch (18048 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (19328 / 60000 train. data). Loss: 1.4792001247406006\n",
      "Training log: 20 epoch (20608 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (21888 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (23168 / 60000 train. data). Loss: 1.461151123046875\n",
      "Training log: 20 epoch (24448 / 60000 train. data). Loss: 1.4611526727676392\n",
      "Training log: 20 epoch (25728 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (27008 / 60000 train. data). Loss: 1.461161494255066\n",
      "Training log: 20 epoch (28288 / 60000 train. data). Loss: 1.4620050191879272\n",
      "Training log: 20 epoch (29568 / 60000 train. data). Loss: 1.4617488384246826\n",
      "Training log: 20 epoch (30848 / 60000 train. data). Loss: 1.4649971723556519\n",
      "Training log: 20 epoch (32128 / 60000 train. data). Loss: 1.4616416692733765\n",
      "Training log: 20 epoch (33408 / 60000 train. data). Loss: 1.468963384628296\n",
      "Training log: 20 epoch (34688 / 60000 train. data). Loss: 1.4736114740371704\n",
      "Training log: 20 epoch (35968 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (37248 / 60000 train. data). Loss: 1.4689630270004272\n",
      "Training log: 20 epoch (38528 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 20 epoch (39808 / 60000 train. data). Loss: 1.476775050163269\n",
      "Training log: 20 epoch (41088 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (42368 / 60000 train. data). Loss: 1.4689632654190063\n",
      "Training log: 20 epoch (43648 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (44928 / 60000 train. data). Loss: 1.4612035751342773\n",
      "Training log: 20 epoch (46208 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (47488 / 60000 train. data). Loss: 1.4611656665802002\n",
      "Training log: 20 epoch (48768 / 60000 train. data). Loss: 1.4611622095108032\n",
      "Training log: 20 epoch (50048 / 60000 train. data). Loss: 1.4611525535583496\n",
      "Training log: 20 epoch (51328 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (52608 / 60000 train. data). Loss: 1.4614195823669434\n",
      "Training log: 20 epoch (53888 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (55168 / 60000 train. data). Loss: 1.4611629247665405\n",
      "Training log: 20 epoch (56448 / 60000 train. data). Loss: 1.4611507654190063\n",
      "Training log: 20 epoch (57728 / 60000 train. data). Loss: 1.468563199043274\n",
      "Training log: 20 epoch (59008 / 60000 train. data). Loss: 1.4613069295883179\n",
      "Test loss (avg): 0.011630372655391694, Accuracy: 0.9886\n"
     ]
    }
   ],
   "source": [
    "for i_epoch in range(num_epoch):\n",
    "    loss = None\n",
    "    \n",
    "    net.train(True)\n",
    "    \n",
    "    for i, (data, target) in enumerate(loaders['train']):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        \n",
    "#         print(output.shape)\n",
    "        \n",
    "#         loss = f.nll_loss(output, target)\n",
    "        loss = criterion(output, target)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(i_epoch+1, (i+1)*128, loss.item()))\n",
    "    \n",
    "    history['train_loss'].append(loss.item())\n",
    "    \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = net(data)\n",
    "            loss = criterion(output, target)\n",
    "#             test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= 10000\n",
    "    print('Test loss (avg): {}, Accuracy: {}'.format(test_loss, correct / 10000))\n",
    "    \n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(correct / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [1.4981040954589844, 1.485031008720398, 1.467021107673645, 1.4827076196670532, 1.4611520767211914, 1.4818629026412964, 1.4726911783218384, 1.470118522644043, 1.4715527296066284, 1.4613404273986816, 1.4756399393081665, 1.467373251914978, 1.4629465341567993, 1.4734553098678589, 1.470405101776123, 1.4611841440200806, 1.4700579643249512, 1.4612990617752075, 1.4611567258834839, 1.4611505270004272], 'test_loss': [0.011788892793655395, 0.011711004602909088, 0.011660100686550141, 0.011666324365139008, 0.011649852967262267, 0.011662299585342407, 0.011646606171131134, 0.01163369289636612, 0.01162231525182724, 0.011644085478782654, 0.01165306967496872, 0.011665533518791198, 0.011630427920818328, 0.011632091116905212, 0.011633664107322693, 0.011663127517700196, 0.011635925495624541, 0.011641063368320465, 0.01162559494972229, 0.011630372655391694], 'test_acc': [0.9714, 0.9803, 0.9855, 0.9845, 0.9866, 0.9848, 0.9872, 0.9885, 0.9895, 0.9878, 0.9857, 0.9846, 0.989, 0.9884, 0.9883, 0.9845, 0.9883, 0.9872, 0.9894, 0.9886]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epoch\n",
    "print(history)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['train_loss'])), history['train_loss'], label='train_loss')\n",
    "plt.plot(range(len(history['test_loss'])), history['test_loss'], label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('cnnloss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['test_acc'])), history['test_acc'])\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('cnn_test_acc2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
