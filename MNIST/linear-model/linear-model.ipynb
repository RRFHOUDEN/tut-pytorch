{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rightcode.co.jp/blog/information-technology/pytorch-mnist-learning\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return f.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    return {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20\n",
    "history = {\n",
    "    'train_loss':[],\n",
    "    'test_loss':[],\n",
    "    'test_acc':[]\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = MyNet().to(device)\n",
    "loaders = load_MNIST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 2.3742623329162598\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 1.8330284357070923\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 1.4400423765182495\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 1.110687017440796\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.8526275157928467\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.7417600154876709\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.6191973686218262\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.6009341478347778\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.4397282898426056\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.48753660917282104\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.508772075176239\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.44476425647735596\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.44303882122039795\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.46643906831741333\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.42294129729270935\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.3369057774543762\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.4093894362449646\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.36369413137435913\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.4509575068950653\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.5464611053466797\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.3559792637825012\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.3510459065437317\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.2952423691749573\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.35691776871681213\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.2983021140098572\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.3329664170742035\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.31578612327575684\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.27572667598724365\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.44584500789642334\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.512673556804657\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.33125701546669006\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.2344924807548523\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.433600515127182\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.3512301743030548\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.3270531892776489\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.2853500247001648\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.26165780425071716\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.38620245456695557\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.2257842719554901\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.23520903289318085\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.28953808546066284\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.46786731481552124\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.17074967920780182\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.16577167809009552\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.2405283898115158\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.3108469843864441\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.2618568241596222\n",
      "Test loss (avg): 0.26288632473945617, Accuracy: 0.9229\n",
      "Training log: 2 epoch (128 / 60000 train. data). Loss: 0.20144623517990112\n",
      "Training log: 2 epoch (1408 / 60000 train. data). Loss: 0.2475382387638092\n",
      "Training log: 2 epoch (2688 / 60000 train. data). Loss: 0.20936313271522522\n",
      "Training log: 2 epoch (3968 / 60000 train. data). Loss: 0.2599003314971924\n",
      "Training log: 2 epoch (5248 / 60000 train. data). Loss: 0.26732438802719116\n",
      "Training log: 2 epoch (6528 / 60000 train. data). Loss: 0.18905113637447357\n",
      "Training log: 2 epoch (7808 / 60000 train. data). Loss: 0.26469236612319946\n",
      "Training log: 2 epoch (9088 / 60000 train. data). Loss: 0.3134848475456238\n",
      "Training log: 2 epoch (10368 / 60000 train. data). Loss: 0.2986007332801819\n",
      "Training log: 2 epoch (11648 / 60000 train. data). Loss: 0.3546788692474365\n",
      "Training log: 2 epoch (12928 / 60000 train. data). Loss: 0.2562526762485504\n",
      "Training log: 2 epoch (14208 / 60000 train. data). Loss: 0.21362636983394623\n",
      "Training log: 2 epoch (15488 / 60000 train. data). Loss: 0.2668789327144623\n",
      "Training log: 2 epoch (16768 / 60000 train. data). Loss: 0.19071736931800842\n",
      "Training log: 2 epoch (18048 / 60000 train. data). Loss: 0.28300905227661133\n",
      "Training log: 2 epoch (19328 / 60000 train. data). Loss: 0.19694317877292633\n",
      "Training log: 2 epoch (20608 / 60000 train. data). Loss: 0.1637965589761734\n",
      "Training log: 2 epoch (21888 / 60000 train. data). Loss: 0.20501847565174103\n",
      "Training log: 2 epoch (23168 / 60000 train. data). Loss: 0.14582684636116028\n",
      "Training log: 2 epoch (24448 / 60000 train. data). Loss: 0.4353240132331848\n",
      "Training log: 2 epoch (25728 / 60000 train. data). Loss: 0.22342383861541748\n",
      "Training log: 2 epoch (27008 / 60000 train. data). Loss: 0.148643359541893\n",
      "Training log: 2 epoch (28288 / 60000 train. data). Loss: 0.1761447787284851\n",
      "Training log: 2 epoch (29568 / 60000 train. data). Loss: 0.2549212574958801\n",
      "Training log: 2 epoch (30848 / 60000 train. data). Loss: 0.2553207278251648\n",
      "Training log: 2 epoch (32128 / 60000 train. data). Loss: 0.20397143065929413\n",
      "Training log: 2 epoch (33408 / 60000 train. data). Loss: 0.1695331335067749\n",
      "Training log: 2 epoch (34688 / 60000 train. data). Loss: 0.3210950493812561\n",
      "Training log: 2 epoch (35968 / 60000 train. data). Loss: 0.1574227213859558\n",
      "Training log: 2 epoch (37248 / 60000 train. data). Loss: 0.2801041007041931\n",
      "Training log: 2 epoch (38528 / 60000 train. data). Loss: 0.23489822447299957\n",
      "Training log: 2 epoch (39808 / 60000 train. data). Loss: 0.18403954803943634\n",
      "Training log: 2 epoch (41088 / 60000 train. data). Loss: 0.2484796643257141\n",
      "Training log: 2 epoch (42368 / 60000 train. data). Loss: 0.38679853081703186\n",
      "Training log: 2 epoch (43648 / 60000 train. data). Loss: 0.15316185355186462\n",
      "Training log: 2 epoch (44928 / 60000 train. data). Loss: 0.2268550544977188\n",
      "Training log: 2 epoch (46208 / 60000 train. data). Loss: 0.20984870195388794\n",
      "Training log: 2 epoch (47488 / 60000 train. data). Loss: 0.17061977088451385\n",
      "Training log: 2 epoch (48768 / 60000 train. data). Loss: 0.3211718499660492\n",
      "Training log: 2 epoch (50048 / 60000 train. data). Loss: 0.2087276726961136\n",
      "Training log: 2 epoch (51328 / 60000 train. data). Loss: 0.26716098189353943\n",
      "Training log: 2 epoch (52608 / 60000 train. data). Loss: 0.1874735802412033\n",
      "Training log: 2 epoch (53888 / 60000 train. data). Loss: 0.23779873549938202\n",
      "Training log: 2 epoch (55168 / 60000 train. data). Loss: 0.25649550557136536\n",
      "Training log: 2 epoch (56448 / 60000 train. data). Loss: 0.23985527455806732\n",
      "Training log: 2 epoch (57728 / 60000 train. data). Loss: 0.1560382843017578\n",
      "Training log: 2 epoch (59008 / 60000 train. data). Loss: 0.1698085516691208\n",
      "Test loss (avg): 0.19992919902801515, Accuracy: 0.9411\n",
      "Training log: 3 epoch (128 / 60000 train. data). Loss: 0.26830315589904785\n",
      "Training log: 3 epoch (1408 / 60000 train. data). Loss: 0.23217788338661194\n",
      "Training log: 3 epoch (2688 / 60000 train. data). Loss: 0.17955580353736877\n",
      "Training log: 3 epoch (3968 / 60000 train. data). Loss: 0.26414456963539124\n",
      "Training log: 3 epoch (5248 / 60000 train. data). Loss: 0.23001685738563538\n",
      "Training log: 3 epoch (6528 / 60000 train. data). Loss: 0.13759064674377441\n",
      "Training log: 3 epoch (7808 / 60000 train. data). Loss: 0.19243046641349792\n",
      "Training log: 3 epoch (9088 / 60000 train. data). Loss: 0.06856804341077805\n",
      "Training log: 3 epoch (10368 / 60000 train. data). Loss: 0.12101756781339645\n",
      "Training log: 3 epoch (11648 / 60000 train. data). Loss: 0.22260147333145142\n",
      "Training log: 3 epoch (12928 / 60000 train. data). Loss: 0.1837468445301056\n",
      "Training log: 3 epoch (14208 / 60000 train. data). Loss: 0.19801020622253418\n",
      "Training log: 3 epoch (15488 / 60000 train. data). Loss: 0.1586698442697525\n",
      "Training log: 3 epoch (16768 / 60000 train. data). Loss: 0.1990649551153183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 3 epoch (18048 / 60000 train. data). Loss: 0.25662538409233093\n",
      "Training log: 3 epoch (19328 / 60000 train. data). Loss: 0.2523912787437439\n",
      "Training log: 3 epoch (20608 / 60000 train. data). Loss: 0.12961171567440033\n",
      "Training log: 3 epoch (21888 / 60000 train. data). Loss: 0.16393984854221344\n",
      "Training log: 3 epoch (23168 / 60000 train. data). Loss: 0.13004344701766968\n",
      "Training log: 3 epoch (24448 / 60000 train. data). Loss: 0.22103813290596008\n",
      "Training log: 3 epoch (25728 / 60000 train. data). Loss: 0.2638709247112274\n",
      "Training log: 3 epoch (27008 / 60000 train. data). Loss: 0.19675329327583313\n",
      "Training log: 3 epoch (28288 / 60000 train. data). Loss: 0.3017708361148834\n",
      "Training log: 3 epoch (29568 / 60000 train. data). Loss: 0.1567334532737732\n",
      "Training log: 3 epoch (30848 / 60000 train. data). Loss: 0.2551848888397217\n",
      "Training log: 3 epoch (32128 / 60000 train. data). Loss: 0.2962338328361511\n",
      "Training log: 3 epoch (33408 / 60000 train. data). Loss: 0.22421838343143463\n",
      "Training log: 3 epoch (34688 / 60000 train. data). Loss: 0.13544614613056183\n",
      "Training log: 3 epoch (35968 / 60000 train. data). Loss: 0.17868578433990479\n",
      "Training log: 3 epoch (37248 / 60000 train. data). Loss: 0.2738773226737976\n",
      "Training log: 3 epoch (38528 / 60000 train. data). Loss: 0.14345304667949677\n",
      "Training log: 3 epoch (39808 / 60000 train. data). Loss: 0.18867725133895874\n",
      "Training log: 3 epoch (41088 / 60000 train. data). Loss: 0.19123965501785278\n",
      "Training log: 3 epoch (42368 / 60000 train. data). Loss: 0.18312394618988037\n",
      "Training log: 3 epoch (43648 / 60000 train. data). Loss: 0.2746660113334656\n",
      "Training log: 3 epoch (44928 / 60000 train. data). Loss: 0.168299600481987\n",
      "Training log: 3 epoch (46208 / 60000 train. data). Loss: 0.08050753176212311\n",
      "Training log: 3 epoch (47488 / 60000 train. data). Loss: 0.1287437230348587\n",
      "Training log: 3 epoch (48768 / 60000 train. data). Loss: 0.1903272569179535\n",
      "Training log: 3 epoch (50048 / 60000 train. data). Loss: 0.09406605362892151\n",
      "Training log: 3 epoch (51328 / 60000 train. data). Loss: 0.0829853042960167\n",
      "Training log: 3 epoch (52608 / 60000 train. data). Loss: 0.12115222215652466\n",
      "Training log: 3 epoch (53888 / 60000 train. data). Loss: 0.10901486128568649\n",
      "Training log: 3 epoch (55168 / 60000 train. data). Loss: 0.11951704323291779\n",
      "Training log: 3 epoch (56448 / 60000 train. data). Loss: 0.10677970945835114\n",
      "Training log: 3 epoch (57728 / 60000 train. data). Loss: 0.15913043916225433\n",
      "Training log: 3 epoch (59008 / 60000 train. data). Loss: 0.16092950105667114\n",
      "Test loss (avg): 0.16125908455848695, Accuracy: 0.9501\n",
      "Training log: 4 epoch (128 / 60000 train. data). Loss: 0.1354973167181015\n",
      "Training log: 4 epoch (1408 / 60000 train. data). Loss: 0.13327465951442719\n",
      "Training log: 4 epoch (2688 / 60000 train. data). Loss: 0.17349585890769958\n",
      "Training log: 4 epoch (3968 / 60000 train. data). Loss: 0.11393095552921295\n",
      "Training log: 4 epoch (5248 / 60000 train. data). Loss: 0.1574353575706482\n",
      "Training log: 4 epoch (6528 / 60000 train. data). Loss: 0.13237059116363525\n",
      "Training log: 4 epoch (7808 / 60000 train. data). Loss: 0.156256303191185\n",
      "Training log: 4 epoch (9088 / 60000 train. data). Loss: 0.11400865763425827\n",
      "Training log: 4 epoch (10368 / 60000 train. data). Loss: 0.2162613719701767\n",
      "Training log: 4 epoch (11648 / 60000 train. data). Loss: 0.10847274959087372\n",
      "Training log: 4 epoch (12928 / 60000 train. data). Loss: 0.20135731995105743\n",
      "Training log: 4 epoch (14208 / 60000 train. data). Loss: 0.10574378818273544\n",
      "Training log: 4 epoch (15488 / 60000 train. data). Loss: 0.18927961587905884\n",
      "Training log: 4 epoch (16768 / 60000 train. data). Loss: 0.06525355577468872\n",
      "Training log: 4 epoch (18048 / 60000 train. data). Loss: 0.19447481632232666\n",
      "Training log: 4 epoch (19328 / 60000 train. data). Loss: 0.10806596279144287\n",
      "Training log: 4 epoch (20608 / 60000 train. data). Loss: 0.16503232717514038\n",
      "Training log: 4 epoch (21888 / 60000 train. data). Loss: 0.17353489995002747\n",
      "Training log: 4 epoch (23168 / 60000 train. data). Loss: 0.09104957431554794\n",
      "Training log: 4 epoch (24448 / 60000 train. data). Loss: 0.1498202085494995\n",
      "Training log: 4 epoch (25728 / 60000 train. data). Loss: 0.2032233029603958\n",
      "Training log: 4 epoch (27008 / 60000 train. data). Loss: 0.13680613040924072\n",
      "Training log: 4 epoch (28288 / 60000 train. data). Loss: 0.19465918838977814\n",
      "Training log: 4 epoch (29568 / 60000 train. data). Loss: 0.12228920310735703\n",
      "Training log: 4 epoch (30848 / 60000 train. data). Loss: 0.21295367181301117\n",
      "Training log: 4 epoch (32128 / 60000 train. data). Loss: 0.12491322308778763\n",
      "Training log: 4 epoch (33408 / 60000 train. data). Loss: 0.15167514979839325\n",
      "Training log: 4 epoch (34688 / 60000 train. data). Loss: 0.07806707918643951\n",
      "Training log: 4 epoch (35968 / 60000 train. data). Loss: 0.1042039543390274\n",
      "Training log: 4 epoch (37248 / 60000 train. data). Loss: 0.11606146395206451\n",
      "Training log: 4 epoch (38528 / 60000 train. data). Loss: 0.16903488337993622\n",
      "Training log: 4 epoch (39808 / 60000 train. data). Loss: 0.13083243370056152\n",
      "Training log: 4 epoch (41088 / 60000 train. data). Loss: 0.21022062003612518\n",
      "Training log: 4 epoch (42368 / 60000 train. data). Loss: 0.12772464752197266\n",
      "Training log: 4 epoch (43648 / 60000 train. data). Loss: 0.20728713274002075\n",
      "Training log: 4 epoch (44928 / 60000 train. data). Loss: 0.04564795270562172\n",
      "Training log: 4 epoch (46208 / 60000 train. data). Loss: 0.15523181855678558\n",
      "Training log: 4 epoch (47488 / 60000 train. data). Loss: 0.11367599666118622\n",
      "Training log: 4 epoch (48768 / 60000 train. data). Loss: 0.11032938957214355\n",
      "Training log: 4 epoch (50048 / 60000 train. data). Loss: 0.08025085926055908\n",
      "Training log: 4 epoch (51328 / 60000 train. data). Loss: 0.0946006029844284\n",
      "Training log: 4 epoch (52608 / 60000 train. data). Loss: 0.14765006303787231\n",
      "Training log: 4 epoch (53888 / 60000 train. data). Loss: 0.23817047476768494\n",
      "Training log: 4 epoch (55168 / 60000 train. data). Loss: 0.08915221691131592\n",
      "Training log: 4 epoch (56448 / 60000 train. data). Loss: 0.14965984225273132\n",
      "Training log: 4 epoch (57728 / 60000 train. data). Loss: 0.12625668942928314\n",
      "Training log: 4 epoch (59008 / 60000 train. data). Loss: 0.14835478365421295\n",
      "Test loss (avg): 0.1258249189376831, Accuracy: 0.962\n",
      "Training log: 5 epoch (128 / 60000 train. data). Loss: 0.15008693933486938\n",
      "Training log: 5 epoch (1408 / 60000 train. data). Loss: 0.17647606134414673\n",
      "Training log: 5 epoch (2688 / 60000 train. data). Loss: 0.1612381935119629\n",
      "Training log: 5 epoch (3968 / 60000 train. data). Loss: 0.16410282254219055\n",
      "Training log: 5 epoch (5248 / 60000 train. data). Loss: 0.1447950005531311\n",
      "Training log: 5 epoch (6528 / 60000 train. data). Loss: 0.11492188274860382\n",
      "Training log: 5 epoch (7808 / 60000 train. data). Loss: 0.0462937094271183\n",
      "Training log: 5 epoch (9088 / 60000 train. data). Loss: 0.1372433453798294\n",
      "Training log: 5 epoch (10368 / 60000 train. data). Loss: 0.14666208624839783\n",
      "Training log: 5 epoch (11648 / 60000 train. data). Loss: 0.08140793442726135\n",
      "Training log: 5 epoch (12928 / 60000 train. data). Loss: 0.11154895275831223\n",
      "Training log: 5 epoch (14208 / 60000 train. data). Loss: 0.058265816420316696\n",
      "Training log: 5 epoch (15488 / 60000 train. data). Loss: 0.11911533027887344\n",
      "Training log: 5 epoch (16768 / 60000 train. data). Loss: 0.15728941559791565\n",
      "Training log: 5 epoch (18048 / 60000 train. data). Loss: 0.0745888203382492\n",
      "Training log: 5 epoch (19328 / 60000 train. data). Loss: 0.22916337847709656\n",
      "Training log: 5 epoch (20608 / 60000 train. data). Loss: 0.08264020085334778\n",
      "Training log: 5 epoch (21888 / 60000 train. data). Loss: 0.07483261823654175\n",
      "Training log: 5 epoch (23168 / 60000 train. data). Loss: 0.1410456895828247\n",
      "Training log: 5 epoch (24448 / 60000 train. data). Loss: 0.14186391234397888\n",
      "Training log: 5 epoch (25728 / 60000 train. data). Loss: 0.0875149667263031\n",
      "Training log: 5 epoch (27008 / 60000 train. data). Loss: 0.12768124043941498\n",
      "Training log: 5 epoch (28288 / 60000 train. data). Loss: 0.11192277073860168\n",
      "Training log: 5 epoch (29568 / 60000 train. data). Loss: 0.031900130212306976\n",
      "Training log: 5 epoch (30848 / 60000 train. data). Loss: 0.08367399126291275\n",
      "Training log: 5 epoch (32128 / 60000 train. data). Loss: 0.07932303845882416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 5 epoch (33408 / 60000 train. data). Loss: 0.26760607957839966\n",
      "Training log: 5 epoch (34688 / 60000 train. data). Loss: 0.12701576948165894\n",
      "Training log: 5 epoch (35968 / 60000 train. data). Loss: 0.09237856417894363\n",
      "Training log: 5 epoch (37248 / 60000 train. data). Loss: 0.0912524163722992\n",
      "Training log: 5 epoch (38528 / 60000 train. data). Loss: 0.15327171981334686\n",
      "Training log: 5 epoch (39808 / 60000 train. data). Loss: 0.08282016217708588\n",
      "Training log: 5 epoch (41088 / 60000 train. data). Loss: 0.057571981102228165\n",
      "Training log: 5 epoch (42368 / 60000 train. data). Loss: 0.13315275311470032\n",
      "Training log: 5 epoch (43648 / 60000 train. data). Loss: 0.13219527900218964\n",
      "Training log: 5 epoch (44928 / 60000 train. data). Loss: 0.10404163599014282\n",
      "Training log: 5 epoch (46208 / 60000 train. data). Loss: 0.10168436169624329\n",
      "Training log: 5 epoch (47488 / 60000 train. data). Loss: 0.05063081160187721\n",
      "Training log: 5 epoch (48768 / 60000 train. data). Loss: 0.1351269781589508\n",
      "Training log: 5 epoch (50048 / 60000 train. data). Loss: 0.0709017813205719\n",
      "Training log: 5 epoch (51328 / 60000 train. data). Loss: 0.13821744918823242\n",
      "Training log: 5 epoch (52608 / 60000 train. data). Loss: 0.06385663896799088\n",
      "Training log: 5 epoch (53888 / 60000 train. data). Loss: 0.13606800138950348\n",
      "Training log: 5 epoch (55168 / 60000 train. data). Loss: 0.05745361000299454\n",
      "Training log: 5 epoch (56448 / 60000 train. data). Loss: 0.096469447016716\n",
      "Training log: 5 epoch (57728 / 60000 train. data). Loss: 0.03968796879053116\n",
      "Training log: 5 epoch (59008 / 60000 train. data). Loss: 0.0688314288854599\n",
      "Test loss (avg): 0.11241939449310302, Accuracy: 0.9654\n",
      "Training log: 6 epoch (128 / 60000 train. data). Loss: 0.16604340076446533\n",
      "Training log: 6 epoch (1408 / 60000 train. data). Loss: 0.09529637545347214\n",
      "Training log: 6 epoch (2688 / 60000 train. data). Loss: 0.09230896830558777\n",
      "Training log: 6 epoch (3968 / 60000 train. data). Loss: 0.060238827019929886\n",
      "Training log: 6 epoch (5248 / 60000 train. data). Loss: 0.1465565711259842\n",
      "Training log: 6 epoch (6528 / 60000 train. data). Loss: 0.06914501637220383\n",
      "Training log: 6 epoch (7808 / 60000 train. data). Loss: 0.05079274624586105\n",
      "Training log: 6 epoch (9088 / 60000 train. data). Loss: 0.04827221482992172\n",
      "Training log: 6 epoch (10368 / 60000 train. data). Loss: 0.05719755217432976\n",
      "Training log: 6 epoch (11648 / 60000 train. data). Loss: 0.061792511492967606\n",
      "Training log: 6 epoch (12928 / 60000 train. data). Loss: 0.146896630525589\n",
      "Training log: 6 epoch (14208 / 60000 train. data). Loss: 0.12460648268461227\n",
      "Training log: 6 epoch (15488 / 60000 train. data). Loss: 0.20562469959259033\n",
      "Training log: 6 epoch (16768 / 60000 train. data). Loss: 0.06953799724578857\n",
      "Training log: 6 epoch (18048 / 60000 train. data). Loss: 0.13888752460479736\n",
      "Training log: 6 epoch (19328 / 60000 train. data). Loss: 0.09644392132759094\n",
      "Training log: 6 epoch (20608 / 60000 train. data). Loss: 0.12798742949962616\n",
      "Training log: 6 epoch (21888 / 60000 train. data). Loss: 0.1812858283519745\n",
      "Training log: 6 epoch (23168 / 60000 train. data). Loss: 0.1227930560708046\n",
      "Training log: 6 epoch (24448 / 60000 train. data). Loss: 0.1268262416124344\n",
      "Training log: 6 epoch (25728 / 60000 train. data). Loss: 0.12997625768184662\n",
      "Training log: 6 epoch (27008 / 60000 train. data). Loss: 0.1310877948999405\n",
      "Training log: 6 epoch (28288 / 60000 train. data). Loss: 0.07072725147008896\n",
      "Training log: 6 epoch (29568 / 60000 train. data). Loss: 0.07640126347541809\n",
      "Training log: 6 epoch (30848 / 60000 train. data). Loss: 0.03724820539355278\n",
      "Training log: 6 epoch (32128 / 60000 train. data). Loss: 0.04501970484852791\n",
      "Training log: 6 epoch (33408 / 60000 train. data). Loss: 0.04462606459856033\n",
      "Training log: 6 epoch (34688 / 60000 train. data). Loss: 0.06450006365776062\n",
      "Training log: 6 epoch (35968 / 60000 train. data). Loss: 0.02499900385737419\n",
      "Training log: 6 epoch (37248 / 60000 train. data). Loss: 0.05506083369255066\n",
      "Training log: 6 epoch (38528 / 60000 train. data). Loss: 0.08681061863899231\n",
      "Training log: 6 epoch (39808 / 60000 train. data). Loss: 0.06255719065666199\n",
      "Training log: 6 epoch (41088 / 60000 train. data). Loss: 0.15337716042995453\n",
      "Training log: 6 epoch (42368 / 60000 train. data). Loss: 0.09499212354421616\n",
      "Training log: 6 epoch (43648 / 60000 train. data). Loss: 0.06699247658252716\n",
      "Training log: 6 epoch (44928 / 60000 train. data). Loss: 0.14858582615852356\n",
      "Training log: 6 epoch (46208 / 60000 train. data). Loss: 0.05083401873707771\n",
      "Training log: 6 epoch (47488 / 60000 train. data). Loss: 0.10593797266483307\n",
      "Training log: 6 epoch (48768 / 60000 train. data). Loss: 0.12812131643295288\n",
      "Training log: 6 epoch (50048 / 60000 train. data). Loss: 0.10275758802890778\n",
      "Training log: 6 epoch (51328 / 60000 train. data). Loss: 0.049490250647068024\n",
      "Training log: 6 epoch (52608 / 60000 train. data). Loss: 0.07611630856990814\n",
      "Training log: 6 epoch (53888 / 60000 train. data). Loss: 0.04779033362865448\n",
      "Training log: 6 epoch (55168 / 60000 train. data). Loss: 0.08558022975921631\n",
      "Training log: 6 epoch (56448 / 60000 train. data). Loss: 0.13018634915351868\n",
      "Training log: 6 epoch (57728 / 60000 train. data). Loss: 0.08746840059757233\n",
      "Training log: 6 epoch (59008 / 60000 train. data). Loss: 0.02921387366950512\n",
      "Test loss (avg): 0.09403710088729858, Accuracy: 0.9718\n",
      "Training log: 7 epoch (128 / 60000 train. data). Loss: 0.07848191261291504\n",
      "Training log: 7 epoch (1408 / 60000 train. data). Loss: 0.07541345059871674\n",
      "Training log: 7 epoch (2688 / 60000 train. data). Loss: 0.23359887301921844\n",
      "Training log: 7 epoch (3968 / 60000 train. data). Loss: 0.08336631953716278\n",
      "Training log: 7 epoch (5248 / 60000 train. data). Loss: 0.04902968555688858\n",
      "Training log: 7 epoch (6528 / 60000 train. data). Loss: 0.08112770318984985\n",
      "Training log: 7 epoch (7808 / 60000 train. data). Loss: 0.046966928988695145\n",
      "Training log: 7 epoch (9088 / 60000 train. data). Loss: 0.05134155973792076\n",
      "Training log: 7 epoch (10368 / 60000 train. data). Loss: 0.051615796983242035\n",
      "Training log: 7 epoch (11648 / 60000 train. data). Loss: 0.03973231092095375\n",
      "Training log: 7 epoch (12928 / 60000 train. data). Loss: 0.050629742443561554\n",
      "Training log: 7 epoch (14208 / 60000 train. data). Loss: 0.047823186963796616\n",
      "Training log: 7 epoch (15488 / 60000 train. data). Loss: 0.04917412996292114\n",
      "Training log: 7 epoch (16768 / 60000 train. data). Loss: 0.11158241331577301\n",
      "Training log: 7 epoch (18048 / 60000 train. data). Loss: 0.06262490153312683\n",
      "Training log: 7 epoch (19328 / 60000 train. data). Loss: 0.06239192187786102\n",
      "Training log: 7 epoch (20608 / 60000 train. data). Loss: 0.13378754258155823\n",
      "Training log: 7 epoch (21888 / 60000 train. data). Loss: 0.15747754275798798\n",
      "Training log: 7 epoch (23168 / 60000 train. data). Loss: 0.07218082249164581\n",
      "Training log: 7 epoch (24448 / 60000 train. data). Loss: 0.0662219375371933\n",
      "Training log: 7 epoch (25728 / 60000 train. data). Loss: 0.05532426759600639\n",
      "Training log: 7 epoch (27008 / 60000 train. data). Loss: 0.12838996946811676\n",
      "Training log: 7 epoch (28288 / 60000 train. data). Loss: 0.07124268263578415\n",
      "Training log: 7 epoch (29568 / 60000 train. data). Loss: 0.054727647453546524\n",
      "Training log: 7 epoch (30848 / 60000 train. data). Loss: 0.02662050910294056\n",
      "Training log: 7 epoch (32128 / 60000 train. data). Loss: 0.05163595452904701\n",
      "Training log: 7 epoch (33408 / 60000 train. data). Loss: 0.07069575786590576\n",
      "Training log: 7 epoch (34688 / 60000 train. data). Loss: 0.04843490570783615\n",
      "Training log: 7 epoch (35968 / 60000 train. data). Loss: 0.05208640918135643\n",
      "Training log: 7 epoch (37248 / 60000 train. data). Loss: 0.0684952586889267\n",
      "Training log: 7 epoch (38528 / 60000 train. data). Loss: 0.10442782938480377\n",
      "Training log: 7 epoch (39808 / 60000 train. data). Loss: 0.032276835292577744\n",
      "Training log: 7 epoch (41088 / 60000 train. data). Loss: 0.04343147203326225\n",
      "Training log: 7 epoch (42368 / 60000 train. data). Loss: 0.05211585387587547\n",
      "Training log: 7 epoch (43648 / 60000 train. data). Loss: 0.09856432676315308\n",
      "Training log: 7 epoch (44928 / 60000 train. data). Loss: 0.05150454491376877\n",
      "Training log: 7 epoch (46208 / 60000 train. data). Loss: 0.047320980578660965\n",
      "Training log: 7 epoch (47488 / 60000 train. data). Loss: 0.061146192252635956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 7 epoch (48768 / 60000 train. data). Loss: 0.07196397334337234\n",
      "Training log: 7 epoch (50048 / 60000 train. data). Loss: 0.031773217022418976\n",
      "Training log: 7 epoch (51328 / 60000 train. data). Loss: 0.03906729444861412\n",
      "Training log: 7 epoch (52608 / 60000 train. data). Loss: 0.03045079857110977\n",
      "Training log: 7 epoch (53888 / 60000 train. data). Loss: 0.08923190832138062\n",
      "Training log: 7 epoch (55168 / 60000 train. data). Loss: 0.07156632095575333\n",
      "Training log: 7 epoch (56448 / 60000 train. data). Loss: 0.03833051770925522\n",
      "Training log: 7 epoch (57728 / 60000 train. data). Loss: 0.05567598715424538\n",
      "Training log: 7 epoch (59008 / 60000 train. data). Loss: 0.048354487866163254\n",
      "Test loss (avg): 0.08853961670398712, Accuracy: 0.974\n",
      "Training log: 8 epoch (128 / 60000 train. data). Loss: 0.032253243029117584\n",
      "Training log: 8 epoch (1408 / 60000 train. data). Loss: 0.07439969480037689\n",
      "Training log: 8 epoch (2688 / 60000 train. data). Loss: 0.09719149768352509\n",
      "Training log: 8 epoch (3968 / 60000 train. data). Loss: 0.0930224061012268\n",
      "Training log: 8 epoch (5248 / 60000 train. data). Loss: 0.04444167762994766\n",
      "Training log: 8 epoch (6528 / 60000 train. data). Loss: 0.055590976029634476\n",
      "Training log: 8 epoch (7808 / 60000 train. data). Loss: 0.05138427019119263\n",
      "Training log: 8 epoch (9088 / 60000 train. data). Loss: 0.03775063902139664\n",
      "Training log: 8 epoch (10368 / 60000 train. data). Loss: 0.04730468988418579\n",
      "Training log: 8 epoch (11648 / 60000 train. data). Loss: 0.07519125938415527\n",
      "Training log: 8 epoch (12928 / 60000 train. data). Loss: 0.05160367861390114\n",
      "Training log: 8 epoch (14208 / 60000 train. data). Loss: 0.07703720033168793\n",
      "Training log: 8 epoch (15488 / 60000 train. data). Loss: 0.06856121122837067\n",
      "Training log: 8 epoch (16768 / 60000 train. data). Loss: 0.052434422075748444\n",
      "Training log: 8 epoch (18048 / 60000 train. data). Loss: 0.053582049906253815\n",
      "Training log: 8 epoch (19328 / 60000 train. data). Loss: 0.037934739142656326\n",
      "Training log: 8 epoch (20608 / 60000 train. data). Loss: 0.07511843740940094\n",
      "Training log: 8 epoch (21888 / 60000 train. data). Loss: 0.03287101536989212\n",
      "Training log: 8 epoch (23168 / 60000 train. data). Loss: 0.07334010303020477\n",
      "Training log: 8 epoch (24448 / 60000 train. data). Loss: 0.019347120076417923\n",
      "Training log: 8 epoch (25728 / 60000 train. data). Loss: 0.014004100114107132\n",
      "Training log: 8 epoch (27008 / 60000 train. data). Loss: 0.1338667869567871\n",
      "Training log: 8 epoch (28288 / 60000 train. data). Loss: 0.15455052256584167\n",
      "Training log: 8 epoch (29568 / 60000 train. data). Loss: 0.03564043343067169\n",
      "Training log: 8 epoch (30848 / 60000 train. data). Loss: 0.029906047508120537\n",
      "Training log: 8 epoch (32128 / 60000 train. data). Loss: 0.09366331994533539\n",
      "Training log: 8 epoch (33408 / 60000 train. data). Loss: 0.04449719190597534\n",
      "Training log: 8 epoch (34688 / 60000 train. data). Loss: 0.07764585316181183\n",
      "Training log: 8 epoch (35968 / 60000 train. data). Loss: 0.029549162834882736\n",
      "Training log: 8 epoch (37248 / 60000 train. data). Loss: 0.0749843567609787\n",
      "Training log: 8 epoch (38528 / 60000 train. data). Loss: 0.1056506335735321\n",
      "Training log: 8 epoch (39808 / 60000 train. data). Loss: 0.03565560653805733\n",
      "Training log: 8 epoch (41088 / 60000 train. data). Loss: 0.06691820919513702\n",
      "Training log: 8 epoch (42368 / 60000 train. data). Loss: 0.031519465148448944\n",
      "Training log: 8 epoch (43648 / 60000 train. data). Loss: 0.03802299126982689\n",
      "Training log: 8 epoch (44928 / 60000 train. data). Loss: 0.05076032131910324\n",
      "Training log: 8 epoch (46208 / 60000 train. data). Loss: 0.028201710432767868\n",
      "Training log: 8 epoch (47488 / 60000 train. data). Loss: 0.03271150961518288\n",
      "Training log: 8 epoch (48768 / 60000 train. data). Loss: 0.02564467117190361\n",
      "Training log: 8 epoch (50048 / 60000 train. data). Loss: 0.06333358585834503\n",
      "Training log: 8 epoch (51328 / 60000 train. data). Loss: 0.09658455848693848\n",
      "Training log: 8 epoch (52608 / 60000 train. data). Loss: 0.08992014825344086\n",
      "Training log: 8 epoch (53888 / 60000 train. data). Loss: 0.02850339189171791\n",
      "Training log: 8 epoch (55168 / 60000 train. data). Loss: 0.10466144979000092\n",
      "Training log: 8 epoch (56448 / 60000 train. data). Loss: 0.1154317632317543\n",
      "Training log: 8 epoch (57728 / 60000 train. data). Loss: 0.05220900848507881\n",
      "Training log: 8 epoch (59008 / 60000 train. data). Loss: 0.01704380288720131\n",
      "Test loss (avg): 0.08204093081951142, Accuracy: 0.9753\n",
      "Training log: 9 epoch (128 / 60000 train. data). Loss: 0.08951970934867859\n",
      "Training log: 9 epoch (1408 / 60000 train. data). Loss: 0.09946133196353912\n",
      "Training log: 9 epoch (2688 / 60000 train. data). Loss: 0.06117836758494377\n",
      "Training log: 9 epoch (3968 / 60000 train. data). Loss: 0.0307321660220623\n",
      "Training log: 9 epoch (5248 / 60000 train. data). Loss: 0.026561524718999863\n",
      "Training log: 9 epoch (6528 / 60000 train. data). Loss: 0.014287199825048447\n",
      "Training log: 9 epoch (7808 / 60000 train. data). Loss: 0.12678059935569763\n",
      "Training log: 9 epoch (9088 / 60000 train. data). Loss: 0.023074815049767494\n",
      "Training log: 9 epoch (10368 / 60000 train. data). Loss: 0.037646643817424774\n",
      "Training log: 9 epoch (11648 / 60000 train. data). Loss: 0.028321420773863792\n",
      "Training log: 9 epoch (12928 / 60000 train. data). Loss: 0.019901638850569725\n",
      "Training log: 9 epoch (14208 / 60000 train. data). Loss: 0.02347567304968834\n",
      "Training log: 9 epoch (15488 / 60000 train. data). Loss: 0.07465232908725739\n",
      "Training log: 9 epoch (16768 / 60000 train. data). Loss: 0.022998524829745293\n",
      "Training log: 9 epoch (18048 / 60000 train. data). Loss: 0.042559172958135605\n",
      "Training log: 9 epoch (19328 / 60000 train. data). Loss: 0.07262422144412994\n",
      "Training log: 9 epoch (20608 / 60000 train. data). Loss: 0.02730279415845871\n",
      "Training log: 9 epoch (21888 / 60000 train. data). Loss: 0.01821509748697281\n",
      "Training log: 9 epoch (23168 / 60000 train. data). Loss: 0.029740674421191216\n",
      "Training log: 9 epoch (24448 / 60000 train. data). Loss: 0.03446878120303154\n",
      "Training log: 9 epoch (25728 / 60000 train. data). Loss: 0.023205192759633064\n",
      "Training log: 9 epoch (27008 / 60000 train. data). Loss: 0.07458429038524628\n",
      "Training log: 9 epoch (28288 / 60000 train. data). Loss: 0.04218493029475212\n",
      "Training log: 9 epoch (29568 / 60000 train. data). Loss: 0.06744818389415741\n",
      "Training log: 9 epoch (30848 / 60000 train. data). Loss: 0.04184975102543831\n",
      "Training log: 9 epoch (32128 / 60000 train. data). Loss: 0.024597549811005592\n",
      "Training log: 9 epoch (33408 / 60000 train. data). Loss: 0.0819786787033081\n",
      "Training log: 9 epoch (34688 / 60000 train. data). Loss: 0.03177022188901901\n",
      "Training log: 9 epoch (35968 / 60000 train. data). Loss: 0.05984155461192131\n",
      "Training log: 9 epoch (37248 / 60000 train. data). Loss: 0.027762513607740402\n",
      "Training log: 9 epoch (38528 / 60000 train. data). Loss: 0.04665016382932663\n",
      "Training log: 9 epoch (39808 / 60000 train. data). Loss: 0.04398905858397484\n",
      "Training log: 9 epoch (41088 / 60000 train. data). Loss: 0.037462569773197174\n",
      "Training log: 9 epoch (42368 / 60000 train. data). Loss: 0.04394782707095146\n",
      "Training log: 9 epoch (43648 / 60000 train. data). Loss: 0.024351852014660835\n",
      "Training log: 9 epoch (44928 / 60000 train. data). Loss: 0.03458342328667641\n",
      "Training log: 9 epoch (46208 / 60000 train. data). Loss: 0.04418034479022026\n",
      "Training log: 9 epoch (47488 / 60000 train. data). Loss: 0.039795175194740295\n",
      "Training log: 9 epoch (48768 / 60000 train. data). Loss: 0.0871511921286583\n",
      "Training log: 9 epoch (50048 / 60000 train. data). Loss: 0.06629769504070282\n",
      "Training log: 9 epoch (51328 / 60000 train. data). Loss: 0.058957789093256\n",
      "Training log: 9 epoch (52608 / 60000 train. data). Loss: 0.026130683720111847\n",
      "Training log: 9 epoch (53888 / 60000 train. data). Loss: 0.08059145510196686\n",
      "Training log: 9 epoch (55168 / 60000 train. data). Loss: 0.08969375491142273\n",
      "Training log: 9 epoch (56448 / 60000 train. data). Loss: 0.05081917345523834\n",
      "Training log: 9 epoch (57728 / 60000 train. data). Loss: 0.03779115527868271\n",
      "Training log: 9 epoch (59008 / 60000 train. data). Loss: 0.029065683484077454\n",
      "Test loss (avg): 0.07768251745700837, Accuracy: 0.9768\n",
      "Training log: 10 epoch (128 / 60000 train. data). Loss: 0.014960573986172676\n",
      "Training log: 10 epoch (1408 / 60000 train. data). Loss: 0.02266000770032406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 10 epoch (2688 / 60000 train. data). Loss: 0.05782614275813103\n",
      "Training log: 10 epoch (3968 / 60000 train. data). Loss: 0.01547076553106308\n",
      "Training log: 10 epoch (5248 / 60000 train. data). Loss: 0.01763887330889702\n",
      "Training log: 10 epoch (6528 / 60000 train. data). Loss: 0.06048644334077835\n",
      "Training log: 10 epoch (7808 / 60000 train. data). Loss: 0.02987394668161869\n",
      "Training log: 10 epoch (9088 / 60000 train. data). Loss: 0.032492198050022125\n",
      "Training log: 10 epoch (10368 / 60000 train. data). Loss: 0.155377596616745\n",
      "Training log: 10 epoch (11648 / 60000 train. data). Loss: 0.04216752201318741\n",
      "Training log: 10 epoch (12928 / 60000 train. data). Loss: 0.0850716382265091\n",
      "Training log: 10 epoch (14208 / 60000 train. data). Loss: 0.01918836683034897\n",
      "Training log: 10 epoch (15488 / 60000 train. data). Loss: 0.024115698412060738\n",
      "Training log: 10 epoch (16768 / 60000 train. data). Loss: 0.0077512748539447784\n",
      "Training log: 10 epoch (18048 / 60000 train. data). Loss: 0.030929546803236008\n",
      "Training log: 10 epoch (19328 / 60000 train. data). Loss: 0.031484924256801605\n",
      "Training log: 10 epoch (20608 / 60000 train. data). Loss: 0.0166231207549572\n",
      "Training log: 10 epoch (21888 / 60000 train. data). Loss: 0.05877308547496796\n",
      "Training log: 10 epoch (23168 / 60000 train. data). Loss: 0.03908694535493851\n",
      "Training log: 10 epoch (24448 / 60000 train. data). Loss: 0.036098334938287735\n",
      "Training log: 10 epoch (25728 / 60000 train. data). Loss: 0.06316161900758743\n",
      "Training log: 10 epoch (27008 / 60000 train. data). Loss: 0.06060303375124931\n",
      "Training log: 10 epoch (28288 / 60000 train. data). Loss: 0.03289671987295151\n",
      "Training log: 10 epoch (29568 / 60000 train. data). Loss: 0.058980897068977356\n",
      "Training log: 10 epoch (30848 / 60000 train. data). Loss: 0.021179188042879105\n",
      "Training log: 10 epoch (32128 / 60000 train. data). Loss: 0.010830705985426903\n",
      "Training log: 10 epoch (33408 / 60000 train. data). Loss: 0.023106969892978668\n",
      "Training log: 10 epoch (34688 / 60000 train. data). Loss: 0.019722657278180122\n",
      "Training log: 10 epoch (35968 / 60000 train. data). Loss: 0.06113565340638161\n",
      "Training log: 10 epoch (37248 / 60000 train. data). Loss: 0.060305915772914886\n",
      "Training log: 10 epoch (38528 / 60000 train. data). Loss: 0.014452914707362652\n",
      "Training log: 10 epoch (39808 / 60000 train. data). Loss: 0.01816031150519848\n",
      "Training log: 10 epoch (41088 / 60000 train. data). Loss: 0.02559584192931652\n",
      "Training log: 10 epoch (42368 / 60000 train. data). Loss: 0.03866498917341232\n",
      "Training log: 10 epoch (43648 / 60000 train. data). Loss: 0.036535050719976425\n",
      "Training log: 10 epoch (44928 / 60000 train. data). Loss: 0.022527093067765236\n",
      "Training log: 10 epoch (46208 / 60000 train. data). Loss: 0.03153442591428757\n",
      "Training log: 10 epoch (47488 / 60000 train. data). Loss: 0.035049472004175186\n",
      "Training log: 10 epoch (48768 / 60000 train. data). Loss: 0.03842420503497124\n",
      "Training log: 10 epoch (50048 / 60000 train. data). Loss: 0.04033687338232994\n",
      "Training log: 10 epoch (51328 / 60000 train. data). Loss: 0.02888774499297142\n",
      "Training log: 10 epoch (52608 / 60000 train. data). Loss: 0.02555849589407444\n",
      "Training log: 10 epoch (53888 / 60000 train. data). Loss: 0.033260807394981384\n",
      "Training log: 10 epoch (55168 / 60000 train. data). Loss: 0.08043565601110458\n",
      "Training log: 10 epoch (56448 / 60000 train. data). Loss: 0.023662781342864037\n",
      "Training log: 10 epoch (57728 / 60000 train. data). Loss: 0.10932983458042145\n",
      "Training log: 10 epoch (59008 / 60000 train. data). Loss: 0.041357990354299545\n",
      "Test loss (avg): 0.07529574549198151, Accuracy: 0.9773\n",
      "Training log: 11 epoch (128 / 60000 train. data). Loss: 0.04756028205156326\n",
      "Training log: 11 epoch (1408 / 60000 train. data). Loss: 0.016707032918930054\n",
      "Training log: 11 epoch (2688 / 60000 train. data). Loss: 0.018084878101944923\n",
      "Training log: 11 epoch (3968 / 60000 train. data). Loss: 0.016635386273264885\n",
      "Training log: 11 epoch (5248 / 60000 train. data). Loss: 0.020223822444677353\n",
      "Training log: 11 epoch (6528 / 60000 train. data). Loss: 0.03522185981273651\n",
      "Training log: 11 epoch (7808 / 60000 train. data). Loss: 0.05183793604373932\n",
      "Training log: 11 epoch (9088 / 60000 train. data). Loss: 0.01167919673025608\n",
      "Training log: 11 epoch (10368 / 60000 train. data). Loss: 0.04627655819058418\n",
      "Training log: 11 epoch (11648 / 60000 train. data). Loss: 0.022828293964266777\n",
      "Training log: 11 epoch (12928 / 60000 train. data). Loss: 0.05019761621952057\n",
      "Training log: 11 epoch (14208 / 60000 train. data). Loss: 0.02817184291779995\n",
      "Training log: 11 epoch (15488 / 60000 train. data). Loss: 0.018942423164844513\n",
      "Training log: 11 epoch (16768 / 60000 train. data). Loss: 0.03741656243801117\n",
      "Training log: 11 epoch (18048 / 60000 train. data). Loss: 0.010305382311344147\n",
      "Training log: 11 epoch (19328 / 60000 train. data). Loss: 0.026470065116882324\n",
      "Training log: 11 epoch (20608 / 60000 train. data). Loss: 0.026691336184740067\n",
      "Training log: 11 epoch (21888 / 60000 train. data). Loss: 0.00584915466606617\n",
      "Training log: 11 epoch (23168 / 60000 train. data). Loss: 0.12190650403499603\n",
      "Training log: 11 epoch (24448 / 60000 train. data). Loss: 0.04791259765625\n",
      "Training log: 11 epoch (25728 / 60000 train. data). Loss: 0.03397827968001366\n",
      "Training log: 11 epoch (27008 / 60000 train. data). Loss: 0.03825727850198746\n",
      "Training log: 11 epoch (28288 / 60000 train. data). Loss: 0.02904556691646576\n",
      "Training log: 11 epoch (29568 / 60000 train. data). Loss: 0.02254049852490425\n",
      "Training log: 11 epoch (30848 / 60000 train. data). Loss: 0.015182074159383774\n",
      "Training log: 11 epoch (32128 / 60000 train. data). Loss: 0.0702877938747406\n",
      "Training log: 11 epoch (33408 / 60000 train. data). Loss: 0.05697523057460785\n",
      "Training log: 11 epoch (34688 / 60000 train. data). Loss: 0.07526443153619766\n",
      "Training log: 11 epoch (35968 / 60000 train. data). Loss: 0.07089574635028839\n",
      "Training log: 11 epoch (37248 / 60000 train. data). Loss: 0.04882025346159935\n",
      "Training log: 11 epoch (38528 / 60000 train. data). Loss: 0.012024343013763428\n",
      "Training log: 11 epoch (39808 / 60000 train. data). Loss: 0.034597914665937424\n",
      "Training log: 11 epoch (41088 / 60000 train. data). Loss: 0.0379452258348465\n",
      "Training log: 11 epoch (42368 / 60000 train. data). Loss: 0.03146370127797127\n",
      "Training log: 11 epoch (43648 / 60000 train. data). Loss: 0.043876443058252335\n",
      "Training log: 11 epoch (44928 / 60000 train. data). Loss: 0.03605164960026741\n",
      "Training log: 11 epoch (46208 / 60000 train. data). Loss: 0.027112528681755066\n",
      "Training log: 11 epoch (47488 / 60000 train. data). Loss: 0.017617017030715942\n",
      "Training log: 11 epoch (48768 / 60000 train. data). Loss: 0.08024057745933533\n",
      "Training log: 11 epoch (50048 / 60000 train. data). Loss: 0.02845100685954094\n",
      "Training log: 11 epoch (51328 / 60000 train. data). Loss: 0.0185207799077034\n",
      "Training log: 11 epoch (52608 / 60000 train. data). Loss: 0.06597663462162018\n",
      "Training log: 11 epoch (53888 / 60000 train. data). Loss: 0.05027293413877487\n",
      "Training log: 11 epoch (55168 / 60000 train. data). Loss: 0.03021225519478321\n",
      "Training log: 11 epoch (56448 / 60000 train. data). Loss: 0.010274454951286316\n",
      "Training log: 11 epoch (57728 / 60000 train. data). Loss: 0.11431077122688293\n",
      "Training log: 11 epoch (59008 / 60000 train. data). Loss: 0.021151099354028702\n",
      "Test loss (avg): 0.06827726354599, Accuracy: 0.9789\n",
      "Training log: 12 epoch (128 / 60000 train. data). Loss: 0.04107559099793434\n",
      "Training log: 12 epoch (1408 / 60000 train. data). Loss: 0.017967868596315384\n",
      "Training log: 12 epoch (2688 / 60000 train. data). Loss: 0.04455205798149109\n",
      "Training log: 12 epoch (3968 / 60000 train. data). Loss: 0.02449488267302513\n",
      "Training log: 12 epoch (5248 / 60000 train. data). Loss: 0.011528199538588524\n",
      "Training log: 12 epoch (6528 / 60000 train. data). Loss: 0.03796970471739769\n",
      "Training log: 12 epoch (7808 / 60000 train. data). Loss: 0.023524776101112366\n",
      "Training log: 12 epoch (9088 / 60000 train. data). Loss: 0.021658839657902718\n",
      "Training log: 12 epoch (10368 / 60000 train. data). Loss: 0.03822861611843109\n",
      "Training log: 12 epoch (11648 / 60000 train. data). Loss: 0.017459390684962273\n",
      "Training log: 12 epoch (12928 / 60000 train. data). Loss: 0.03135187551379204\n",
      "Training log: 12 epoch (14208 / 60000 train. data). Loss: 0.01473189890384674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 12 epoch (15488 / 60000 train. data). Loss: 0.031724993139505386\n",
      "Training log: 12 epoch (16768 / 60000 train. data). Loss: 0.01511036790907383\n",
      "Training log: 12 epoch (18048 / 60000 train. data). Loss: 0.02396358922123909\n",
      "Training log: 12 epoch (19328 / 60000 train. data). Loss: 0.016912104561924934\n",
      "Training log: 12 epoch (20608 / 60000 train. data). Loss: 0.027938932180404663\n",
      "Training log: 12 epoch (21888 / 60000 train. data). Loss: 0.006200578063726425\n",
      "Training log: 12 epoch (23168 / 60000 train. data). Loss: 0.01081027276813984\n",
      "Training log: 12 epoch (24448 / 60000 train. data). Loss: 0.006410099565982819\n",
      "Training log: 12 epoch (25728 / 60000 train. data). Loss: 0.0675923228263855\n",
      "Training log: 12 epoch (27008 / 60000 train. data). Loss: 0.029404420405626297\n",
      "Training log: 12 epoch (28288 / 60000 train. data). Loss: 0.04663942754268646\n",
      "Training log: 12 epoch (29568 / 60000 train. data). Loss: 0.08363132178783417\n",
      "Training log: 12 epoch (30848 / 60000 train. data). Loss: 0.009207719936966896\n",
      "Training log: 12 epoch (32128 / 60000 train. data). Loss: 0.021712936460971832\n",
      "Training log: 12 epoch (33408 / 60000 train. data). Loss: 0.02969508245587349\n",
      "Training log: 12 epoch (34688 / 60000 train. data). Loss: 0.056667719036340714\n",
      "Training log: 12 epoch (35968 / 60000 train. data). Loss: 0.025766286998987198\n",
      "Training log: 12 epoch (37248 / 60000 train. data). Loss: 0.017996907234191895\n",
      "Training log: 12 epoch (38528 / 60000 train. data). Loss: 0.026699697598814964\n",
      "Training log: 12 epoch (39808 / 60000 train. data). Loss: 0.023491831496357918\n",
      "Training log: 12 epoch (41088 / 60000 train. data). Loss: 0.03380543738603592\n",
      "Training log: 12 epoch (42368 / 60000 train. data). Loss: 0.02368747442960739\n",
      "Training log: 12 epoch (43648 / 60000 train. data). Loss: 0.02707730419933796\n",
      "Training log: 12 epoch (44928 / 60000 train. data). Loss: 0.020895393565297127\n",
      "Training log: 12 epoch (46208 / 60000 train. data). Loss: 0.01860831305384636\n",
      "Training log: 12 epoch (47488 / 60000 train. data). Loss: 0.032526805996894836\n",
      "Training log: 12 epoch (48768 / 60000 train. data). Loss: 0.013838982209563255\n",
      "Training log: 12 epoch (50048 / 60000 train. data). Loss: 0.025569073855876923\n",
      "Training log: 12 epoch (51328 / 60000 train. data). Loss: 0.02844553254544735\n",
      "Training log: 12 epoch (52608 / 60000 train. data). Loss: 0.010335661470890045\n",
      "Training log: 12 epoch (53888 / 60000 train. data). Loss: 0.02445249632000923\n",
      "Training log: 12 epoch (55168 / 60000 train. data). Loss: 0.028583908453583717\n",
      "Training log: 12 epoch (56448 / 60000 train. data). Loss: 0.0671040415763855\n",
      "Training log: 12 epoch (57728 / 60000 train. data). Loss: 0.03765253350138664\n",
      "Training log: 12 epoch (59008 / 60000 train. data). Loss: 0.053050894290208817\n",
      "Test loss (avg): 0.06576207578182221, Accuracy: 0.9803\n",
      "Training log: 13 epoch (128 / 60000 train. data). Loss: 0.013291975483298302\n",
      "Training log: 13 epoch (1408 / 60000 train. data). Loss: 0.06325174123048782\n",
      "Training log: 13 epoch (2688 / 60000 train. data). Loss: 0.005680028349161148\n",
      "Training log: 13 epoch (3968 / 60000 train. data). Loss: 0.019477730616927147\n",
      "Training log: 13 epoch (5248 / 60000 train. data). Loss: 0.016334934160113335\n",
      "Training log: 13 epoch (6528 / 60000 train. data). Loss: 0.0114667359739542\n",
      "Training log: 13 epoch (7808 / 60000 train. data). Loss: 0.020180299878120422\n",
      "Training log: 13 epoch (9088 / 60000 train. data). Loss: 0.02193957380950451\n",
      "Training log: 13 epoch (10368 / 60000 train. data). Loss: 0.021886028349399567\n",
      "Training log: 13 epoch (11648 / 60000 train. data). Loss: 0.007881423458456993\n",
      "Training log: 13 epoch (12928 / 60000 train. data). Loss: 0.01301436498761177\n",
      "Training log: 13 epoch (14208 / 60000 train. data). Loss: 0.026796644553542137\n",
      "Training log: 13 epoch (15488 / 60000 train. data). Loss: 0.03266514837741852\n",
      "Training log: 13 epoch (16768 / 60000 train. data). Loss: 0.008812760934233665\n",
      "Training log: 13 epoch (18048 / 60000 train. data). Loss: 0.007357945665717125\n",
      "Training log: 13 epoch (19328 / 60000 train. data). Loss: 0.007625913247466087\n",
      "Training log: 13 epoch (20608 / 60000 train. data). Loss: 0.023932036012411118\n",
      "Training log: 13 epoch (21888 / 60000 train. data). Loss: 0.0470457449555397\n",
      "Training log: 13 epoch (23168 / 60000 train. data). Loss: 0.008036650717258453\n",
      "Training log: 13 epoch (24448 / 60000 train. data). Loss: 0.02941276505589485\n",
      "Training log: 13 epoch (25728 / 60000 train. data). Loss: 0.027597231790423393\n",
      "Training log: 13 epoch (27008 / 60000 train. data). Loss: 0.0359162837266922\n",
      "Training log: 13 epoch (28288 / 60000 train. data). Loss: 0.038028936833143234\n",
      "Training log: 13 epoch (29568 / 60000 train. data). Loss: 0.014266187325119972\n",
      "Training log: 13 epoch (30848 / 60000 train. data). Loss: 0.07004720717668533\n",
      "Training log: 13 epoch (32128 / 60000 train. data). Loss: 0.020155157893896103\n",
      "Training log: 13 epoch (33408 / 60000 train. data). Loss: 0.014234879985451698\n",
      "Training log: 13 epoch (34688 / 60000 train. data). Loss: 0.03037079982459545\n",
      "Training log: 13 epoch (35968 / 60000 train. data). Loss: 0.010423021391034126\n",
      "Training log: 13 epoch (37248 / 60000 train. data). Loss: 0.012402346357703209\n",
      "Training log: 13 epoch (38528 / 60000 train. data). Loss: 0.014988929033279419\n",
      "Training log: 13 epoch (39808 / 60000 train. data). Loss: 0.04902409762144089\n",
      "Training log: 13 epoch (41088 / 60000 train. data). Loss: 0.016805393621325493\n",
      "Training log: 13 epoch (42368 / 60000 train. data). Loss: 0.025767620652914047\n",
      "Training log: 13 epoch (43648 / 60000 train. data). Loss: 0.019112763926386833\n",
      "Training log: 13 epoch (44928 / 60000 train. data). Loss: 0.01079816184937954\n",
      "Training log: 13 epoch (46208 / 60000 train. data). Loss: 0.01646401919424534\n",
      "Training log: 13 epoch (47488 / 60000 train. data). Loss: 0.00345771387219429\n",
      "Training log: 13 epoch (48768 / 60000 train. data). Loss: 0.013431366533041\n",
      "Training log: 13 epoch (50048 / 60000 train. data). Loss: 0.007636994123458862\n",
      "Training log: 13 epoch (51328 / 60000 train. data). Loss: 0.0411682054400444\n",
      "Training log: 13 epoch (52608 / 60000 train. data). Loss: 0.029527243226766586\n",
      "Training log: 13 epoch (53888 / 60000 train. data). Loss: 0.004350006580352783\n",
      "Training log: 13 epoch (55168 / 60000 train. data). Loss: 0.010044696740806103\n",
      "Training log: 13 epoch (56448 / 60000 train. data). Loss: 0.018559541553258896\n",
      "Training log: 13 epoch (57728 / 60000 train. data). Loss: 0.01840892806649208\n",
      "Training log: 13 epoch (59008 / 60000 train. data). Loss: 0.019693609327077866\n",
      "Test loss (avg): 0.06716034919023514, Accuracy: 0.9811\n",
      "Training log: 14 epoch (128 / 60000 train. data). Loss: 0.02015286311507225\n",
      "Training log: 14 epoch (1408 / 60000 train. data). Loss: 0.024275247007608414\n",
      "Training log: 14 epoch (2688 / 60000 train. data). Loss: 0.013930311426520348\n",
      "Training log: 14 epoch (3968 / 60000 train. data). Loss: 0.0051851775497198105\n",
      "Training log: 14 epoch (5248 / 60000 train. data). Loss: 0.021842749789357185\n",
      "Training log: 14 epoch (6528 / 60000 train. data). Loss: 0.009667295962572098\n",
      "Training log: 14 epoch (7808 / 60000 train. data). Loss: 0.015902215614914894\n",
      "Training log: 14 epoch (9088 / 60000 train. data). Loss: 0.02894643321633339\n",
      "Training log: 14 epoch (10368 / 60000 train. data). Loss: 0.012558795511722565\n",
      "Training log: 14 epoch (11648 / 60000 train. data). Loss: 0.05703634396195412\n",
      "Training log: 14 epoch (12928 / 60000 train. data). Loss: 0.018179917708039284\n",
      "Training log: 14 epoch (14208 / 60000 train. data). Loss: 0.02350662089884281\n",
      "Training log: 14 epoch (15488 / 60000 train. data). Loss: 0.03120339661836624\n",
      "Training log: 14 epoch (16768 / 60000 train. data). Loss: 0.009577181190252304\n",
      "Training log: 14 epoch (18048 / 60000 train. data). Loss: 0.014505919069051743\n",
      "Training log: 14 epoch (19328 / 60000 train. data). Loss: 0.0866292417049408\n",
      "Training log: 14 epoch (20608 / 60000 train. data). Loss: 0.016034957021474838\n",
      "Training log: 14 epoch (21888 / 60000 train. data). Loss: 0.028624363243579865\n",
      "Training log: 14 epoch (23168 / 60000 train. data). Loss: 0.009517038241028786\n",
      "Training log: 14 epoch (24448 / 60000 train. data). Loss: 0.013228649273514748\n",
      "Training log: 14 epoch (25728 / 60000 train. data). Loss: 0.02983064204454422\n",
      "Training log: 14 epoch (27008 / 60000 train. data). Loss: 0.013029100373387337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 14 epoch (28288 / 60000 train. data). Loss: 0.00708794966340065\n",
      "Training log: 14 epoch (29568 / 60000 train. data). Loss: 0.007176162675023079\n",
      "Training log: 14 epoch (30848 / 60000 train. data). Loss: 0.025706909596920013\n",
      "Training log: 14 epoch (32128 / 60000 train. data). Loss: 0.017962682992219925\n",
      "Training log: 14 epoch (33408 / 60000 train. data). Loss: 0.016125187277793884\n",
      "Training log: 14 epoch (34688 / 60000 train. data). Loss: 0.018385285511612892\n",
      "Training log: 14 epoch (35968 / 60000 train. data). Loss: 0.008936632424592972\n",
      "Training log: 14 epoch (37248 / 60000 train. data). Loss: 0.04235277697443962\n",
      "Training log: 14 epoch (38528 / 60000 train. data). Loss: 0.01141069084405899\n",
      "Training log: 14 epoch (39808 / 60000 train. data). Loss: 0.01474378164857626\n",
      "Training log: 14 epoch (41088 / 60000 train. data). Loss: 0.02502991072833538\n",
      "Training log: 14 epoch (42368 / 60000 train. data). Loss: 0.007966428995132446\n",
      "Training log: 14 epoch (43648 / 60000 train. data). Loss: 0.0045165009796619415\n",
      "Training log: 14 epoch (44928 / 60000 train. data). Loss: 0.018628394231200218\n",
      "Training log: 14 epoch (46208 / 60000 train. data). Loss: 0.009000103920698166\n",
      "Training log: 14 epoch (47488 / 60000 train. data). Loss: 0.008111622184515\n",
      "Training log: 14 epoch (48768 / 60000 train. data). Loss: 0.024903055280447006\n",
      "Training log: 14 epoch (50048 / 60000 train. data). Loss: 0.020874911919236183\n",
      "Training log: 14 epoch (51328 / 60000 train. data). Loss: 0.02101948671042919\n",
      "Training log: 14 epoch (52608 / 60000 train. data). Loss: 0.0123424232006073\n",
      "Training log: 14 epoch (53888 / 60000 train. data). Loss: 0.0374307744204998\n",
      "Training log: 14 epoch (55168 / 60000 train. data). Loss: 0.015707552433013916\n",
      "Training log: 14 epoch (56448 / 60000 train. data). Loss: 0.014634343795478344\n",
      "Training log: 14 epoch (57728 / 60000 train. data). Loss: 0.013231825083494186\n",
      "Training log: 14 epoch (59008 / 60000 train. data). Loss: 0.030782001093029976\n",
      "Test loss (avg): 0.061372332894802095, Accuracy: 0.9826\n",
      "Training log: 15 epoch (128 / 60000 train. data). Loss: 0.017700597643852234\n",
      "Training log: 15 epoch (1408 / 60000 train. data). Loss: 0.004112746566534042\n",
      "Training log: 15 epoch (2688 / 60000 train. data). Loss: 0.03741683065891266\n",
      "Training log: 15 epoch (3968 / 60000 train. data). Loss: 0.009506216272711754\n",
      "Training log: 15 epoch (5248 / 60000 train. data). Loss: 0.007659481838345528\n",
      "Training log: 15 epoch (6528 / 60000 train. data). Loss: 0.017656398937106133\n",
      "Training log: 15 epoch (7808 / 60000 train. data). Loss: 0.00730864517390728\n",
      "Training log: 15 epoch (9088 / 60000 train. data). Loss: 0.0049124909564852715\n",
      "Training log: 15 epoch (10368 / 60000 train. data). Loss: 0.03882865607738495\n",
      "Training log: 15 epoch (11648 / 60000 train. data). Loss: 0.015096435323357582\n",
      "Training log: 15 epoch (12928 / 60000 train. data). Loss: 0.010595042258501053\n",
      "Training log: 15 epoch (14208 / 60000 train. data). Loss: 0.026832209900021553\n",
      "Training log: 15 epoch (15488 / 60000 train. data). Loss: 0.030008465051651\n",
      "Training log: 15 epoch (16768 / 60000 train. data). Loss: 0.017595961689949036\n",
      "Training log: 15 epoch (18048 / 60000 train. data). Loss: 0.008667425252497196\n",
      "Training log: 15 epoch (19328 / 60000 train. data). Loss: 0.00950220413506031\n",
      "Training log: 15 epoch (20608 / 60000 train. data). Loss: 0.007802359759807587\n",
      "Training log: 15 epoch (21888 / 60000 train. data). Loss: 0.018179459497332573\n",
      "Training log: 15 epoch (23168 / 60000 train. data). Loss: 0.01843196339905262\n",
      "Training log: 15 epoch (24448 / 60000 train. data). Loss: 0.012819749303162098\n",
      "Training log: 15 epoch (25728 / 60000 train. data). Loss: 0.008676070719957352\n",
      "Training log: 15 epoch (27008 / 60000 train. data). Loss: 0.009887076914310455\n",
      "Training log: 15 epoch (28288 / 60000 train. data). Loss: 0.0049593765288591385\n",
      "Training log: 15 epoch (29568 / 60000 train. data). Loss: 0.07138818502426147\n",
      "Training log: 15 epoch (30848 / 60000 train. data). Loss: 0.03386737033724785\n",
      "Training log: 15 epoch (32128 / 60000 train. data). Loss: 0.008522571995854378\n",
      "Training log: 15 epoch (33408 / 60000 train. data). Loss: 0.006400071084499359\n",
      "Training log: 15 epoch (34688 / 60000 train. data). Loss: 0.012919772416353226\n",
      "Training log: 15 epoch (35968 / 60000 train. data). Loss: 0.004567243158817291\n",
      "Training log: 15 epoch (37248 / 60000 train. data). Loss: 0.011613184586167336\n",
      "Training log: 15 epoch (38528 / 60000 train. data). Loss: 0.007839538156986237\n",
      "Training log: 15 epoch (39808 / 60000 train. data). Loss: 0.0036084093153476715\n",
      "Training log: 15 epoch (41088 / 60000 train. data). Loss: 0.013789817690849304\n",
      "Training log: 15 epoch (42368 / 60000 train. data). Loss: 0.007856737822294235\n",
      "Training log: 15 epoch (43648 / 60000 train. data). Loss: 0.01426321268081665\n",
      "Training log: 15 epoch (44928 / 60000 train. data). Loss: 0.009324321523308754\n",
      "Training log: 15 epoch (46208 / 60000 train. data). Loss: 0.04112056642770767\n",
      "Training log: 15 epoch (47488 / 60000 train. data). Loss: 0.021075313910841942\n",
      "Training log: 15 epoch (48768 / 60000 train. data). Loss: 0.01260010339319706\n",
      "Training log: 15 epoch (50048 / 60000 train. data). Loss: 0.005150165408849716\n",
      "Training log: 15 epoch (51328 / 60000 train. data). Loss: 0.04286464303731918\n",
      "Training log: 15 epoch (52608 / 60000 train. data). Loss: 0.02074495330452919\n",
      "Training log: 15 epoch (53888 / 60000 train. data). Loss: 0.012809528969228268\n",
      "Training log: 15 epoch (55168 / 60000 train. data). Loss: 0.006864888593554497\n",
      "Training log: 15 epoch (56448 / 60000 train. data). Loss: 0.012747356668114662\n",
      "Training log: 15 epoch (57728 / 60000 train. data). Loss: 0.0087638720870018\n",
      "Training log: 15 epoch (59008 / 60000 train. data). Loss: 0.015501204878091812\n",
      "Test loss (avg): 0.06533857176303863, Accuracy: 0.9802\n",
      "Training log: 16 epoch (128 / 60000 train. data). Loss: 0.028486406430602074\n",
      "Training log: 16 epoch (1408 / 60000 train. data). Loss: 0.006674375385046005\n",
      "Training log: 16 epoch (2688 / 60000 train. data). Loss: 0.009608224034309387\n",
      "Training log: 16 epoch (3968 / 60000 train. data). Loss: 0.005186755210161209\n",
      "Training log: 16 epoch (5248 / 60000 train. data). Loss: 0.008185360580682755\n",
      "Training log: 16 epoch (6528 / 60000 train. data). Loss: 0.01691751927137375\n",
      "Training log: 16 epoch (7808 / 60000 train. data). Loss: 0.019837936386466026\n",
      "Training log: 16 epoch (9088 / 60000 train. data). Loss: 0.011060629040002823\n",
      "Training log: 16 epoch (10368 / 60000 train. data). Loss: 0.009141065180301666\n",
      "Training log: 16 epoch (11648 / 60000 train. data). Loss: 0.005881024524569511\n",
      "Training log: 16 epoch (12928 / 60000 train. data). Loss: 0.006356565281748772\n",
      "Training log: 16 epoch (14208 / 60000 train. data). Loss: 0.011590790003538132\n",
      "Training log: 16 epoch (15488 / 60000 train. data). Loss: 0.03448254242539406\n",
      "Training log: 16 epoch (16768 / 60000 train. data). Loss: 0.014912803657352924\n",
      "Training log: 16 epoch (18048 / 60000 train. data). Loss: 0.006752144545316696\n",
      "Training log: 16 epoch (19328 / 60000 train. data). Loss: 0.0299527570605278\n",
      "Training log: 16 epoch (20608 / 60000 train. data). Loss: 0.013256428763270378\n",
      "Training log: 16 epoch (21888 / 60000 train. data). Loss: 0.01744486764073372\n",
      "Training log: 16 epoch (23168 / 60000 train. data). Loss: 0.007159937173128128\n",
      "Training log: 16 epoch (24448 / 60000 train. data). Loss: 0.004747986793518066\n",
      "Training log: 16 epoch (25728 / 60000 train. data). Loss: 0.0062932297587394714\n",
      "Training log: 16 epoch (27008 / 60000 train. data). Loss: 0.006791466847062111\n",
      "Training log: 16 epoch (28288 / 60000 train. data). Loss: 0.004061762243509293\n",
      "Training log: 16 epoch (29568 / 60000 train. data). Loss: 0.01013745553791523\n",
      "Training log: 16 epoch (30848 / 60000 train. data). Loss: 0.02347414195537567\n",
      "Training log: 16 epoch (32128 / 60000 train. data). Loss: 0.036755818873643875\n",
      "Training log: 16 epoch (33408 / 60000 train. data). Loss: 0.007014434784650803\n",
      "Training log: 16 epoch (34688 / 60000 train. data). Loss: 0.007929176092147827\n",
      "Training log: 16 epoch (35968 / 60000 train. data). Loss: 0.01994948647916317\n",
      "Training log: 16 epoch (37248 / 60000 train. data). Loss: 0.010633707046508789\n",
      "Training log: 16 epoch (38528 / 60000 train. data). Loss: 0.023280996829271317\n",
      "Training log: 16 epoch (39808 / 60000 train. data). Loss: 0.006010251119732857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 16 epoch (41088 / 60000 train. data). Loss: 0.003306800499558449\n",
      "Training log: 16 epoch (42368 / 60000 train. data). Loss: 0.019229354336857796\n",
      "Training log: 16 epoch (43648 / 60000 train. data). Loss: 0.010998949408531189\n",
      "Training log: 16 epoch (44928 / 60000 train. data). Loss: 0.0035711564123630524\n",
      "Training log: 16 epoch (46208 / 60000 train. data). Loss: 0.02222268097102642\n",
      "Training log: 16 epoch (47488 / 60000 train. data). Loss: 0.0029833298176527023\n",
      "Training log: 16 epoch (48768 / 60000 train. data). Loss: 0.00518084317445755\n",
      "Training log: 16 epoch (50048 / 60000 train. data). Loss: 0.011039737612009048\n",
      "Training log: 16 epoch (51328 / 60000 train. data). Loss: 0.003871031105518341\n",
      "Training log: 16 epoch (52608 / 60000 train. data). Loss: 0.023653695359826088\n",
      "Training log: 16 epoch (53888 / 60000 train. data). Loss: 0.008417051285505295\n",
      "Training log: 16 epoch (55168 / 60000 train. data). Loss: 0.06301162391901016\n",
      "Training log: 16 epoch (56448 / 60000 train. data). Loss: 0.03400890901684761\n",
      "Training log: 16 epoch (57728 / 60000 train. data). Loss: 0.05609918385744095\n",
      "Training log: 16 epoch (59008 / 60000 train. data). Loss: 0.024181123822927475\n",
      "Test loss (avg): 0.06301233727931976, Accuracy: 0.9819\n",
      "Training log: 17 epoch (128 / 60000 train. data). Loss: 0.003799052909016609\n",
      "Training log: 17 epoch (1408 / 60000 train. data). Loss: 0.01867019571363926\n",
      "Training log: 17 epoch (2688 / 60000 train. data). Loss: 0.010476415976881981\n",
      "Training log: 17 epoch (3968 / 60000 train. data). Loss: 0.004809390753507614\n",
      "Training log: 17 epoch (5248 / 60000 train. data). Loss: 0.005796592682600021\n",
      "Training log: 17 epoch (6528 / 60000 train. data). Loss: 0.006068350747227669\n",
      "Training log: 17 epoch (7808 / 60000 train. data). Loss: 0.011794419959187508\n",
      "Training log: 17 epoch (9088 / 60000 train. data). Loss: 0.004993923008441925\n",
      "Training log: 17 epoch (10368 / 60000 train. data). Loss: 0.01943221688270569\n",
      "Training log: 17 epoch (11648 / 60000 train. data). Loss: 0.005435001105070114\n",
      "Training log: 17 epoch (12928 / 60000 train. data). Loss: 0.004103027284145355\n",
      "Training log: 17 epoch (14208 / 60000 train. data). Loss: 0.006984880194067955\n",
      "Training log: 17 epoch (15488 / 60000 train. data). Loss: 0.009957969188690186\n",
      "Training log: 17 epoch (16768 / 60000 train. data). Loss: 0.028613459318876266\n",
      "Training log: 17 epoch (18048 / 60000 train. data). Loss: 0.01474672555923462\n",
      "Training log: 17 epoch (19328 / 60000 train. data). Loss: 0.010439526289701462\n",
      "Training log: 17 epoch (20608 / 60000 train. data). Loss: 0.005451284348964691\n",
      "Training log: 17 epoch (21888 / 60000 train. data). Loss: 0.01664063148200512\n",
      "Training log: 17 epoch (23168 / 60000 train. data). Loss: 0.0025071855634450912\n",
      "Training log: 17 epoch (24448 / 60000 train. data). Loss: 0.010133050382137299\n",
      "Training log: 17 epoch (25728 / 60000 train. data). Loss: 0.01595660299062729\n",
      "Training log: 17 epoch (27008 / 60000 train. data). Loss: 0.009895500726997852\n",
      "Training log: 17 epoch (28288 / 60000 train. data). Loss: 0.0039853863418102264\n",
      "Training log: 17 epoch (29568 / 60000 train. data). Loss: 0.010618064552545547\n",
      "Training log: 17 epoch (30848 / 60000 train. data). Loss: 0.006605001166462898\n",
      "Training log: 17 epoch (32128 / 60000 train. data). Loss: 0.004087524488568306\n",
      "Training log: 17 epoch (33408 / 60000 train. data). Loss: 0.01816738396883011\n",
      "Training log: 17 epoch (34688 / 60000 train. data). Loss: 0.026570819318294525\n",
      "Training log: 17 epoch (35968 / 60000 train. data). Loss: 0.005944252014160156\n",
      "Training log: 17 epoch (37248 / 60000 train. data). Loss: 0.01142021082341671\n",
      "Training log: 17 epoch (38528 / 60000 train. data). Loss: 0.002592332661151886\n",
      "Training log: 17 epoch (39808 / 60000 train. data). Loss: 0.006503370590507984\n",
      "Training log: 17 epoch (41088 / 60000 train. data). Loss: 0.020571459084749222\n",
      "Training log: 17 epoch (42368 / 60000 train. data). Loss: 0.01225191354751587\n",
      "Training log: 17 epoch (43648 / 60000 train. data). Loss: 0.008774247020483017\n",
      "Training log: 17 epoch (44928 / 60000 train. data). Loss: 0.00355454720556736\n",
      "Training log: 17 epoch (46208 / 60000 train. data). Loss: 0.003305211663246155\n",
      "Training log: 17 epoch (47488 / 60000 train. data). Loss: 0.0021241623908281326\n",
      "Training log: 17 epoch (48768 / 60000 train. data). Loss: 0.006271808408200741\n",
      "Training log: 17 epoch (50048 / 60000 train. data). Loss: 0.030559416860342026\n",
      "Training log: 17 epoch (51328 / 60000 train. data). Loss: 0.012132828123867512\n",
      "Training log: 17 epoch (52608 / 60000 train. data). Loss: 0.01010088250041008\n",
      "Training log: 17 epoch (53888 / 60000 train. data). Loss: 0.008696280419826508\n",
      "Training log: 17 epoch (55168 / 60000 train. data). Loss: 0.03180237114429474\n",
      "Training log: 17 epoch (56448 / 60000 train. data). Loss: 0.00878259725868702\n",
      "Training log: 17 epoch (57728 / 60000 train. data). Loss: 0.031894318759441376\n",
      "Training log: 17 epoch (59008 / 60000 train. data). Loss: 0.004326131194829941\n",
      "Test loss (avg): 0.06424263126850129, Accuracy: 0.9812\n",
      "Training log: 18 epoch (128 / 60000 train. data). Loss: 0.005294196307659149\n",
      "Training log: 18 epoch (1408 / 60000 train. data). Loss: 0.0027611926198005676\n",
      "Training log: 18 epoch (2688 / 60000 train. data). Loss: 0.030102234333753586\n",
      "Training log: 18 epoch (3968 / 60000 train. data). Loss: 0.006154842674732208\n",
      "Training log: 18 epoch (5248 / 60000 train. data). Loss: 0.005511920899152756\n",
      "Training log: 18 epoch (6528 / 60000 train. data). Loss: 0.006909117102622986\n",
      "Training log: 18 epoch (7808 / 60000 train. data). Loss: 0.007188867777585983\n",
      "Training log: 18 epoch (9088 / 60000 train. data). Loss: 0.006206240504980087\n",
      "Training log: 18 epoch (10368 / 60000 train. data). Loss: 0.006286660209298134\n",
      "Training log: 18 epoch (11648 / 60000 train. data). Loss: 0.018481947481632233\n",
      "Training log: 18 epoch (12928 / 60000 train. data). Loss: 0.011385438963770866\n",
      "Training log: 18 epoch (14208 / 60000 train. data). Loss: 0.0039579980075359344\n",
      "Training log: 18 epoch (15488 / 60000 train. data). Loss: 0.005227543413639069\n",
      "Training log: 18 epoch (16768 / 60000 train. data). Loss: 0.0033746864646673203\n",
      "Training log: 18 epoch (18048 / 60000 train. data). Loss: 0.006554696708917618\n",
      "Training log: 18 epoch (19328 / 60000 train. data). Loss: 0.0026548393070697784\n",
      "Training log: 18 epoch (20608 / 60000 train. data). Loss: 0.0021281512454152107\n",
      "Training log: 18 epoch (21888 / 60000 train. data). Loss: 0.007055345922708511\n",
      "Training log: 18 epoch (23168 / 60000 train. data). Loss: 0.008781109005212784\n",
      "Training log: 18 epoch (24448 / 60000 train. data). Loss: 0.010867299512028694\n",
      "Training log: 18 epoch (25728 / 60000 train. data). Loss: 0.005742117762565613\n",
      "Training log: 18 epoch (27008 / 60000 train. data). Loss: 0.008123284205794334\n",
      "Training log: 18 epoch (28288 / 60000 train. data). Loss: 0.024301499128341675\n",
      "Training log: 18 epoch (29568 / 60000 train. data). Loss: 0.0031824689358472824\n",
      "Training log: 18 epoch (30848 / 60000 train. data). Loss: 0.006080176681280136\n",
      "Training log: 18 epoch (32128 / 60000 train. data). Loss: 0.01260860450565815\n",
      "Training log: 18 epoch (33408 / 60000 train. data). Loss: 0.003868972882628441\n",
      "Training log: 18 epoch (34688 / 60000 train. data). Loss: 0.018892617896199226\n",
      "Training log: 18 epoch (35968 / 60000 train. data). Loss: 0.004967331886291504\n",
      "Training log: 18 epoch (37248 / 60000 train. data). Loss: 0.0068433284759521484\n",
      "Training log: 18 epoch (38528 / 60000 train. data). Loss: 0.0020644674077630043\n",
      "Training log: 18 epoch (39808 / 60000 train. data). Loss: 0.007148422300815582\n",
      "Training log: 18 epoch (41088 / 60000 train. data). Loss: 0.009661881253123283\n",
      "Training log: 18 epoch (42368 / 60000 train. data). Loss: 0.007300488650798798\n",
      "Training log: 18 epoch (43648 / 60000 train. data). Loss: 0.007140720263123512\n",
      "Training log: 18 epoch (44928 / 60000 train. data). Loss: 0.012107651680707932\n",
      "Training log: 18 epoch (46208 / 60000 train. data). Loss: 0.005485469475388527\n",
      "Training log: 18 epoch (47488 / 60000 train. data). Loss: 0.008449837565422058\n",
      "Training log: 18 epoch (48768 / 60000 train. data). Loss: 0.006048237904906273\n",
      "Training log: 18 epoch (50048 / 60000 train. data). Loss: 0.0017869845032691956\n",
      "Training log: 18 epoch (51328 / 60000 train. data). Loss: 0.0036950604990124702\n",
      "Training log: 18 epoch (52608 / 60000 train. data). Loss: 0.018943974748253822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 18 epoch (53888 / 60000 train. data). Loss: 0.004988580942153931\n",
      "Training log: 18 epoch (55168 / 60000 train. data). Loss: 0.004127036780118942\n",
      "Training log: 18 epoch (56448 / 60000 train. data). Loss: 0.003063960000872612\n",
      "Training log: 18 epoch (57728 / 60000 train. data). Loss: 0.0033054370433092117\n",
      "Training log: 18 epoch (59008 / 60000 train. data). Loss: 0.008224725723266602\n",
      "Test loss (avg): 0.06391300194263458, Accuracy: 0.9818\n",
      "Training log: 19 epoch (128 / 60000 train. data). Loss: 0.0014110468327999115\n",
      "Training log: 19 epoch (1408 / 60000 train. data). Loss: 0.020414691418409348\n",
      "Training log: 19 epoch (2688 / 60000 train. data). Loss: 0.003924533724784851\n",
      "Training log: 19 epoch (3968 / 60000 train. data). Loss: 0.004197664558887482\n",
      "Training log: 19 epoch (5248 / 60000 train. data). Loss: 0.0025051236152648926\n",
      "Training log: 19 epoch (6528 / 60000 train. data). Loss: 0.004896709695458412\n",
      "Training log: 19 epoch (7808 / 60000 train. data). Loss: 0.0032860711216926575\n",
      "Training log: 19 epoch (9088 / 60000 train. data). Loss: 0.004011297598481178\n",
      "Training log: 19 epoch (10368 / 60000 train. data). Loss: 0.005172371864318848\n",
      "Training log: 19 epoch (11648 / 60000 train. data). Loss: 0.005175992846488953\n",
      "Training log: 19 epoch (12928 / 60000 train. data). Loss: 0.003084799274802208\n",
      "Training log: 19 epoch (14208 / 60000 train. data). Loss: 0.0010425522923469543\n",
      "Training log: 19 epoch (15488 / 60000 train. data). Loss: 0.004319731146097183\n",
      "Training log: 19 epoch (16768 / 60000 train. data). Loss: 0.002937052398920059\n",
      "Training log: 19 epoch (18048 / 60000 train. data). Loss: 0.005877099931240082\n",
      "Training log: 19 epoch (19328 / 60000 train. data). Loss: 0.0036002546548843384\n",
      "Training log: 19 epoch (20608 / 60000 train. data). Loss: 0.008131816983222961\n",
      "Training log: 19 epoch (21888 / 60000 train. data). Loss: 0.002384170889854431\n",
      "Training log: 19 epoch (23168 / 60000 train. data). Loss: 0.0029360204935073853\n",
      "Training log: 19 epoch (24448 / 60000 train. data). Loss: 0.0038666948676109314\n",
      "Training log: 19 epoch (25728 / 60000 train. data). Loss: 0.00942455418407917\n",
      "Training log: 19 epoch (27008 / 60000 train. data). Loss: 0.0016990117728710175\n",
      "Training log: 19 epoch (28288 / 60000 train. data). Loss: 0.0039985086768865585\n",
      "Training log: 19 epoch (29568 / 60000 train. data). Loss: 0.00627632811665535\n",
      "Training log: 19 epoch (30848 / 60000 train. data). Loss: 0.003755258396267891\n",
      "Training log: 19 epoch (32128 / 60000 train. data). Loss: 0.004553960636258125\n",
      "Training log: 19 epoch (33408 / 60000 train. data). Loss: 0.003149455413222313\n",
      "Training log: 19 epoch (34688 / 60000 train. data). Loss: 0.002814345061779022\n",
      "Training log: 19 epoch (35968 / 60000 train. data). Loss: 0.002463802695274353\n",
      "Training log: 19 epoch (37248 / 60000 train. data). Loss: 0.0023176148533821106\n",
      "Training log: 19 epoch (38528 / 60000 train. data). Loss: 0.010838814079761505\n",
      "Training log: 19 epoch (39808 / 60000 train. data). Loss: 0.011604184284806252\n",
      "Training log: 19 epoch (41088 / 60000 train. data). Loss: 0.012863699346780777\n",
      "Training log: 19 epoch (42368 / 60000 train. data). Loss: 0.002576451748609543\n",
      "Training log: 19 epoch (43648 / 60000 train. data). Loss: 0.03154349327087402\n",
      "Training log: 19 epoch (44928 / 60000 train. data). Loss: 0.004644237458705902\n",
      "Training log: 19 epoch (46208 / 60000 train. data). Loss: 0.003985730931162834\n",
      "Training log: 19 epoch (47488 / 60000 train. data). Loss: 0.0037023723125457764\n",
      "Training log: 19 epoch (48768 / 60000 train. data). Loss: 0.0027751363813877106\n",
      "Training log: 19 epoch (50048 / 60000 train. data). Loss: 0.004267983138561249\n",
      "Training log: 19 epoch (51328 / 60000 train. data). Loss: 0.0070329997688531876\n",
      "Training log: 19 epoch (52608 / 60000 train. data). Loss: 0.004446141421794891\n",
      "Training log: 19 epoch (53888 / 60000 train. data). Loss: 0.004170410335063934\n",
      "Training log: 19 epoch (55168 / 60000 train. data). Loss: 0.006797828711569309\n",
      "Training log: 19 epoch (56448 / 60000 train. data). Loss: 0.004643622785806656\n",
      "Training log: 19 epoch (57728 / 60000 train. data). Loss: 0.002441834658384323\n",
      "Training log: 19 epoch (59008 / 60000 train. data). Loss: 0.005283026956021786\n",
      "Test loss (avg): 0.06729843227863312, Accuracy: 0.9809\n",
      "Training log: 20 epoch (128 / 60000 train. data). Loss: 0.002567017450928688\n",
      "Training log: 20 epoch (1408 / 60000 train. data). Loss: 0.0035778526216745377\n",
      "Training log: 20 epoch (2688 / 60000 train. data). Loss: 0.00729685602709651\n",
      "Training log: 20 epoch (3968 / 60000 train. data). Loss: 0.0016427896916866302\n",
      "Training log: 20 epoch (5248 / 60000 train. data). Loss: 0.006064057350158691\n",
      "Training log: 20 epoch (6528 / 60000 train. data). Loss: 0.0021073538810014725\n",
      "Training log: 20 epoch (7808 / 60000 train. data). Loss: 0.008825171738862991\n",
      "Training log: 20 epoch (9088 / 60000 train. data). Loss: 0.0026345904916524887\n",
      "Training log: 20 epoch (10368 / 60000 train. data). Loss: 0.007737066596746445\n",
      "Training log: 20 epoch (11648 / 60000 train. data). Loss: 0.006201662123203278\n",
      "Training log: 20 epoch (12928 / 60000 train. data). Loss: 0.0034715384244918823\n",
      "Training log: 20 epoch (14208 / 60000 train. data). Loss: 0.001346401870250702\n",
      "Training log: 20 epoch (15488 / 60000 train. data). Loss: 0.003142768517136574\n",
      "Training log: 20 epoch (16768 / 60000 train. data). Loss: 0.003224443644285202\n",
      "Training log: 20 epoch (18048 / 60000 train. data). Loss: 0.0016517508774995804\n",
      "Training log: 20 epoch (19328 / 60000 train. data). Loss: 0.010641602799296379\n",
      "Training log: 20 epoch (20608 / 60000 train. data). Loss: 0.0013827532529830933\n",
      "Training log: 20 epoch (21888 / 60000 train. data). Loss: 0.0023047346621751785\n",
      "Training log: 20 epoch (23168 / 60000 train. data). Loss: 0.004424843937158585\n",
      "Training log: 20 epoch (24448 / 60000 train. data). Loss: 0.0038507822901010513\n",
      "Training log: 20 epoch (25728 / 60000 train. data). Loss: 0.005711978301405907\n",
      "Training log: 20 epoch (27008 / 60000 train. data). Loss: 0.019797546789050102\n",
      "Training log: 20 epoch (28288 / 60000 train. data). Loss: 0.013185705989599228\n",
      "Training log: 20 epoch (29568 / 60000 train. data). Loss: 0.0032556410878896713\n",
      "Training log: 20 epoch (30848 / 60000 train. data). Loss: 0.003208193928003311\n",
      "Training log: 20 epoch (32128 / 60000 train. data). Loss: 0.0030280854552984238\n",
      "Training log: 20 epoch (33408 / 60000 train. data). Loss: 0.0010536722838878632\n",
      "Training log: 20 epoch (34688 / 60000 train. data). Loss: 0.002452649176120758\n",
      "Training log: 20 epoch (35968 / 60000 train. data). Loss: 0.003636833280324936\n",
      "Training log: 20 epoch (37248 / 60000 train. data). Loss: 0.004112261813133955\n",
      "Training log: 20 epoch (38528 / 60000 train. data). Loss: 0.0009735375642776489\n",
      "Training log: 20 epoch (39808 / 60000 train. data). Loss: 0.004611391574144363\n",
      "Training log: 20 epoch (41088 / 60000 train. data). Loss: 0.004153138026595116\n",
      "Training log: 20 epoch (42368 / 60000 train. data). Loss: 0.011713463813066483\n",
      "Training log: 20 epoch (43648 / 60000 train. data). Loss: 0.009085463359951973\n",
      "Training log: 20 epoch (44928 / 60000 train. data). Loss: 0.010228748433291912\n",
      "Training log: 20 epoch (46208 / 60000 train. data). Loss: 0.0019362904131412506\n",
      "Training log: 20 epoch (47488 / 60000 train. data). Loss: 0.003886893391609192\n",
      "Training log: 20 epoch (48768 / 60000 train. data). Loss: 0.004254745319485664\n",
      "Training log: 20 epoch (50048 / 60000 train. data). Loss: 0.008648514747619629\n",
      "Training log: 20 epoch (51328 / 60000 train. data). Loss: 0.010997837409377098\n",
      "Training log: 20 epoch (52608 / 60000 train. data). Loss: 0.004133269190788269\n",
      "Training log: 20 epoch (53888 / 60000 train. data). Loss: 0.00289330817759037\n",
      "Training log: 20 epoch (55168 / 60000 train. data). Loss: 0.011239057406783104\n",
      "Training log: 20 epoch (56448 / 60000 train. data). Loss: 0.007698453962802887\n",
      "Training log: 20 epoch (57728 / 60000 train. data). Loss: 0.009981142356991768\n",
      "Training log: 20 epoch (59008 / 60000 train. data). Loss: 0.0026082992553710938\n",
      "Test loss (avg): 0.07088353017568588, Accuracy: 0.9799\n"
     ]
    }
   ],
   "source": [
    "for i_epoch in range(num_epoch):\n",
    "    loss = None\n",
    "    \n",
    "    net.train(True)\n",
    "    \n",
    "    for i, (data, target) in enumerate(loaders['train']):\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = f.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(i_epoch+1, (i+1)*128, loss.item()))\n",
    "    \n",
    "    history['train_loss'].append(loss)\n",
    "    \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = net(data)\n",
    "            test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= 10000\n",
    "    print('Test loss (avg): {}, Accuracy: {}'.format(test_loss, correct / 10000))\n",
    "    \n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(correct / 10000)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [tensor(0.4600, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)], 'test_loss': [0.26288632473945617, 0.19992919902801515, 0.16125908455848695, 0.1258249189376831, 0.11241939449310302, 0.09403710088729858, 0.08853961670398712, 0.08204093081951142, 0.07768251745700837, 0.07529574549198151, 0.06827726354599, 0.06576207578182221, 0.06716034919023514, 0.061372332894802095, 0.06533857176303863, 0.06301233727931976, 0.06424263126850129, 0.06391300194263458, 0.06729843227863312, 0.07088353017568588], 'test_acc': [0.9229, 0.9411, 0.9501, 0.962, 0.9654, 0.9718, 0.974, 0.9753, 0.9768, 0.9773, 0.9789, 0.9803, 0.9811, 0.9826, 0.9802, 0.9819, 0.9812, 0.9818, 0.9809, 0.9799]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epoch\n",
    "print(history)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['train_loss'])), history['train_loss'], label='train_loss')\n",
    "plt.plot(range(len(history['test_loss'])), history['test_loss'], label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['test_acc'])), history['test_acc'])\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('test_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
