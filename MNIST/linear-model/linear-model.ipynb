{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rightcode.co.jp/blog/information-technology/pytorch-mnist-learning\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return f.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    return {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20\n",
    "history = {\n",
    "    'train_loss':[],\n",
    "    'test_loss':[],\n",
    "    'test_acc':[]\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = MyNet().to(device)\n",
    "loaders = load_MNIST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 2.3162682056427\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 1.7962558269500732\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 1.3721257448196411\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 1.0675668716430664\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.8228241205215454\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.7336229085922241\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.5817764401435852\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.5774012804031372\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.44856444001197815\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.45691654086112976\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.48051416873931885\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.42083701491355896\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.45340049266815186\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.4648989140987396\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.3717310428619385\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.29692792892456055\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.300973117351532\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.3068314790725708\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.3458016812801361\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.3206237554550171\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.41640087962150574\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.25648611783981323\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.20553193986415863\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.45562034845352173\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.3124499022960663\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.3202727437019348\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.3845460116863251\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.2940314710140228\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.2776692807674408\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.2920657694339752\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.377407968044281\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.3009207546710968\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.3624923527240753\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.22972510755062103\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.3217697739601135\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.1988200843334198\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.37946808338165283\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.2684914171695709\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.283882737159729\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.38332268595695496\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.3898957371711731\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.24514570832252502\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.38510069251060486\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.3415628671646118\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.2991776466369629\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.2513252794742584\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.33144545555114746\n",
      "Test loss (avg): 0.2569544126033783, Accuracy: 0.9248\n",
      "Training log: 2 epoch (128 / 60000 train. data). Loss: 0.3376501500606537\n",
      "Training log: 2 epoch (1408 / 60000 train. data). Loss: 0.2907417118549347\n",
      "Training log: 2 epoch (2688 / 60000 train. data). Loss: 0.24149878323078156\n",
      "Training log: 2 epoch (3968 / 60000 train. data). Loss: 0.34984642267227173\n",
      "Training log: 2 epoch (5248 / 60000 train. data). Loss: 0.20895090699195862\n",
      "Training log: 2 epoch (6528 / 60000 train. data). Loss: 0.20318454504013062\n",
      "Training log: 2 epoch (7808 / 60000 train. data). Loss: 0.18202528357505798\n",
      "Training log: 2 epoch (9088 / 60000 train. data). Loss: 0.2170390486717224\n",
      "Training log: 2 epoch (10368 / 60000 train. data). Loss: 0.2942376136779785\n",
      "Training log: 2 epoch (11648 / 60000 train. data). Loss: 0.1852857768535614\n",
      "Training log: 2 epoch (12928 / 60000 train. data). Loss: 0.3747940957546234\n",
      "Training log: 2 epoch (14208 / 60000 train. data). Loss: 0.2565813958644867\n",
      "Training log: 2 epoch (15488 / 60000 train. data). Loss: 0.1447008103132248\n",
      "Training log: 2 epoch (16768 / 60000 train. data). Loss: 0.331951767206192\n",
      "Training log: 2 epoch (18048 / 60000 train. data). Loss: 0.2884860336780548\n",
      "Training log: 2 epoch (19328 / 60000 train. data). Loss: 0.2741613984107971\n",
      "Training log: 2 epoch (20608 / 60000 train. data). Loss: 0.14453499019145966\n",
      "Training log: 2 epoch (21888 / 60000 train. data). Loss: 0.20311203598976135\n",
      "Training log: 2 epoch (23168 / 60000 train. data). Loss: 0.13822582364082336\n",
      "Training log: 2 epoch (24448 / 60000 train. data). Loss: 0.2841150164604187\n",
      "Training log: 2 epoch (25728 / 60000 train. data). Loss: 0.14314894378185272\n",
      "Training log: 2 epoch (27008 / 60000 train. data). Loss: 0.2952541410923004\n",
      "Training log: 2 epoch (28288 / 60000 train. data). Loss: 0.1800987273454666\n",
      "Training log: 2 epoch (29568 / 60000 train. data). Loss: 0.22871962189674377\n",
      "Training log: 2 epoch (30848 / 60000 train. data). Loss: 0.30066460371017456\n",
      "Training log: 2 epoch (32128 / 60000 train. data). Loss: 0.27836552262306213\n",
      "Training log: 2 epoch (33408 / 60000 train. data). Loss: 0.32689040899276733\n",
      "Training log: 2 epoch (34688 / 60000 train. data). Loss: 0.22300919890403748\n",
      "Training log: 2 epoch (35968 / 60000 train. data). Loss: 0.22064386308193207\n",
      "Training log: 2 epoch (37248 / 60000 train. data). Loss: 0.2641246020793915\n",
      "Training log: 2 epoch (38528 / 60000 train. data). Loss: 0.254208505153656\n",
      "Training log: 2 epoch (39808 / 60000 train. data). Loss: 0.3320692181587219\n",
      "Training log: 2 epoch (41088 / 60000 train. data). Loss: 0.2553838789463043\n",
      "Training log: 2 epoch (42368 / 60000 train. data). Loss: 0.25448912382125854\n",
      "Training log: 2 epoch (43648 / 60000 train. data). Loss: 0.17998403310775757\n",
      "Training log: 2 epoch (44928 / 60000 train. data). Loss: 0.3359288275241852\n",
      "Training log: 2 epoch (46208 / 60000 train. data). Loss: 0.28765812516212463\n",
      "Training log: 2 epoch (47488 / 60000 train. data). Loss: 0.21700634062290192\n",
      "Training log: 2 epoch (48768 / 60000 train. data). Loss: 0.21084511280059814\n",
      "Training log: 2 epoch (50048 / 60000 train. data). Loss: 0.2786870300769806\n",
      "Training log: 2 epoch (51328 / 60000 train. data). Loss: 0.2086745798587799\n",
      "Training log: 2 epoch (52608 / 60000 train. data). Loss: 0.2173852175474167\n",
      "Training log: 2 epoch (53888 / 60000 train. data). Loss: 0.17466717958450317\n",
      "Training log: 2 epoch (55168 / 60000 train. data). Loss: 0.19148162007331848\n",
      "Training log: 2 epoch (56448 / 60000 train. data). Loss: 0.2260693907737732\n",
      "Training log: 2 epoch (57728 / 60000 train. data). Loss: 0.13369295001029968\n",
      "Training log: 2 epoch (59008 / 60000 train. data). Loss: 0.2563880681991577\n",
      "Test loss (avg): 0.19186804113388062, Accuracy: 0.9426\n",
      "Training log: 3 epoch (128 / 60000 train. data). Loss: 0.22033581137657166\n",
      "Training log: 3 epoch (1408 / 60000 train. data). Loss: 0.23641563951969147\n",
      "Training log: 3 epoch (2688 / 60000 train. data). Loss: 0.2303418517112732\n",
      "Training log: 3 epoch (3968 / 60000 train. data). Loss: 0.17635071277618408\n",
      "Training log: 3 epoch (5248 / 60000 train. data). Loss: 0.12386640906333923\n",
      "Training log: 3 epoch (6528 / 60000 train. data). Loss: 0.16687187552452087\n",
      "Training log: 3 epoch (7808 / 60000 train. data). Loss: 0.2480669915676117\n",
      "Training log: 3 epoch (9088 / 60000 train. data). Loss: 0.13249456882476807\n",
      "Training log: 3 epoch (10368 / 60000 train. data). Loss: 0.17759433388710022\n",
      "Training log: 3 epoch (11648 / 60000 train. data). Loss: 0.10401995480060577\n",
      "Training log: 3 epoch (12928 / 60000 train. data). Loss: 0.07569954544305801\n",
      "Training log: 3 epoch (14208 / 60000 train. data). Loss: 0.1556587517261505\n",
      "Training log: 3 epoch (15488 / 60000 train. data). Loss: 0.10195639729499817\n",
      "Training log: 3 epoch (16768 / 60000 train. data). Loss: 0.22125330567359924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 3 epoch (18048 / 60000 train. data). Loss: 0.1395236849784851\n",
      "Training log: 3 epoch (19328 / 60000 train. data). Loss: 0.27121981978416443\n",
      "Training log: 3 epoch (20608 / 60000 train. data). Loss: 0.19735509157180786\n",
      "Training log: 3 epoch (21888 / 60000 train. data). Loss: 0.20224367082118988\n",
      "Training log: 3 epoch (23168 / 60000 train. data). Loss: 0.34012436866760254\n",
      "Training log: 3 epoch (24448 / 60000 train. data). Loss: 0.1712912619113922\n",
      "Training log: 3 epoch (25728 / 60000 train. data). Loss: 0.11843408644199371\n",
      "Training log: 3 epoch (27008 / 60000 train. data). Loss: 0.12495217472314835\n",
      "Training log: 3 epoch (28288 / 60000 train. data). Loss: 0.18759121000766754\n",
      "Training log: 3 epoch (29568 / 60000 train. data). Loss: 0.17633309960365295\n",
      "Training log: 3 epoch (30848 / 60000 train. data). Loss: 0.17662309110164642\n",
      "Training log: 3 epoch (32128 / 60000 train. data). Loss: 0.17821402847766876\n",
      "Training log: 3 epoch (33408 / 60000 train. data). Loss: 0.12728384137153625\n",
      "Training log: 3 epoch (34688 / 60000 train. data). Loss: 0.20914986729621887\n",
      "Training log: 3 epoch (35968 / 60000 train. data). Loss: 0.15299052000045776\n",
      "Training log: 3 epoch (37248 / 60000 train. data). Loss: 0.11856546252965927\n",
      "Training log: 3 epoch (38528 / 60000 train. data). Loss: 0.1775745153427124\n",
      "Training log: 3 epoch (39808 / 60000 train. data). Loss: 0.20952054858207703\n",
      "Training log: 3 epoch (41088 / 60000 train. data). Loss: 0.20827722549438477\n",
      "Training log: 3 epoch (42368 / 60000 train. data). Loss: 0.1505429595708847\n",
      "Training log: 3 epoch (43648 / 60000 train. data). Loss: 0.21555139124393463\n",
      "Training log: 3 epoch (44928 / 60000 train. data). Loss: 0.14694838225841522\n",
      "Training log: 3 epoch (46208 / 60000 train. data). Loss: 0.33069148659706116\n",
      "Training log: 3 epoch (47488 / 60000 train. data). Loss: 0.17170168459415436\n",
      "Training log: 3 epoch (48768 / 60000 train. data). Loss: 0.18674883246421814\n",
      "Training log: 3 epoch (50048 / 60000 train. data). Loss: 0.23969987034797668\n",
      "Training log: 3 epoch (51328 / 60000 train. data). Loss: 0.11177504807710648\n",
      "Training log: 3 epoch (52608 / 60000 train. data). Loss: 0.12405969202518463\n",
      "Training log: 3 epoch (53888 / 60000 train. data). Loss: 0.2628955543041229\n",
      "Training log: 3 epoch (55168 / 60000 train. data). Loss: 0.2034534215927124\n",
      "Training log: 3 epoch (56448 / 60000 train. data). Loss: 0.10421086847782135\n",
      "Training log: 3 epoch (57728 / 60000 train. data). Loss: 0.1335570365190506\n",
      "Training log: 3 epoch (59008 / 60000 train. data). Loss: 0.12552621960639954\n",
      "Test loss (avg): 0.1537026689052582, Accuracy: 0.9554\n",
      "Training log: 4 epoch (128 / 60000 train. data). Loss: 0.12534379959106445\n",
      "Training log: 4 epoch (1408 / 60000 train. data). Loss: 0.1429276019334793\n",
      "Training log: 4 epoch (2688 / 60000 train. data). Loss: 0.2019164115190506\n",
      "Training log: 4 epoch (3968 / 60000 train. data). Loss: 0.11670801043510437\n",
      "Training log: 4 epoch (5248 / 60000 train. data). Loss: 0.17019683122634888\n",
      "Training log: 4 epoch (6528 / 60000 train. data). Loss: 0.19492195546627045\n",
      "Training log: 4 epoch (7808 / 60000 train. data). Loss: 0.17492789030075073\n",
      "Training log: 4 epoch (9088 / 60000 train. data). Loss: 0.20930904150009155\n",
      "Training log: 4 epoch (10368 / 60000 train. data). Loss: 0.0844789445400238\n",
      "Training log: 4 epoch (11648 / 60000 train. data). Loss: 0.09731298685073853\n",
      "Training log: 4 epoch (12928 / 60000 train. data). Loss: 0.1553666591644287\n",
      "Training log: 4 epoch (14208 / 60000 train. data). Loss: 0.07327011227607727\n",
      "Training log: 4 epoch (15488 / 60000 train. data). Loss: 0.20477138459682465\n",
      "Training log: 4 epoch (16768 / 60000 train. data). Loss: 0.14148549735546112\n",
      "Training log: 4 epoch (18048 / 60000 train. data). Loss: 0.0930214375257492\n",
      "Training log: 4 epoch (19328 / 60000 train. data). Loss: 0.06630363315343857\n",
      "Training log: 4 epoch (20608 / 60000 train. data). Loss: 0.14743450284004211\n",
      "Training log: 4 epoch (21888 / 60000 train. data). Loss: 0.16492965817451477\n",
      "Training log: 4 epoch (23168 / 60000 train. data). Loss: 0.23782841861248016\n",
      "Training log: 4 epoch (24448 / 60000 train. data). Loss: 0.09448401629924774\n",
      "Training log: 4 epoch (25728 / 60000 train. data). Loss: 0.10780949145555496\n",
      "Training log: 4 epoch (27008 / 60000 train. data). Loss: 0.10461938381195068\n",
      "Training log: 4 epoch (28288 / 60000 train. data). Loss: 0.14821802079677582\n",
      "Training log: 4 epoch (29568 / 60000 train. data). Loss: 0.11552826315164566\n",
      "Training log: 4 epoch (30848 / 60000 train. data). Loss: 0.14078044891357422\n",
      "Training log: 4 epoch (32128 / 60000 train. data). Loss: 0.2568489611148834\n",
      "Training log: 4 epoch (33408 / 60000 train. data). Loss: 0.07340514659881592\n",
      "Training log: 4 epoch (34688 / 60000 train. data). Loss: 0.13738684356212616\n",
      "Training log: 4 epoch (35968 / 60000 train. data). Loss: 0.14928200840950012\n",
      "Training log: 4 epoch (37248 / 60000 train. data). Loss: 0.09044648706912994\n",
      "Training log: 4 epoch (38528 / 60000 train. data). Loss: 0.1988762617111206\n",
      "Training log: 4 epoch (39808 / 60000 train. data). Loss: 0.25325220823287964\n",
      "Training log: 4 epoch (41088 / 60000 train. data). Loss: 0.1587369441986084\n",
      "Training log: 4 epoch (42368 / 60000 train. data). Loss: 0.08030115067958832\n",
      "Training log: 4 epoch (43648 / 60000 train. data). Loss: 0.16671894490718842\n",
      "Training log: 4 epoch (44928 / 60000 train. data). Loss: 0.07410340756177902\n",
      "Training log: 4 epoch (46208 / 60000 train. data). Loss: 0.08975480496883392\n",
      "Training log: 4 epoch (47488 / 60000 train. data). Loss: 0.07872143387794495\n",
      "Training log: 4 epoch (48768 / 60000 train. data). Loss: 0.08627697080373764\n",
      "Training log: 4 epoch (50048 / 60000 train. data). Loss: 0.11871904134750366\n",
      "Training log: 4 epoch (51328 / 60000 train. data). Loss: 0.20408159494400024\n",
      "Training log: 4 epoch (52608 / 60000 train. data). Loss: 0.14500361680984497\n",
      "Training log: 4 epoch (53888 / 60000 train. data). Loss: 0.1791878044605255\n",
      "Training log: 4 epoch (55168 / 60000 train. data). Loss: 0.08943648636341095\n",
      "Training log: 4 epoch (56448 / 60000 train. data). Loss: 0.08755148947238922\n",
      "Training log: 4 epoch (57728 / 60000 train. data). Loss: 0.08433158695697784\n",
      "Training log: 4 epoch (59008 / 60000 train. data). Loss: 0.154013991355896\n",
      "Test loss (avg): 0.13245936026573182, Accuracy: 0.9599\n",
      "Training log: 5 epoch (128 / 60000 train. data). Loss: 0.10060200840234756\n",
      "Training log: 5 epoch (1408 / 60000 train. data). Loss: 0.13454890251159668\n",
      "Training log: 5 epoch (2688 / 60000 train. data). Loss: 0.10145309567451477\n",
      "Training log: 5 epoch (3968 / 60000 train. data). Loss: 0.1720268428325653\n",
      "Training log: 5 epoch (5248 / 60000 train. data). Loss: 0.11658056080341339\n",
      "Training log: 5 epoch (6528 / 60000 train. data). Loss: 0.09265156090259552\n",
      "Training log: 5 epoch (7808 / 60000 train. data). Loss: 0.08492457121610641\n",
      "Training log: 5 epoch (9088 / 60000 train. data). Loss: 0.058492645621299744\n",
      "Training log: 5 epoch (10368 / 60000 train. data). Loss: 0.18386352062225342\n",
      "Training log: 5 epoch (11648 / 60000 train. data). Loss: 0.10570845007896423\n",
      "Training log: 5 epoch (12928 / 60000 train. data). Loss: 0.07545571029186249\n",
      "Training log: 5 epoch (14208 / 60000 train. data). Loss: 0.1553184986114502\n",
      "Training log: 5 epoch (15488 / 60000 train. data). Loss: 0.09297052770853043\n",
      "Training log: 5 epoch (16768 / 60000 train. data). Loss: 0.10667484998703003\n",
      "Training log: 5 epoch (18048 / 60000 train. data). Loss: 0.13175402581691742\n",
      "Training log: 5 epoch (19328 / 60000 train. data). Loss: 0.03358444944024086\n",
      "Training log: 5 epoch (20608 / 60000 train. data). Loss: 0.05459455028176308\n",
      "Training log: 5 epoch (21888 / 60000 train. data). Loss: 0.0668981671333313\n",
      "Training log: 5 epoch (23168 / 60000 train. data). Loss: 0.15827354788780212\n",
      "Training log: 5 epoch (24448 / 60000 train. data). Loss: 0.06116265431046486\n",
      "Training log: 5 epoch (25728 / 60000 train. data). Loss: 0.06462539732456207\n",
      "Training log: 5 epoch (27008 / 60000 train. data). Loss: 0.050660159438848495\n",
      "Training log: 5 epoch (28288 / 60000 train. data). Loss: 0.0505991093814373\n",
      "Training log: 5 epoch (29568 / 60000 train. data). Loss: 0.059499382972717285\n",
      "Training log: 5 epoch (30848 / 60000 train. data). Loss: 0.10195720195770264\n",
      "Training log: 5 epoch (32128 / 60000 train. data). Loss: 0.14842352271080017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 5 epoch (33408 / 60000 train. data). Loss: 0.09063932299613953\n",
      "Training log: 5 epoch (34688 / 60000 train. data). Loss: 0.12141234427690506\n",
      "Training log: 5 epoch (35968 / 60000 train. data). Loss: 0.05259301885962486\n",
      "Training log: 5 epoch (37248 / 60000 train. data). Loss: 0.0984300971031189\n",
      "Training log: 5 epoch (38528 / 60000 train. data). Loss: 0.09386767446994781\n",
      "Training log: 5 epoch (39808 / 60000 train. data). Loss: 0.14235177636146545\n",
      "Training log: 5 epoch (41088 / 60000 train. data). Loss: 0.16881124675273895\n",
      "Training log: 5 epoch (42368 / 60000 train. data). Loss: 0.11027459800243378\n",
      "Training log: 5 epoch (43648 / 60000 train. data). Loss: 0.11123707890510559\n",
      "Training log: 5 epoch (44928 / 60000 train. data). Loss: 0.08134406805038452\n",
      "Training log: 5 epoch (46208 / 60000 train. data). Loss: 0.10475914180278778\n",
      "Training log: 5 epoch (47488 / 60000 train. data). Loss: 0.08341480046510696\n",
      "Training log: 5 epoch (48768 / 60000 train. data). Loss: 0.09280708432197571\n",
      "Training log: 5 epoch (50048 / 60000 train. data). Loss: 0.16082258522510529\n",
      "Training log: 5 epoch (51328 / 60000 train. data). Loss: 0.01742720603942871\n",
      "Training log: 5 epoch (52608 / 60000 train. data). Loss: 0.05176006630063057\n",
      "Training log: 5 epoch (53888 / 60000 train. data). Loss: 0.1920686811208725\n",
      "Training log: 5 epoch (55168 / 60000 train. data). Loss: 0.04345772787928581\n",
      "Training log: 5 epoch (56448 / 60000 train. data). Loss: 0.10388689488172531\n",
      "Training log: 5 epoch (57728 / 60000 train. data). Loss: 0.1683080643415451\n",
      "Training log: 5 epoch (59008 / 60000 train. data). Loss: 0.09817900508642197\n",
      "Test loss (avg): 0.11040756318569184, Accuracy: 0.9669\n",
      "Training log: 6 epoch (128 / 60000 train. data). Loss: 0.08480027318000793\n",
      "Training log: 6 epoch (1408 / 60000 train. data). Loss: 0.10434794425964355\n",
      "Training log: 6 epoch (2688 / 60000 train. data). Loss: 0.1851302832365036\n",
      "Training log: 6 epoch (3968 / 60000 train. data). Loss: 0.06185184791684151\n",
      "Training log: 6 epoch (5248 / 60000 train. data). Loss: 0.08357647806406021\n",
      "Training log: 6 epoch (6528 / 60000 train. data). Loss: 0.14961230754852295\n",
      "Training log: 6 epoch (7808 / 60000 train. data). Loss: 0.08544057607650757\n",
      "Training log: 6 epoch (9088 / 60000 train. data). Loss: 0.0768728107213974\n",
      "Training log: 6 epoch (10368 / 60000 train. data). Loss: 0.10715292394161224\n",
      "Training log: 6 epoch (11648 / 60000 train. data). Loss: 0.07725988328456879\n",
      "Training log: 6 epoch (12928 / 60000 train. data). Loss: 0.08773086220026016\n",
      "Training log: 6 epoch (14208 / 60000 train. data). Loss: 0.16208617389202118\n",
      "Training log: 6 epoch (15488 / 60000 train. data). Loss: 0.1293245106935501\n",
      "Training log: 6 epoch (16768 / 60000 train. data). Loss: 0.03544018790125847\n",
      "Training log: 6 epoch (18048 / 60000 train. data). Loss: 0.06633014976978302\n",
      "Training log: 6 epoch (19328 / 60000 train. data). Loss: 0.0851096659898758\n",
      "Training log: 6 epoch (20608 / 60000 train. data). Loss: 0.17505119740962982\n",
      "Training log: 6 epoch (21888 / 60000 train. data). Loss: 0.13059403002262115\n",
      "Training log: 6 epoch (23168 / 60000 train. data). Loss: 0.0332157239317894\n",
      "Training log: 6 epoch (24448 / 60000 train. data). Loss: 0.08246195316314697\n",
      "Training log: 6 epoch (25728 / 60000 train. data). Loss: 0.1043921560049057\n",
      "Training log: 6 epoch (27008 / 60000 train. data). Loss: 0.1854112446308136\n",
      "Training log: 6 epoch (28288 / 60000 train. data). Loss: 0.06180762127041817\n",
      "Training log: 6 epoch (29568 / 60000 train. data). Loss: 0.05473708361387253\n",
      "Training log: 6 epoch (30848 / 60000 train. data). Loss: 0.1394190490245819\n",
      "Training log: 6 epoch (32128 / 60000 train. data). Loss: 0.04233564808964729\n",
      "Training log: 6 epoch (33408 / 60000 train. data). Loss: 0.1377880871295929\n",
      "Training log: 6 epoch (34688 / 60000 train. data). Loss: 0.1470986008644104\n",
      "Training log: 6 epoch (35968 / 60000 train. data). Loss: 0.06641670316457748\n",
      "Training log: 6 epoch (37248 / 60000 train. data). Loss: 0.031034808605909348\n",
      "Training log: 6 epoch (38528 / 60000 train. data). Loss: 0.09066520631313324\n",
      "Training log: 6 epoch (39808 / 60000 train. data). Loss: 0.05415680259466171\n",
      "Training log: 6 epoch (41088 / 60000 train. data). Loss: 0.11436571180820465\n",
      "Training log: 6 epoch (42368 / 60000 train. data). Loss: 0.05771499127149582\n",
      "Training log: 6 epoch (43648 / 60000 train. data). Loss: 0.0481717549264431\n",
      "Training log: 6 epoch (44928 / 60000 train. data). Loss: 0.17536139488220215\n",
      "Training log: 6 epoch (46208 / 60000 train. data). Loss: 0.12018633633852005\n",
      "Training log: 6 epoch (47488 / 60000 train. data). Loss: 0.05748101696372032\n",
      "Training log: 6 epoch (48768 / 60000 train. data). Loss: 0.05841634422540665\n",
      "Training log: 6 epoch (50048 / 60000 train. data). Loss: 0.10069626569747925\n",
      "Training log: 6 epoch (51328 / 60000 train. data). Loss: 0.05393204838037491\n",
      "Training log: 6 epoch (52608 / 60000 train. data). Loss: 0.054538849741220474\n",
      "Training log: 6 epoch (53888 / 60000 train. data). Loss: 0.08428992331027985\n",
      "Training log: 6 epoch (55168 / 60000 train. data). Loss: 0.0476849339902401\n",
      "Training log: 6 epoch (56448 / 60000 train. data). Loss: 0.021550506353378296\n",
      "Training log: 6 epoch (57728 / 60000 train. data). Loss: 0.08173631131649017\n",
      "Training log: 6 epoch (59008 / 60000 train. data). Loss: 0.04908151552081108\n",
      "Test loss (avg): 0.09716337895393372, Accuracy: 0.9711\n",
      "Training log: 7 epoch (128 / 60000 train. data). Loss: 0.05163777992129326\n",
      "Training log: 7 epoch (1408 / 60000 train. data). Loss: 0.1659851223230362\n",
      "Training log: 7 epoch (2688 / 60000 train. data). Loss: 0.05810485780239105\n",
      "Training log: 7 epoch (3968 / 60000 train. data). Loss: 0.03165494650602341\n",
      "Training log: 7 epoch (5248 / 60000 train. data). Loss: 0.043970927596092224\n",
      "Training log: 7 epoch (6528 / 60000 train. data). Loss: 0.054487444460392\n",
      "Training log: 7 epoch (7808 / 60000 train. data). Loss: 0.05702069401741028\n",
      "Training log: 7 epoch (9088 / 60000 train. data). Loss: 0.057575687766075134\n",
      "Training log: 7 epoch (10368 / 60000 train. data). Loss: 0.06382327526807785\n",
      "Training log: 7 epoch (11648 / 60000 train. data). Loss: 0.10335615277290344\n",
      "Training log: 7 epoch (12928 / 60000 train. data). Loss: 0.06281444430351257\n",
      "Training log: 7 epoch (14208 / 60000 train. data). Loss: 0.06494105607271194\n",
      "Training log: 7 epoch (15488 / 60000 train. data). Loss: 0.08876726031303406\n",
      "Training log: 7 epoch (16768 / 60000 train. data). Loss: 0.05965171009302139\n",
      "Training log: 7 epoch (18048 / 60000 train. data). Loss: 0.08464120328426361\n",
      "Training log: 7 epoch (19328 / 60000 train. data). Loss: 0.07215483486652374\n",
      "Training log: 7 epoch (20608 / 60000 train. data). Loss: 0.05247205123305321\n",
      "Training log: 7 epoch (21888 / 60000 train. data). Loss: 0.12354366481304169\n",
      "Training log: 7 epoch (23168 / 60000 train. data). Loss: 0.08121070265769958\n",
      "Training log: 7 epoch (24448 / 60000 train. data). Loss: 0.07742290198802948\n",
      "Training log: 7 epoch (25728 / 60000 train. data). Loss: 0.03093062713742256\n",
      "Training log: 7 epoch (27008 / 60000 train. data). Loss: 0.055200111120939255\n",
      "Training log: 7 epoch (28288 / 60000 train. data). Loss: 0.08186869323253632\n",
      "Training log: 7 epoch (29568 / 60000 train. data). Loss: 0.05397458001971245\n",
      "Training log: 7 epoch (30848 / 60000 train. data). Loss: 0.025830931961536407\n",
      "Training log: 7 epoch (32128 / 60000 train. data). Loss: 0.0356922373175621\n",
      "Training log: 7 epoch (33408 / 60000 train. data). Loss: 0.026371117681264877\n",
      "Training log: 7 epoch (34688 / 60000 train. data). Loss: 0.09703463315963745\n",
      "Training log: 7 epoch (35968 / 60000 train. data). Loss: 0.06185527145862579\n",
      "Training log: 7 epoch (37248 / 60000 train. data). Loss: 0.0482487790286541\n",
      "Training log: 7 epoch (38528 / 60000 train. data). Loss: 0.05008729174733162\n",
      "Training log: 7 epoch (39808 / 60000 train. data). Loss: 0.08733516931533813\n",
      "Training log: 7 epoch (41088 / 60000 train. data). Loss: 0.1419832855463028\n",
      "Training log: 7 epoch (42368 / 60000 train. data). Loss: 0.13620077073574066\n",
      "Training log: 7 epoch (43648 / 60000 train. data). Loss: 0.04222704470157623\n",
      "Training log: 7 epoch (44928 / 60000 train. data). Loss: 0.07399357855319977\n",
      "Training log: 7 epoch (46208 / 60000 train. data). Loss: 0.0627385675907135\n",
      "Training log: 7 epoch (47488 / 60000 train. data). Loss: 0.05158392712473869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 7 epoch (48768 / 60000 train. data). Loss: 0.054858386516571045\n",
      "Training log: 7 epoch (50048 / 60000 train. data). Loss: 0.07039310038089752\n",
      "Training log: 7 epoch (51328 / 60000 train. data). Loss: 0.07780404388904572\n",
      "Training log: 7 epoch (52608 / 60000 train. data). Loss: 0.03291697055101395\n",
      "Training log: 7 epoch (53888 / 60000 train. data). Loss: 0.06875099241733551\n",
      "Training log: 7 epoch (55168 / 60000 train. data). Loss: 0.04637705162167549\n",
      "Training log: 7 epoch (56448 / 60000 train. data). Loss: 0.054294146597385406\n",
      "Training log: 7 epoch (57728 / 60000 train. data). Loss: 0.02882697805762291\n",
      "Training log: 7 epoch (59008 / 60000 train. data). Loss: 0.053605593740940094\n",
      "Test loss (avg): 0.08556960549354553, Accuracy: 0.9752\n",
      "Training log: 8 epoch (128 / 60000 train. data). Loss: 0.041996851563453674\n",
      "Training log: 8 epoch (1408 / 60000 train. data). Loss: 0.03265228495001793\n",
      "Training log: 8 epoch (2688 / 60000 train. data). Loss: 0.03874948248267174\n",
      "Training log: 8 epoch (3968 / 60000 train. data). Loss: 0.01696823723614216\n",
      "Training log: 8 epoch (5248 / 60000 train. data). Loss: 0.032029591500759125\n",
      "Training log: 8 epoch (6528 / 60000 train. data). Loss: 0.13258154690265656\n",
      "Training log: 8 epoch (7808 / 60000 train. data). Loss: 0.049797702580690384\n",
      "Training log: 8 epoch (9088 / 60000 train. data). Loss: 0.059960804879665375\n",
      "Training log: 8 epoch (10368 / 60000 train. data). Loss: 0.10419487953186035\n",
      "Training log: 8 epoch (11648 / 60000 train. data). Loss: 0.04692811146378517\n",
      "Training log: 8 epoch (12928 / 60000 train. data). Loss: 0.08243998885154724\n",
      "Training log: 8 epoch (14208 / 60000 train. data). Loss: 0.04498763754963875\n",
      "Training log: 8 epoch (15488 / 60000 train. data). Loss: 0.08609052002429962\n",
      "Training log: 8 epoch (16768 / 60000 train. data). Loss: 0.055364783853292465\n",
      "Training log: 8 epoch (18048 / 60000 train. data). Loss: 0.03981590270996094\n",
      "Training log: 8 epoch (19328 / 60000 train. data). Loss: 0.05951849743723869\n",
      "Training log: 8 epoch (20608 / 60000 train. data). Loss: 0.025553829967975616\n",
      "Training log: 8 epoch (21888 / 60000 train. data). Loss: 0.11472912132740021\n",
      "Training log: 8 epoch (23168 / 60000 train. data). Loss: 0.121675044298172\n",
      "Training log: 8 epoch (24448 / 60000 train. data). Loss: 0.03240569680929184\n",
      "Training log: 8 epoch (25728 / 60000 train. data). Loss: 0.04287238419055939\n",
      "Training log: 8 epoch (27008 / 60000 train. data). Loss: 0.08632265031337738\n",
      "Training log: 8 epoch (28288 / 60000 train. data). Loss: 0.041345447301864624\n",
      "Training log: 8 epoch (29568 / 60000 train. data). Loss: 0.024734048172831535\n",
      "Training log: 8 epoch (30848 / 60000 train. data). Loss: 0.08510120213031769\n",
      "Training log: 8 epoch (32128 / 60000 train. data). Loss: 0.05331273749470711\n",
      "Training log: 8 epoch (33408 / 60000 train. data). Loss: 0.026728643104434013\n",
      "Training log: 8 epoch (34688 / 60000 train. data). Loss: 0.038717545568943024\n",
      "Training log: 8 epoch (35968 / 60000 train. data). Loss: 0.06391017138957977\n",
      "Training log: 8 epoch (37248 / 60000 train. data). Loss: 0.08163408935070038\n",
      "Training log: 8 epoch (38528 / 60000 train. data). Loss: 0.07213345170021057\n",
      "Training log: 8 epoch (39808 / 60000 train. data). Loss: 0.05797453224658966\n",
      "Training log: 8 epoch (41088 / 60000 train. data). Loss: 0.07651704549789429\n",
      "Training log: 8 epoch (42368 / 60000 train. data). Loss: 0.024667352437973022\n",
      "Training log: 8 epoch (43648 / 60000 train. data). Loss: 0.0464823953807354\n",
      "Training log: 8 epoch (44928 / 60000 train. data). Loss: 0.027942651882767677\n",
      "Training log: 8 epoch (46208 / 60000 train. data). Loss: 0.050265420228242874\n",
      "Training log: 8 epoch (47488 / 60000 train. data). Loss: 0.06887765228748322\n",
      "Training log: 8 epoch (48768 / 60000 train. data). Loss: 0.06400568038225174\n",
      "Training log: 8 epoch (50048 / 60000 train. data). Loss: 0.040661200881004333\n",
      "Training log: 8 epoch (51328 / 60000 train. data). Loss: 0.09391853213310242\n",
      "Training log: 8 epoch (52608 / 60000 train. data). Loss: 0.08917258679866791\n",
      "Training log: 8 epoch (53888 / 60000 train. data). Loss: 0.02871355414390564\n",
      "Training log: 8 epoch (55168 / 60000 train. data). Loss: 0.046288009732961655\n",
      "Training log: 8 epoch (56448 / 60000 train. data). Loss: 0.043026041239500046\n",
      "Training log: 8 epoch (57728 / 60000 train. data). Loss: 0.0645560547709465\n",
      "Training log: 8 epoch (59008 / 60000 train. data). Loss: 0.02411462739109993\n",
      "Test loss (avg): 0.08030927042961121, Accuracy: 0.9765\n",
      "Training log: 9 epoch (128 / 60000 train. data). Loss: 0.031162414699792862\n",
      "Training log: 9 epoch (1408 / 60000 train. data). Loss: 0.01719817891716957\n",
      "Training log: 9 epoch (2688 / 60000 train. data). Loss: 0.08224973827600479\n",
      "Training log: 9 epoch (3968 / 60000 train. data). Loss: 0.055426113307476044\n",
      "Training log: 9 epoch (5248 / 60000 train. data). Loss: 0.06609809398651123\n",
      "Training log: 9 epoch (6528 / 60000 train. data). Loss: 0.06039153411984444\n",
      "Training log: 9 epoch (7808 / 60000 train. data). Loss: 0.03611258789896965\n",
      "Training log: 9 epoch (9088 / 60000 train. data). Loss: 0.039324283599853516\n",
      "Training log: 9 epoch (10368 / 60000 train. data). Loss: 0.026982475072145462\n",
      "Training log: 9 epoch (11648 / 60000 train. data). Loss: 0.09625329077243805\n",
      "Training log: 9 epoch (12928 / 60000 train. data). Loss: 0.057759176939725876\n",
      "Training log: 9 epoch (14208 / 60000 train. data). Loss: 0.066744863986969\n",
      "Training log: 9 epoch (15488 / 60000 train. data). Loss: 0.1000688299536705\n",
      "Training log: 9 epoch (16768 / 60000 train. data). Loss: 0.06334877014160156\n",
      "Training log: 9 epoch (18048 / 60000 train. data). Loss: 0.014897439628839493\n",
      "Training log: 9 epoch (19328 / 60000 train. data). Loss: 0.04081805795431137\n",
      "Training log: 9 epoch (20608 / 60000 train. data). Loss: 0.19387871026992798\n",
      "Training log: 9 epoch (21888 / 60000 train. data). Loss: 0.08011463284492493\n",
      "Training log: 9 epoch (23168 / 60000 train. data). Loss: 0.03789915144443512\n",
      "Training log: 9 epoch (24448 / 60000 train. data). Loss: 0.06263241916894913\n",
      "Training log: 9 epoch (25728 / 60000 train. data). Loss: 0.1157771497964859\n",
      "Training log: 9 epoch (27008 / 60000 train. data). Loss: 0.07109144330024719\n",
      "Training log: 9 epoch (28288 / 60000 train. data). Loss: 0.05845237523317337\n",
      "Training log: 9 epoch (29568 / 60000 train. data). Loss: 0.031925611197948456\n",
      "Training log: 9 epoch (30848 / 60000 train. data). Loss: 0.09101536124944687\n",
      "Training log: 9 epoch (32128 / 60000 train. data). Loss: 0.04135654866695404\n",
      "Training log: 9 epoch (33408 / 60000 train. data). Loss: 0.026994355022907257\n",
      "Training log: 9 epoch (34688 / 60000 train. data). Loss: 0.03213930130004883\n",
      "Training log: 9 epoch (35968 / 60000 train. data). Loss: 0.07305681705474854\n",
      "Training log: 9 epoch (37248 / 60000 train. data). Loss: 0.021975737065076828\n",
      "Training log: 9 epoch (38528 / 60000 train. data). Loss: 0.04440050944685936\n",
      "Training log: 9 epoch (39808 / 60000 train. data). Loss: 0.04101136326789856\n",
      "Training log: 9 epoch (41088 / 60000 train. data). Loss: 0.012128271162509918\n",
      "Training log: 9 epoch (42368 / 60000 train. data). Loss: 0.007204590365290642\n",
      "Training log: 9 epoch (43648 / 60000 train. data). Loss: 0.041754234582185745\n",
      "Training log: 9 epoch (44928 / 60000 train. data). Loss: 0.03350880742073059\n",
      "Training log: 9 epoch (46208 / 60000 train. data). Loss: 0.037772417068481445\n",
      "Training log: 9 epoch (47488 / 60000 train. data). Loss: 0.01961539499461651\n",
      "Training log: 9 epoch (48768 / 60000 train. data). Loss: 0.026457365602254868\n",
      "Training log: 9 epoch (50048 / 60000 train. data). Loss: 0.06816302239894867\n",
      "Training log: 9 epoch (51328 / 60000 train. data). Loss: 0.09475647658109665\n",
      "Training log: 9 epoch (52608 / 60000 train. data). Loss: 0.032451655715703964\n",
      "Training log: 9 epoch (53888 / 60000 train. data). Loss: 0.13364282250404358\n",
      "Training log: 9 epoch (55168 / 60000 train. data). Loss: 0.02809564210474491\n",
      "Training log: 9 epoch (56448 / 60000 train. data). Loss: 0.06165403500199318\n",
      "Training log: 9 epoch (57728 / 60000 train. data). Loss: 0.09875963628292084\n",
      "Training log: 9 epoch (59008 / 60000 train. data). Loss: 0.028788231313228607\n",
      "Test loss (avg): 0.0761869901418686, Accuracy: 0.9776\n",
      "Training log: 10 epoch (128 / 60000 train. data). Loss: 0.03164556995034218\n",
      "Training log: 10 epoch (1408 / 60000 train. data). Loss: 0.03750336915254593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 10 epoch (2688 / 60000 train. data). Loss: 0.02461164817214012\n",
      "Training log: 10 epoch (3968 / 60000 train. data). Loss: 0.047325409948825836\n",
      "Training log: 10 epoch (5248 / 60000 train. data). Loss: 0.029684768989682198\n",
      "Training log: 10 epoch (6528 / 60000 train. data). Loss: 0.019812919199466705\n",
      "Training log: 10 epoch (7808 / 60000 train. data). Loss: 0.09304306656122208\n",
      "Training log: 10 epoch (9088 / 60000 train. data). Loss: 0.03569022938609123\n",
      "Training log: 10 epoch (10368 / 60000 train. data). Loss: 0.06693615019321442\n",
      "Training log: 10 epoch (11648 / 60000 train. data). Loss: 0.032540999352931976\n",
      "Training log: 10 epoch (12928 / 60000 train. data). Loss: 0.046546030789613724\n",
      "Training log: 10 epoch (14208 / 60000 train. data). Loss: 0.051475320011377335\n",
      "Training log: 10 epoch (15488 / 60000 train. data). Loss: 0.03770950064063072\n",
      "Training log: 10 epoch (16768 / 60000 train. data). Loss: 0.05569068714976311\n",
      "Training log: 10 epoch (18048 / 60000 train. data). Loss: 0.058404210954904556\n",
      "Training log: 10 epoch (19328 / 60000 train. data). Loss: 0.05433501675724983\n",
      "Training log: 10 epoch (20608 / 60000 train. data). Loss: 0.039425745606422424\n",
      "Training log: 10 epoch (21888 / 60000 train. data). Loss: 0.02014690637588501\n",
      "Training log: 10 epoch (23168 / 60000 train. data). Loss: 0.02236311137676239\n",
      "Training log: 10 epoch (24448 / 60000 train. data). Loss: 0.02239004708826542\n",
      "Training log: 10 epoch (25728 / 60000 train. data). Loss: 0.04390911012887955\n",
      "Training log: 10 epoch (27008 / 60000 train. data). Loss: 0.020860327407717705\n",
      "Training log: 10 epoch (28288 / 60000 train. data). Loss: 0.05334911122918129\n",
      "Training log: 10 epoch (29568 / 60000 train. data). Loss: 0.05595342069864273\n",
      "Training log: 10 epoch (30848 / 60000 train. data). Loss: 0.06523416191339493\n",
      "Training log: 10 epoch (32128 / 60000 train. data). Loss: 0.028105322271585464\n",
      "Training log: 10 epoch (33408 / 60000 train. data). Loss: 0.054707836359739304\n",
      "Training log: 10 epoch (34688 / 60000 train. data). Loss: 0.03850043565034866\n",
      "Training log: 10 epoch (35968 / 60000 train. data). Loss: 0.01377321220934391\n",
      "Training log: 10 epoch (37248 / 60000 train. data). Loss: 0.009772867895662785\n",
      "Training log: 10 epoch (38528 / 60000 train. data). Loss: 0.017021173611283302\n",
      "Training log: 10 epoch (39808 / 60000 train. data). Loss: 0.010722838342189789\n",
      "Training log: 10 epoch (41088 / 60000 train. data). Loss: 0.016696659848093987\n",
      "Training log: 10 epoch (42368 / 60000 train. data). Loss: 0.017088767141103745\n",
      "Training log: 10 epoch (43648 / 60000 train. data). Loss: 0.05837683752179146\n",
      "Training log: 10 epoch (44928 / 60000 train. data). Loss: 0.026474379003047943\n",
      "Training log: 10 epoch (46208 / 60000 train. data). Loss: 0.023949207738041878\n",
      "Training log: 10 epoch (47488 / 60000 train. data). Loss: 0.0469345860183239\n",
      "Training log: 10 epoch (48768 / 60000 train. data). Loss: 0.022269809618592262\n",
      "Training log: 10 epoch (50048 / 60000 train. data). Loss: 0.023166755214333534\n",
      "Training log: 10 epoch (51328 / 60000 train. data). Loss: 0.02746305614709854\n",
      "Training log: 10 epoch (52608 / 60000 train. data). Loss: 0.048295773565769196\n",
      "Training log: 10 epoch (53888 / 60000 train. data). Loss: 0.0765577107667923\n",
      "Training log: 10 epoch (55168 / 60000 train. data). Loss: 0.09289278090000153\n",
      "Training log: 10 epoch (56448 / 60000 train. data). Loss: 0.03796955570578575\n",
      "Training log: 10 epoch (57728 / 60000 train. data). Loss: 0.05129196494817734\n",
      "Training log: 10 epoch (59008 / 60000 train. data). Loss: 0.009136082604527473\n",
      "Test loss (avg): 0.07248760697841644, Accuracy: 0.9771\n",
      "Training log: 11 epoch (128 / 60000 train. data). Loss: 0.01384763978421688\n",
      "Training log: 11 epoch (1408 / 60000 train. data). Loss: 0.012462373822927475\n",
      "Training log: 11 epoch (2688 / 60000 train. data). Loss: 0.043329861015081406\n",
      "Training log: 11 epoch (3968 / 60000 train. data). Loss: 0.014011824503540993\n",
      "Training log: 11 epoch (5248 / 60000 train. data). Loss: 0.03471031039953232\n",
      "Training log: 11 epoch (6528 / 60000 train. data). Loss: 0.028386320918798447\n",
      "Training log: 11 epoch (7808 / 60000 train. data). Loss: 0.048621151596307755\n",
      "Training log: 11 epoch (9088 / 60000 train. data). Loss: 0.02340838499367237\n",
      "Training log: 11 epoch (10368 / 60000 train. data). Loss: 0.0054273102432489395\n",
      "Training log: 11 epoch (11648 / 60000 train. data). Loss: 0.03471488878130913\n",
      "Training log: 11 epoch (12928 / 60000 train. data). Loss: 0.02799302339553833\n",
      "Training log: 11 epoch (14208 / 60000 train. data). Loss: 0.09351849555969238\n",
      "Training log: 11 epoch (15488 / 60000 train. data). Loss: 0.04779405891895294\n",
      "Training log: 11 epoch (16768 / 60000 train. data). Loss: 0.038235556334257126\n",
      "Training log: 11 epoch (18048 / 60000 train. data). Loss: 0.06829731166362762\n",
      "Training log: 11 epoch (19328 / 60000 train. data). Loss: 0.031940948218107224\n",
      "Training log: 11 epoch (20608 / 60000 train. data). Loss: 0.017930172383785248\n",
      "Training log: 11 epoch (21888 / 60000 train. data). Loss: 0.018554741516709328\n",
      "Training log: 11 epoch (23168 / 60000 train. data). Loss: 0.03474567458033562\n",
      "Training log: 11 epoch (24448 / 60000 train. data). Loss: 0.02606154792010784\n",
      "Training log: 11 epoch (25728 / 60000 train. data). Loss: 0.041283681988716125\n",
      "Training log: 11 epoch (27008 / 60000 train. data). Loss: 0.023200618103146553\n",
      "Training log: 11 epoch (28288 / 60000 train. data). Loss: 0.025123169645667076\n",
      "Training log: 11 epoch (29568 / 60000 train. data). Loss: 0.020730970427393913\n",
      "Training log: 11 epoch (30848 / 60000 train. data). Loss: 0.03279190510511398\n",
      "Training log: 11 epoch (32128 / 60000 train. data). Loss: 0.013373389840126038\n",
      "Training log: 11 epoch (33408 / 60000 train. data). Loss: 0.014226902276277542\n",
      "Training log: 11 epoch (34688 / 60000 train. data). Loss: 0.007112974300980568\n",
      "Training log: 11 epoch (35968 / 60000 train. data). Loss: 0.006521470844745636\n",
      "Training log: 11 epoch (37248 / 60000 train. data). Loss: 0.023417606949806213\n",
      "Training log: 11 epoch (38528 / 60000 train. data). Loss: 0.04840875044465065\n",
      "Training log: 11 epoch (39808 / 60000 train. data). Loss: 0.045507896691560745\n",
      "Training log: 11 epoch (41088 / 60000 train. data). Loss: 0.02080950140953064\n",
      "Training log: 11 epoch (42368 / 60000 train. data). Loss: 0.02241433411836624\n",
      "Training log: 11 epoch (43648 / 60000 train. data). Loss: 0.04596931114792824\n",
      "Training log: 11 epoch (44928 / 60000 train. data). Loss: 0.01656322181224823\n",
      "Training log: 11 epoch (46208 / 60000 train. data). Loss: 0.09388233721256256\n",
      "Training log: 11 epoch (47488 / 60000 train. data). Loss: 0.024229755625128746\n",
      "Training log: 11 epoch (48768 / 60000 train. data). Loss: 0.03204118460416794\n",
      "Training log: 11 epoch (50048 / 60000 train. data). Loss: 0.04681241139769554\n",
      "Training log: 11 epoch (51328 / 60000 train. data). Loss: 0.007134381681680679\n",
      "Training log: 11 epoch (52608 / 60000 train. data). Loss: 0.031752049922943115\n",
      "Training log: 11 epoch (53888 / 60000 train. data). Loss: 0.012149987742304802\n",
      "Training log: 11 epoch (55168 / 60000 train. data). Loss: 0.013746671378612518\n",
      "Training log: 11 epoch (56448 / 60000 train. data). Loss: 0.013176843523979187\n",
      "Training log: 11 epoch (57728 / 60000 train. data). Loss: 0.03217952698469162\n",
      "Training log: 11 epoch (59008 / 60000 train. data). Loss: 0.047086913138628006\n",
      "Test loss (avg): 0.06956369469165802, Accuracy: 0.9782\n",
      "Training log: 12 epoch (128 / 60000 train. data). Loss: 0.0479113906621933\n",
      "Training log: 12 epoch (1408 / 60000 train. data). Loss: 0.032045334577560425\n",
      "Training log: 12 epoch (2688 / 60000 train. data). Loss: 0.025569159537553787\n",
      "Training log: 12 epoch (3968 / 60000 train. data). Loss: 0.016771802678704262\n",
      "Training log: 12 epoch (5248 / 60000 train. data). Loss: 0.03240697830915451\n",
      "Training log: 12 epoch (6528 / 60000 train. data). Loss: 0.024601638317108154\n",
      "Training log: 12 epoch (7808 / 60000 train. data). Loss: 0.026362905278801918\n",
      "Training log: 12 epoch (9088 / 60000 train. data). Loss: 0.04767625406384468\n",
      "Training log: 12 epoch (10368 / 60000 train. data). Loss: 0.029012510553002357\n",
      "Training log: 12 epoch (11648 / 60000 train. data). Loss: 0.0812201127409935\n",
      "Training log: 12 epoch (12928 / 60000 train. data). Loss: 0.009737901389598846\n",
      "Training log: 12 epoch (14208 / 60000 train. data). Loss: 0.030726708471775055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 12 epoch (15488 / 60000 train. data). Loss: 0.051251888275146484\n",
      "Training log: 12 epoch (16768 / 60000 train. data). Loss: 0.04138277843594551\n",
      "Training log: 12 epoch (18048 / 60000 train. data). Loss: 0.009924862533807755\n",
      "Training log: 12 epoch (19328 / 60000 train. data). Loss: 0.02506682276725769\n",
      "Training log: 12 epoch (20608 / 60000 train. data). Loss: 0.018612518906593323\n",
      "Training log: 12 epoch (21888 / 60000 train. data). Loss: 0.013508802279829979\n",
      "Training log: 12 epoch (23168 / 60000 train. data). Loss: 0.02085224539041519\n",
      "Training log: 12 epoch (24448 / 60000 train. data). Loss: 0.03904496878385544\n",
      "Training log: 12 epoch (25728 / 60000 train. data). Loss: 0.025361565873026848\n",
      "Training log: 12 epoch (27008 / 60000 train. data). Loss: 0.012164702638983727\n",
      "Training log: 12 epoch (28288 / 60000 train. data). Loss: 0.015780463814735413\n",
      "Training log: 12 epoch (29568 / 60000 train. data). Loss: 0.04239964112639427\n",
      "Training log: 12 epoch (30848 / 60000 train. data). Loss: 0.07301979511976242\n",
      "Training log: 12 epoch (32128 / 60000 train. data). Loss: 0.023098304867744446\n",
      "Training log: 12 epoch (33408 / 60000 train. data). Loss: 0.021094854921102524\n",
      "Training log: 12 epoch (34688 / 60000 train. data). Loss: 0.012024546042084694\n",
      "Training log: 12 epoch (35968 / 60000 train. data). Loss: 0.026798343285918236\n",
      "Training log: 12 epoch (37248 / 60000 train. data). Loss: 0.013333382084965706\n",
      "Training log: 12 epoch (38528 / 60000 train. data). Loss: 0.03593754395842552\n",
      "Training log: 12 epoch (39808 / 60000 train. data). Loss: 0.015547491610050201\n",
      "Training log: 12 epoch (41088 / 60000 train. data). Loss: 0.01586243510246277\n",
      "Training log: 12 epoch (42368 / 60000 train. data). Loss: 0.0158520694822073\n",
      "Training log: 12 epoch (43648 / 60000 train. data). Loss: 0.019327569752931595\n",
      "Training log: 12 epoch (44928 / 60000 train. data). Loss: 0.02548004873096943\n",
      "Training log: 12 epoch (46208 / 60000 train. data). Loss: 0.024050593376159668\n",
      "Training log: 12 epoch (47488 / 60000 train. data). Loss: 0.02242453396320343\n",
      "Training log: 12 epoch (48768 / 60000 train. data). Loss: 0.051062025129795074\n",
      "Training log: 12 epoch (50048 / 60000 train. data). Loss: 0.031238984316587448\n",
      "Training log: 12 epoch (51328 / 60000 train. data). Loss: 0.01312785129994154\n",
      "Training log: 12 epoch (52608 / 60000 train. data). Loss: 0.02749643474817276\n",
      "Training log: 12 epoch (53888 / 60000 train. data). Loss: 0.03457758575677872\n",
      "Training log: 12 epoch (55168 / 60000 train. data). Loss: 0.033476073294878006\n",
      "Training log: 12 epoch (56448 / 60000 train. data). Loss: 0.015647685155272484\n",
      "Training log: 12 epoch (57728 / 60000 train. data). Loss: 0.018001724034547806\n",
      "Training log: 12 epoch (59008 / 60000 train. data). Loss: 0.024463461712002754\n",
      "Test loss (avg): 0.06797629067897797, Accuracy: 0.9792\n",
      "Training log: 13 epoch (128 / 60000 train. data). Loss: 0.0842052549123764\n",
      "Training log: 13 epoch (1408 / 60000 train. data). Loss: 0.013855446130037308\n",
      "Training log: 13 epoch (2688 / 60000 train. data). Loss: 0.01614152267575264\n",
      "Training log: 13 epoch (3968 / 60000 train. data). Loss: 0.0077559370547533035\n",
      "Training log: 13 epoch (5248 / 60000 train. data). Loss: 0.038222841918468475\n",
      "Training log: 13 epoch (6528 / 60000 train. data). Loss: 0.02248133160173893\n",
      "Training log: 13 epoch (7808 / 60000 train. data). Loss: 0.010474132373929024\n",
      "Training log: 13 epoch (9088 / 60000 train. data). Loss: 0.009491415694355965\n",
      "Training log: 13 epoch (10368 / 60000 train. data). Loss: 0.01117120124399662\n",
      "Training log: 13 epoch (11648 / 60000 train. data). Loss: 0.009312894195318222\n",
      "Training log: 13 epoch (12928 / 60000 train. data). Loss: 0.022257762029767036\n",
      "Training log: 13 epoch (14208 / 60000 train. data). Loss: 0.028958061710000038\n",
      "Training log: 13 epoch (15488 / 60000 train. data). Loss: 0.045514240860939026\n",
      "Training log: 13 epoch (16768 / 60000 train. data). Loss: 0.012553391978144646\n",
      "Training log: 13 epoch (18048 / 60000 train. data). Loss: 0.0152074433863163\n",
      "Training log: 13 epoch (19328 / 60000 train. data). Loss: 0.006907988339662552\n",
      "Training log: 13 epoch (20608 / 60000 train. data). Loss: 0.019173255190253258\n",
      "Training log: 13 epoch (21888 / 60000 train. data). Loss: 0.010789260268211365\n",
      "Training log: 13 epoch (23168 / 60000 train. data). Loss: 0.021619422361254692\n",
      "Training log: 13 epoch (24448 / 60000 train. data). Loss: 0.020766057074069977\n",
      "Training log: 13 epoch (25728 / 60000 train. data). Loss: 0.019749153405427933\n",
      "Training log: 13 epoch (27008 / 60000 train. data). Loss: 0.008170617744326591\n",
      "Training log: 13 epoch (28288 / 60000 train. data). Loss: 0.021183405071496964\n",
      "Training log: 13 epoch (29568 / 60000 train. data). Loss: 0.03427429124712944\n",
      "Training log: 13 epoch (30848 / 60000 train. data). Loss: 0.01540137268602848\n",
      "Training log: 13 epoch (32128 / 60000 train. data). Loss: 0.021775009110569954\n",
      "Training log: 13 epoch (33408 / 60000 train. data). Loss: 0.012122157961130142\n",
      "Training log: 13 epoch (34688 / 60000 train. data). Loss: 0.016310594975948334\n",
      "Training log: 13 epoch (35968 / 60000 train. data). Loss: 0.012692119926214218\n",
      "Training log: 13 epoch (37248 / 60000 train. data). Loss: 0.08905775845050812\n",
      "Training log: 13 epoch (38528 / 60000 train. data). Loss: 0.02264425717294216\n",
      "Training log: 13 epoch (39808 / 60000 train. data). Loss: 0.0218434426933527\n",
      "Training log: 13 epoch (41088 / 60000 train. data). Loss: 0.012244458310306072\n",
      "Training log: 13 epoch (42368 / 60000 train. data). Loss: 0.010423306375741959\n",
      "Training log: 13 epoch (43648 / 60000 train. data). Loss: 0.013241231441497803\n",
      "Training log: 13 epoch (44928 / 60000 train. data). Loss: 0.05258918181061745\n",
      "Training log: 13 epoch (46208 / 60000 train. data). Loss: 0.015046568587422371\n",
      "Training log: 13 epoch (47488 / 60000 train. data). Loss: 0.01278779935091734\n",
      "Training log: 13 epoch (48768 / 60000 train. data). Loss: 0.012734293937683105\n",
      "Training log: 13 epoch (50048 / 60000 train. data). Loss: 0.014401251450181007\n",
      "Training log: 13 epoch (51328 / 60000 train. data). Loss: 0.13250121474266052\n",
      "Training log: 13 epoch (52608 / 60000 train. data). Loss: 0.00964423082768917\n",
      "Training log: 13 epoch (53888 / 60000 train. data). Loss: 0.014069229364395142\n",
      "Training log: 13 epoch (55168 / 60000 train. data). Loss: 0.012690488249063492\n",
      "Training log: 13 epoch (56448 / 60000 train. data). Loss: 0.017949318513274193\n",
      "Training log: 13 epoch (57728 / 60000 train. data). Loss: 0.011341925710439682\n",
      "Training log: 13 epoch (59008 / 60000 train. data). Loss: 0.009743478149175644\n",
      "Test loss (avg): 0.06603885550498963, Accuracy: 0.9796\n",
      "Training log: 14 epoch (128 / 60000 train. data). Loss: 0.056574538350105286\n",
      "Training log: 14 epoch (1408 / 60000 train. data). Loss: 0.006884949281811714\n",
      "Training log: 14 epoch (2688 / 60000 train. data). Loss: 0.08198024332523346\n",
      "Training log: 14 epoch (3968 / 60000 train. data). Loss: 0.007010295987129211\n",
      "Training log: 14 epoch (5248 / 60000 train. data). Loss: 0.030028769746422768\n",
      "Training log: 14 epoch (6528 / 60000 train. data). Loss: 0.01894943043589592\n",
      "Training log: 14 epoch (7808 / 60000 train. data). Loss: 0.006696013733744621\n",
      "Training log: 14 epoch (9088 / 60000 train. data). Loss: 0.04244333505630493\n",
      "Training log: 14 epoch (10368 / 60000 train. data). Loss: 0.015178507193922997\n",
      "Training log: 14 epoch (11648 / 60000 train. data). Loss: 0.017102215439081192\n",
      "Training log: 14 epoch (12928 / 60000 train. data). Loss: 0.006459595635533333\n",
      "Training log: 14 epoch (14208 / 60000 train. data). Loss: 0.017676129937171936\n",
      "Training log: 14 epoch (15488 / 60000 train. data). Loss: 0.016962820664048195\n",
      "Training log: 14 epoch (16768 / 60000 train. data). Loss: 0.009857622906565666\n",
      "Training log: 14 epoch (18048 / 60000 train. data). Loss: 0.015563441440463066\n",
      "Training log: 14 epoch (19328 / 60000 train. data). Loss: 0.01694486476480961\n",
      "Training log: 14 epoch (20608 / 60000 train. data). Loss: 0.0025054439902305603\n",
      "Training log: 14 epoch (21888 / 60000 train. data). Loss: 0.010680397972464561\n",
      "Training log: 14 epoch (23168 / 60000 train. data). Loss: 0.007255151867866516\n",
      "Training log: 14 epoch (24448 / 60000 train. data). Loss: 0.010266575962305069\n",
      "Training log: 14 epoch (25728 / 60000 train. data). Loss: 0.014740077778697014\n",
      "Training log: 14 epoch (27008 / 60000 train. data). Loss: 0.026715360581874847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 14 epoch (28288 / 60000 train. data). Loss: 0.01921653188765049\n",
      "Training log: 14 epoch (29568 / 60000 train. data). Loss: 0.0038495007902383804\n",
      "Training log: 14 epoch (30848 / 60000 train. data). Loss: 0.04519383981823921\n",
      "Training log: 14 epoch (32128 / 60000 train. data). Loss: 0.01795186474919319\n",
      "Training log: 14 epoch (33408 / 60000 train. data). Loss: 0.035499684512615204\n",
      "Training log: 14 epoch (34688 / 60000 train. data). Loss: 0.01165428850799799\n",
      "Training log: 14 epoch (35968 / 60000 train. data). Loss: 0.013560046441853046\n",
      "Training log: 14 epoch (37248 / 60000 train. data). Loss: 0.045105576515197754\n",
      "Training log: 14 epoch (38528 / 60000 train. data). Loss: 0.0036229602992534637\n",
      "Training log: 14 epoch (39808 / 60000 train. data). Loss: 0.007982602342963219\n",
      "Training log: 14 epoch (41088 / 60000 train. data). Loss: 0.009420149028301239\n",
      "Training log: 14 epoch (42368 / 60000 train. data). Loss: 0.027714939787983894\n",
      "Training log: 14 epoch (43648 / 60000 train. data). Loss: 0.03800298646092415\n",
      "Training log: 14 epoch (44928 / 60000 train. data). Loss: 0.025714842602610588\n",
      "Training log: 14 epoch (46208 / 60000 train. data). Loss: 0.01994968391954899\n",
      "Training log: 14 epoch (47488 / 60000 train. data). Loss: 0.014788949862122536\n",
      "Training log: 14 epoch (48768 / 60000 train. data). Loss: 0.08163733780384064\n",
      "Training log: 14 epoch (50048 / 60000 train. data). Loss: 0.014795380644500256\n",
      "Training log: 14 epoch (51328 / 60000 train. data). Loss: 0.01754528470337391\n",
      "Training log: 14 epoch (52608 / 60000 train. data). Loss: 0.030184010043740273\n",
      "Training log: 14 epoch (53888 / 60000 train. data). Loss: 0.012632284313440323\n",
      "Training log: 14 epoch (55168 / 60000 train. data). Loss: 0.030667804181575775\n",
      "Training log: 14 epoch (56448 / 60000 train. data). Loss: 0.017843693494796753\n",
      "Training log: 14 epoch (57728 / 60000 train. data). Loss: 0.058194808661937714\n",
      "Training log: 14 epoch (59008 / 60000 train. data). Loss: 0.018346454948186874\n",
      "Test loss (avg): 0.06565645204782486, Accuracy: 0.9801\n",
      "Training log: 15 epoch (128 / 60000 train. data). Loss: 0.009040610864758492\n",
      "Training log: 15 epoch (1408 / 60000 train. data). Loss: 0.009421007707715034\n",
      "Training log: 15 epoch (2688 / 60000 train. data). Loss: 0.011245789006352425\n",
      "Training log: 15 epoch (3968 / 60000 train. data). Loss: 0.00515466183423996\n",
      "Training log: 15 epoch (5248 / 60000 train. data). Loss: 0.01586008444428444\n",
      "Training log: 15 epoch (6528 / 60000 train. data). Loss: 0.014353005215525627\n",
      "Training log: 15 epoch (7808 / 60000 train. data). Loss: 0.011366132646799088\n",
      "Training log: 15 epoch (9088 / 60000 train. data). Loss: 0.018697433173656464\n",
      "Training log: 15 epoch (10368 / 60000 train. data). Loss: 0.012233436107635498\n",
      "Training log: 15 epoch (11648 / 60000 train. data). Loss: 0.003823375329375267\n",
      "Training log: 15 epoch (12928 / 60000 train. data). Loss: 0.019471511244773865\n",
      "Training log: 15 epoch (14208 / 60000 train. data). Loss: 0.00869886577129364\n",
      "Training log: 15 epoch (15488 / 60000 train. data). Loss: 0.013871343806385994\n",
      "Training log: 15 epoch (16768 / 60000 train. data). Loss: 0.015185976400971413\n",
      "Training log: 15 epoch (18048 / 60000 train. data). Loss: 0.020816175267100334\n",
      "Training log: 15 epoch (19328 / 60000 train. data). Loss: 0.022262679412961006\n",
      "Training log: 15 epoch (20608 / 60000 train. data). Loss: 0.005666464567184448\n",
      "Training log: 15 epoch (21888 / 60000 train. data). Loss: 0.0048883333802223206\n",
      "Training log: 15 epoch (23168 / 60000 train. data). Loss: 0.015887528657913208\n",
      "Training log: 15 epoch (24448 / 60000 train. data). Loss: 0.028970524668693542\n",
      "Training log: 15 epoch (25728 / 60000 train. data). Loss: 0.020701294764876366\n",
      "Training log: 15 epoch (27008 / 60000 train. data). Loss: 0.028818171471357346\n",
      "Training log: 15 epoch (28288 / 60000 train. data). Loss: 0.011988813057541847\n",
      "Training log: 15 epoch (29568 / 60000 train. data). Loss: 0.011377613991498947\n",
      "Training log: 15 epoch (30848 / 60000 train. data). Loss: 0.020880581811070442\n",
      "Training log: 15 epoch (32128 / 60000 train. data). Loss: 0.029067935422062874\n",
      "Training log: 15 epoch (33408 / 60000 train. data). Loss: 0.0100417360663414\n",
      "Training log: 15 epoch (34688 / 60000 train. data). Loss: 0.004948347806930542\n",
      "Training log: 15 epoch (35968 / 60000 train. data). Loss: 0.007299572229385376\n",
      "Training log: 15 epoch (37248 / 60000 train. data). Loss: 0.005255604162812233\n",
      "Training log: 15 epoch (38528 / 60000 train. data). Loss: 0.0065420400351285934\n",
      "Training log: 15 epoch (39808 / 60000 train. data). Loss: 0.024318663403391838\n",
      "Training log: 15 epoch (41088 / 60000 train. data). Loss: 0.017114540562033653\n",
      "Training log: 15 epoch (42368 / 60000 train. data). Loss: 0.012929297983646393\n",
      "Training log: 15 epoch (43648 / 60000 train. data). Loss: 0.006802944466471672\n",
      "Training log: 15 epoch (44928 / 60000 train. data). Loss: 0.013768615201115608\n",
      "Training log: 15 epoch (46208 / 60000 train. data). Loss: 0.010132852010428905\n",
      "Training log: 15 epoch (47488 / 60000 train. data). Loss: 0.005640942603349686\n",
      "Training log: 15 epoch (48768 / 60000 train. data). Loss: 0.006770078092813492\n",
      "Training log: 15 epoch (50048 / 60000 train. data). Loss: 0.03079160302877426\n",
      "Training log: 15 epoch (51328 / 60000 train. data). Loss: 0.009572669863700867\n",
      "Training log: 15 epoch (52608 / 60000 train. data). Loss: 0.004833545535802841\n",
      "Training log: 15 epoch (53888 / 60000 train. data). Loss: 0.015235785394906998\n",
      "Training log: 15 epoch (55168 / 60000 train. data). Loss: 0.017769327387213707\n",
      "Training log: 15 epoch (56448 / 60000 train. data). Loss: 0.015747494995594025\n",
      "Training log: 15 epoch (57728 / 60000 train. data). Loss: 0.0039056427776813507\n",
      "Training log: 15 epoch (59008 / 60000 train. data). Loss: 0.0034856386482715607\n",
      "Test loss (avg): 0.06435730403661728, Accuracy: 0.9813\n",
      "Training log: 16 epoch (128 / 60000 train. data). Loss: 0.01702731102705002\n",
      "Training log: 16 epoch (1408 / 60000 train. data). Loss: 0.009659630246460438\n",
      "Training log: 16 epoch (2688 / 60000 train. data). Loss: 0.007183557376265526\n",
      "Training log: 16 epoch (3968 / 60000 train. data). Loss: 0.018480943515896797\n",
      "Training log: 16 epoch (5248 / 60000 train. data). Loss: 0.0173795185983181\n",
      "Training log: 16 epoch (6528 / 60000 train. data). Loss: 0.008140871301293373\n",
      "Training log: 16 epoch (7808 / 60000 train. data). Loss: 0.0035649947822093964\n",
      "Training log: 16 epoch (9088 / 60000 train. data). Loss: 0.004634438082575798\n",
      "Training log: 16 epoch (10368 / 60000 train. data). Loss: 0.02126534841954708\n",
      "Training log: 16 epoch (11648 / 60000 train. data). Loss: 0.03237563371658325\n",
      "Training log: 16 epoch (12928 / 60000 train. data). Loss: 0.006227407604455948\n",
      "Training log: 16 epoch (14208 / 60000 train. data). Loss: 0.00424216128885746\n",
      "Training log: 16 epoch (15488 / 60000 train. data). Loss: 0.004607809707522392\n",
      "Training log: 16 epoch (16768 / 60000 train. data). Loss: 0.004169704392552376\n",
      "Training log: 16 epoch (18048 / 60000 train. data). Loss: 0.005024749785661697\n",
      "Training log: 16 epoch (19328 / 60000 train. data). Loss: 0.0036056824028491974\n",
      "Training log: 16 epoch (20608 / 60000 train. data). Loss: 0.005478786304593086\n",
      "Training log: 16 epoch (21888 / 60000 train. data). Loss: 0.0035397708415985107\n",
      "Training log: 16 epoch (23168 / 60000 train. data). Loss: 0.016156265512108803\n",
      "Training log: 16 epoch (24448 / 60000 train. data). Loss: 0.02240397036075592\n",
      "Training log: 16 epoch (25728 / 60000 train. data). Loss: 0.00498572364449501\n",
      "Training log: 16 epoch (27008 / 60000 train. data). Loss: 0.007183984853327274\n",
      "Training log: 16 epoch (28288 / 60000 train. data). Loss: 0.003416299819946289\n",
      "Training log: 16 epoch (29568 / 60000 train. data). Loss: 0.012315399944782257\n",
      "Training log: 16 epoch (30848 / 60000 train. data). Loss: 0.005966927856206894\n",
      "Training log: 16 epoch (32128 / 60000 train. data). Loss: 0.01875760778784752\n",
      "Training log: 16 epoch (33408 / 60000 train. data). Loss: 0.005741041619330645\n",
      "Training log: 16 epoch (34688 / 60000 train. data). Loss: 0.006553048267960548\n",
      "Training log: 16 epoch (35968 / 60000 train. data). Loss: 0.009211571887135506\n",
      "Training log: 16 epoch (37248 / 60000 train. data). Loss: 0.035028379410505295\n",
      "Training log: 16 epoch (38528 / 60000 train. data). Loss: 0.012307528406381607\n",
      "Training log: 16 epoch (39808 / 60000 train. data). Loss: 0.006545135751366615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 16 epoch (41088 / 60000 train. data). Loss: 0.009777403436601162\n",
      "Training log: 16 epoch (42368 / 60000 train. data). Loss: 0.003573128953576088\n",
      "Training log: 16 epoch (43648 / 60000 train. data). Loss: 0.007709193974733353\n",
      "Training log: 16 epoch (44928 / 60000 train. data). Loss: 0.0042450278997421265\n",
      "Training log: 16 epoch (46208 / 60000 train. data). Loss: 0.008267104625701904\n",
      "Training log: 16 epoch (47488 / 60000 train. data). Loss: 0.023743154481053352\n",
      "Training log: 16 epoch (48768 / 60000 train. data). Loss: 0.00794592872262001\n",
      "Training log: 16 epoch (50048 / 60000 train. data). Loss: 0.005987100303173065\n",
      "Training log: 16 epoch (51328 / 60000 train. data). Loss: 0.006606485694646835\n",
      "Training log: 16 epoch (52608 / 60000 train. data). Loss: 0.004354953765869141\n",
      "Training log: 16 epoch (53888 / 60000 train. data). Loss: 0.04312510043382645\n",
      "Training log: 16 epoch (55168 / 60000 train. data). Loss: 0.010863606818020344\n",
      "Training log: 16 epoch (56448 / 60000 train. data). Loss: 0.016386136412620544\n",
      "Training log: 16 epoch (57728 / 60000 train. data). Loss: 0.018370386213064194\n",
      "Training log: 16 epoch (59008 / 60000 train. data). Loss: 0.018091220408678055\n",
      "Test loss (avg): 0.06444778653383255, Accuracy: 0.9811\n",
      "Training log: 17 epoch (128 / 60000 train. data). Loss: 0.004552862606942654\n",
      "Training log: 17 epoch (1408 / 60000 train. data). Loss: 0.015814075246453285\n",
      "Training log: 17 epoch (2688 / 60000 train. data). Loss: 0.006146116182208061\n",
      "Training log: 17 epoch (3968 / 60000 train. data). Loss: 0.007058275863528252\n",
      "Training log: 17 epoch (5248 / 60000 train. data). Loss: 0.010730020701885223\n",
      "Training log: 17 epoch (6528 / 60000 train. data). Loss: 0.0073818545788526535\n",
      "Training log: 17 epoch (7808 / 60000 train. data). Loss: 0.00972701609134674\n",
      "Training log: 17 epoch (9088 / 60000 train. data). Loss: 0.006486834958195686\n",
      "Training log: 17 epoch (10368 / 60000 train. data). Loss: 0.01154420804232359\n",
      "Training log: 17 epoch (11648 / 60000 train. data). Loss: 0.024538541212677956\n",
      "Training log: 17 epoch (12928 / 60000 train. data). Loss: 0.012309456244111061\n",
      "Training log: 17 epoch (14208 / 60000 train. data). Loss: 0.003069346770644188\n",
      "Training log: 17 epoch (15488 / 60000 train. data). Loss: 0.008071007207036018\n",
      "Training log: 17 epoch (16768 / 60000 train. data). Loss: 0.0015907324850559235\n",
      "Training log: 17 epoch (18048 / 60000 train. data). Loss: 0.035038482397794724\n",
      "Training log: 17 epoch (19328 / 60000 train. data). Loss: 0.009776826947927475\n",
      "Training log: 17 epoch (20608 / 60000 train. data). Loss: 0.010046692565083504\n",
      "Training log: 17 epoch (21888 / 60000 train. data). Loss: 0.008872749283909798\n",
      "Training log: 17 epoch (23168 / 60000 train. data). Loss: 0.0006117150187492371\n",
      "Training log: 17 epoch (24448 / 60000 train. data). Loss: 0.00763804093003273\n",
      "Training log: 17 epoch (25728 / 60000 train. data). Loss: 0.004599286243319511\n",
      "Training log: 17 epoch (27008 / 60000 train. data). Loss: 0.004979776218533516\n",
      "Training log: 17 epoch (28288 / 60000 train. data). Loss: 0.005935234948992729\n",
      "Training log: 17 epoch (29568 / 60000 train. data). Loss: 0.0035015996545553207\n",
      "Training log: 17 epoch (30848 / 60000 train. data). Loss: 0.013630520552396774\n",
      "Training log: 17 epoch (32128 / 60000 train. data). Loss: 0.036679308861494064\n",
      "Training log: 17 epoch (33408 / 60000 train. data). Loss: 0.0035586394369602203\n",
      "Training log: 17 epoch (34688 / 60000 train. data). Loss: 0.004629746079444885\n",
      "Training log: 17 epoch (35968 / 60000 train. data). Loss: 0.010987920686602592\n",
      "Training log: 17 epoch (37248 / 60000 train. data). Loss: 0.0040513817220926285\n",
      "Training log: 17 epoch (38528 / 60000 train. data). Loss: 0.007520422339439392\n",
      "Training log: 17 epoch (39808 / 60000 train. data). Loss: 0.006710316985845566\n",
      "Training log: 17 epoch (41088 / 60000 train. data). Loss: 0.002477100118994713\n",
      "Training log: 17 epoch (42368 / 60000 train. data). Loss: 0.0049591995775699615\n",
      "Training log: 17 epoch (43648 / 60000 train. data). Loss: 0.01144564338028431\n",
      "Training log: 17 epoch (44928 / 60000 train. data). Loss: 0.007335031405091286\n",
      "Training log: 17 epoch (46208 / 60000 train. data). Loss: 0.012518495321273804\n",
      "Training log: 17 epoch (47488 / 60000 train. data). Loss: 0.002361726015806198\n",
      "Training log: 17 epoch (48768 / 60000 train. data). Loss: 0.016610044986009598\n",
      "Training log: 17 epoch (50048 / 60000 train. data). Loss: 0.013496935367584229\n",
      "Training log: 17 epoch (51328 / 60000 train. data). Loss: 0.0028024762868881226\n",
      "Training log: 17 epoch (52608 / 60000 train. data). Loss: 0.00499693863093853\n",
      "Training log: 17 epoch (53888 / 60000 train. data). Loss: 0.005426013842225075\n",
      "Training log: 17 epoch (55168 / 60000 train. data). Loss: 0.0217453520745039\n",
      "Training log: 17 epoch (56448 / 60000 train. data). Loss: 0.029462702572345734\n",
      "Training log: 17 epoch (57728 / 60000 train. data). Loss: 0.006920786574482918\n",
      "Training log: 17 epoch (59008 / 60000 train. data). Loss: 0.00856417790055275\n",
      "Test loss (avg): 0.0646373416185379, Accuracy: 0.9828\n",
      "Training log: 18 epoch (128 / 60000 train. data). Loss: 0.006969689391553402\n",
      "Training log: 18 epoch (1408 / 60000 train. data). Loss: 0.002339757978916168\n",
      "Training log: 18 epoch (2688 / 60000 train. data). Loss: 0.006694543641060591\n",
      "Training log: 18 epoch (3968 / 60000 train. data). Loss: 0.002456851303577423\n",
      "Training log: 18 epoch (5248 / 60000 train. data). Loss: 0.009990254417061806\n",
      "Training log: 18 epoch (6528 / 60000 train. data). Loss: 0.005117060616612434\n",
      "Training log: 18 epoch (7808 / 60000 train. data). Loss: 0.0055415816605091095\n",
      "Training log: 18 epoch (9088 / 60000 train. data). Loss: 0.006953299045562744\n",
      "Training log: 18 epoch (10368 / 60000 train. data). Loss: 0.0072550661861896515\n",
      "Training log: 18 epoch (11648 / 60000 train. data). Loss: 0.0040704645216465\n",
      "Training log: 18 epoch (12928 / 60000 train. data). Loss: 0.003954235464334488\n",
      "Training log: 18 epoch (14208 / 60000 train. data). Loss: 0.004569033160805702\n",
      "Training log: 18 epoch (15488 / 60000 train. data). Loss: 0.0016761235892772675\n",
      "Training log: 18 epoch (16768 / 60000 train. data). Loss: 0.016609441488981247\n",
      "Training log: 18 epoch (18048 / 60000 train. data). Loss: 0.004919324070215225\n",
      "Training log: 18 epoch (19328 / 60000 train. data). Loss: 0.0033640675246715546\n",
      "Training log: 18 epoch (20608 / 60000 train. data). Loss: 0.0032922662794589996\n",
      "Training log: 18 epoch (21888 / 60000 train. data). Loss: 0.011360179632902145\n",
      "Training log: 18 epoch (23168 / 60000 train. data). Loss: 0.006304842419922352\n",
      "Training log: 18 epoch (24448 / 60000 train. data). Loss: 0.0016530416905879974\n",
      "Training log: 18 epoch (25728 / 60000 train. data). Loss: 0.012420104816555977\n",
      "Training log: 18 epoch (27008 / 60000 train. data). Loss: 0.0024850592017173767\n",
      "Training log: 18 epoch (28288 / 60000 train. data). Loss: 0.006134888157248497\n",
      "Training log: 18 epoch (29568 / 60000 train. data). Loss: 0.004748370498418808\n",
      "Training log: 18 epoch (30848 / 60000 train. data). Loss: 0.008125601336359978\n",
      "Training log: 18 epoch (32128 / 60000 train. data). Loss: 0.007300646975636482\n",
      "Training log: 18 epoch (33408 / 60000 train. data). Loss: 0.0025078393518924713\n",
      "Training log: 18 epoch (34688 / 60000 train. data). Loss: 0.009356431663036346\n",
      "Training log: 18 epoch (35968 / 60000 train. data). Loss: 0.004220001399517059\n",
      "Training log: 18 epoch (37248 / 60000 train. data). Loss: 0.0066841039806604385\n",
      "Training log: 18 epoch (38528 / 60000 train. data). Loss: 0.01170869916677475\n",
      "Training log: 18 epoch (39808 / 60000 train. data). Loss: 0.0043992772698402405\n",
      "Training log: 18 epoch (41088 / 60000 train. data). Loss: 0.010713595896959305\n",
      "Training log: 18 epoch (42368 / 60000 train. data). Loss: 0.0078569445759058\n",
      "Training log: 18 epoch (43648 / 60000 train. data). Loss: 0.002308610826730728\n",
      "Training log: 18 epoch (44928 / 60000 train. data). Loss: 0.0019807275384664536\n",
      "Training log: 18 epoch (46208 / 60000 train. data). Loss: 0.015150424093008041\n",
      "Training log: 18 epoch (47488 / 60000 train. data). Loss: 0.0077770669013261795\n",
      "Training log: 18 epoch (48768 / 60000 train. data). Loss: 0.015291322022676468\n",
      "Training log: 18 epoch (50048 / 60000 train. data). Loss: 0.037200141698122025\n",
      "Training log: 18 epoch (51328 / 60000 train. data). Loss: 0.059827569872140884\n",
      "Training log: 18 epoch (52608 / 60000 train. data). Loss: 0.0028130710124969482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 18 epoch (53888 / 60000 train. data). Loss: 0.004112806171178818\n",
      "Training log: 18 epoch (55168 / 60000 train. data). Loss: 0.006106095388531685\n",
      "Training log: 18 epoch (56448 / 60000 train. data). Loss: 0.011386847123503685\n",
      "Training log: 18 epoch (57728 / 60000 train. data). Loss: 0.011911343783140182\n",
      "Training log: 18 epoch (59008 / 60000 train. data). Loss: 0.004600172862410545\n",
      "Test loss (avg): 0.07011215901374816, Accuracy: 0.9798\n",
      "Training log: 19 epoch (128 / 60000 train. data). Loss: 0.021432483568787575\n",
      "Training log: 19 epoch (1408 / 60000 train. data). Loss: 0.008428698405623436\n",
      "Training log: 19 epoch (2688 / 60000 train. data). Loss: 0.013587994500994682\n",
      "Training log: 19 epoch (3968 / 60000 train. data). Loss: 0.008208619430661201\n",
      "Training log: 19 epoch (5248 / 60000 train. data). Loss: 0.006712013855576515\n",
      "Training log: 19 epoch (6528 / 60000 train. data). Loss: 0.004298120737075806\n",
      "Training log: 19 epoch (7808 / 60000 train. data). Loss: 0.0008708648383617401\n",
      "Training log: 19 epoch (9088 / 60000 train. data). Loss: 0.0022198185324668884\n",
      "Training log: 19 epoch (10368 / 60000 train. data). Loss: 0.005942748859524727\n",
      "Training log: 19 epoch (11648 / 60000 train. data). Loss: 0.004723750054836273\n",
      "Training log: 19 epoch (12928 / 60000 train. data). Loss: 0.006424048915505409\n",
      "Training log: 19 epoch (14208 / 60000 train. data). Loss: 0.008327590301632881\n",
      "Training log: 19 epoch (15488 / 60000 train. data). Loss: 0.006584011949598789\n",
      "Training log: 19 epoch (16768 / 60000 train. data). Loss: 0.00621771439909935\n",
      "Training log: 19 epoch (18048 / 60000 train. data). Loss: 0.004757687449455261\n",
      "Training log: 19 epoch (19328 / 60000 train. data). Loss: 0.007079225033521652\n",
      "Training log: 19 epoch (20608 / 60000 train. data). Loss: 0.0046021584421396255\n",
      "Training log: 19 epoch (21888 / 60000 train. data). Loss: 0.0038458239287137985\n",
      "Training log: 19 epoch (23168 / 60000 train. data). Loss: 0.008208448067307472\n",
      "Training log: 19 epoch (24448 / 60000 train. data). Loss: 0.006389518268406391\n",
      "Training log: 19 epoch (25728 / 60000 train. data). Loss: 0.002166610211133957\n",
      "Training log: 19 epoch (27008 / 60000 train. data). Loss: 0.009848680347204208\n",
      "Training log: 19 epoch (28288 / 60000 train. data). Loss: 0.0035855770111083984\n",
      "Training log: 19 epoch (29568 / 60000 train. data). Loss: 0.0069251516833901405\n",
      "Training log: 19 epoch (30848 / 60000 train. data). Loss: 0.007060149684548378\n",
      "Training log: 19 epoch (32128 / 60000 train. data). Loss: 0.01192278228700161\n",
      "Training log: 19 epoch (33408 / 60000 train. data). Loss: 0.005055045709013939\n",
      "Training log: 19 epoch (34688 / 60000 train. data). Loss: 0.00449448823928833\n",
      "Training log: 19 epoch (35968 / 60000 train. data). Loss: 0.008948476985096931\n",
      "Training log: 19 epoch (37248 / 60000 train. data). Loss: 0.015986494719982147\n",
      "Training log: 19 epoch (38528 / 60000 train. data). Loss: 0.003745509311556816\n",
      "Training log: 19 epoch (39808 / 60000 train. data). Loss: 0.015235114842653275\n",
      "Training log: 19 epoch (41088 / 60000 train. data). Loss: 0.006842456758022308\n",
      "Training log: 19 epoch (42368 / 60000 train. data). Loss: 0.002323441207408905\n",
      "Training log: 19 epoch (43648 / 60000 train. data). Loss: 0.005851371213793755\n",
      "Training log: 19 epoch (44928 / 60000 train. data). Loss: 0.0018935203552246094\n",
      "Training log: 19 epoch (46208 / 60000 train. data). Loss: 0.006227415055036545\n",
      "Training log: 19 epoch (47488 / 60000 train. data). Loss: 0.005123223178088665\n",
      "Training log: 19 epoch (48768 / 60000 train. data). Loss: 0.004175018519163132\n",
      "Training log: 19 epoch (50048 / 60000 train. data). Loss: 0.004064377397298813\n",
      "Training log: 19 epoch (51328 / 60000 train. data). Loss: 0.01168505847454071\n",
      "Training log: 19 epoch (52608 / 60000 train. data). Loss: 0.002166401594877243\n",
      "Training log: 19 epoch (53888 / 60000 train. data). Loss: 0.008656419813632965\n",
      "Training log: 19 epoch (55168 / 60000 train. data). Loss: 0.005631402134895325\n",
      "Training log: 19 epoch (56448 / 60000 train. data). Loss: 0.004101473838090897\n",
      "Training log: 19 epoch (57728 / 60000 train. data). Loss: 0.010175347328186035\n",
      "Training log: 19 epoch (59008 / 60000 train. data). Loss: 0.003107083961367607\n",
      "Test loss (avg): 0.06402575981616974, Accuracy: 0.9814\n",
      "Training log: 20 epoch (128 / 60000 train. data). Loss: 0.0022871941328048706\n",
      "Training log: 20 epoch (1408 / 60000 train. data). Loss: 0.0075260940939188\n",
      "Training log: 20 epoch (2688 / 60000 train. data). Loss: 0.003298681229352951\n",
      "Training log: 20 epoch (3968 / 60000 train. data). Loss: 0.005156440660357475\n",
      "Training log: 20 epoch (5248 / 60000 train. data). Loss: 0.00515340268611908\n",
      "Training log: 20 epoch (6528 / 60000 train. data). Loss: 0.004368520807474852\n",
      "Training log: 20 epoch (7808 / 60000 train. data). Loss: 0.0019428469240665436\n",
      "Training log: 20 epoch (9088 / 60000 train. data). Loss: 0.0019436795264482498\n",
      "Training log: 20 epoch (10368 / 60000 train. data). Loss: 0.0035061584785580635\n",
      "Training log: 20 epoch (11648 / 60000 train. data). Loss: 0.0017470978200435638\n",
      "Training log: 20 epoch (12928 / 60000 train. data). Loss: 0.006985774729400873\n",
      "Training log: 20 epoch (14208 / 60000 train. data). Loss: 0.0018858537077903748\n",
      "Training log: 20 epoch (15488 / 60000 train. data). Loss: 0.003208031877875328\n",
      "Training log: 20 epoch (16768 / 60000 train. data). Loss: 0.0015026405453681946\n",
      "Training log: 20 epoch (18048 / 60000 train. data). Loss: 0.0017922110855579376\n",
      "Training log: 20 epoch (19328 / 60000 train. data). Loss: 0.00535147450864315\n",
      "Training log: 20 epoch (20608 / 60000 train. data). Loss: 0.0011594966053962708\n",
      "Training log: 20 epoch (21888 / 60000 train. data). Loss: 0.004453321918845177\n",
      "Training log: 20 epoch (23168 / 60000 train. data). Loss: 0.0031242333352565765\n",
      "Training log: 20 epoch (24448 / 60000 train. data). Loss: 0.0016386453062295914\n",
      "Training log: 20 epoch (25728 / 60000 train. data). Loss: 0.00435700174421072\n",
      "Training log: 20 epoch (27008 / 60000 train. data). Loss: 0.011223227716982365\n",
      "Training log: 20 epoch (28288 / 60000 train. data). Loss: 0.003621925599873066\n",
      "Training log: 20 epoch (29568 / 60000 train. data). Loss: 0.0038403496146202087\n",
      "Training log: 20 epoch (30848 / 60000 train. data). Loss: 0.002291036769747734\n",
      "Training log: 20 epoch (32128 / 60000 train. data). Loss: 0.0047945380210876465\n",
      "Training log: 20 epoch (33408 / 60000 train. data). Loss: 0.0012763254344463348\n",
      "Training log: 20 epoch (34688 / 60000 train. data). Loss: 0.0011328961700201035\n",
      "Training log: 20 epoch (35968 / 60000 train. data). Loss: 0.0018136613070964813\n",
      "Training log: 20 epoch (37248 / 60000 train. data). Loss: 0.005729516968131065\n",
      "Training log: 20 epoch (38528 / 60000 train. data). Loss: 0.0012056194245815277\n",
      "Training log: 20 epoch (39808 / 60000 train. data). Loss: 0.0030635595321655273\n",
      "Training log: 20 epoch (41088 / 60000 train. data). Loss: 0.006680579390376806\n",
      "Training log: 20 epoch (42368 / 60000 train. data). Loss: 0.00875878892838955\n",
      "Training log: 20 epoch (43648 / 60000 train. data). Loss: 0.004882965236902237\n",
      "Training log: 20 epoch (44928 / 60000 train. data). Loss: 0.0017431564629077911\n",
      "Training log: 20 epoch (46208 / 60000 train. data). Loss: 0.0007828660309314728\n",
      "Training log: 20 epoch (47488 / 60000 train. data). Loss: 0.014439692720770836\n",
      "Training log: 20 epoch (48768 / 60000 train. data). Loss: 0.004754152148962021\n",
      "Training log: 20 epoch (50048 / 60000 train. data). Loss: 0.0044273491948843\n",
      "Training log: 20 epoch (51328 / 60000 train. data). Loss: 0.0033447202295064926\n",
      "Training log: 20 epoch (52608 / 60000 train. data). Loss: 0.006687106564640999\n",
      "Training log: 20 epoch (53888 / 60000 train. data). Loss: 0.004659602418541908\n",
      "Training log: 20 epoch (55168 / 60000 train. data). Loss: 0.00410325825214386\n",
      "Training log: 20 epoch (56448 / 60000 train. data). Loss: 0.0026377681642770767\n",
      "Training log: 20 epoch (57728 / 60000 train. data). Loss: 0.004520360380411148\n",
      "Training log: 20 epoch (59008 / 60000 train. data). Loss: 0.023237373679876328\n",
      "Test loss (avg): 0.06755600819587708, Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "for i_epoch in range(num_epoch):\n",
    "    loss = None\n",
    "    \n",
    "    net.train(True)\n",
    "    \n",
    "    for i, (data, target) in enumerate(loaders['train']):\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = f.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(i_epoch+1, (i+1)*128, loss.item()))\n",
    "    \n",
    "    history['train_loss'].append(loss)\n",
    "    \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = net(data)\n",
    "            test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= 10000\n",
    "    print('Test loss (avg): {}, Accuracy: {}'.format(test_loss, correct / 10000))\n",
    "    \n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(correct / 10000)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [tensor(0.4600, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)], 'test_loss': [0.26288632473945617, 0.19992919902801515, 0.16125908455848695, 0.1258249189376831, 0.11241939449310302, 0.09403710088729858, 0.08853961670398712, 0.08204093081951142, 0.07768251745700837, 0.07529574549198151, 0.06827726354599, 0.06576207578182221, 0.06716034919023514, 0.061372332894802095, 0.06533857176303863, 0.06301233727931976, 0.06424263126850129, 0.06391300194263458, 0.06729843227863312, 0.07088353017568588], 'test_acc': [0.9229, 0.9411, 0.9501, 0.962, 0.9654, 0.9718, 0.974, 0.9753, 0.9768, 0.9773, 0.9789, 0.9803, 0.9811, 0.9826, 0.9802, 0.9819, 0.9812, 0.9818, 0.9809, 0.9799]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epoch\n",
    "print(history)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['train_loss'])), history['train_loss'], label='train_loss')\n",
    "plt.plot(range(len(history['test_loss'])), history['test_loss'], label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['test_acc'])), history['test_acc'])\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('test_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
