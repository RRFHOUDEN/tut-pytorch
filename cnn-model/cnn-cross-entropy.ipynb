{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCnn(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyCnn, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # kernel_size:畳み込む行列のサイズ\n",
    "            # stride:スライドするときのサイズ\n",
    "            # padding:上下左右に追加する空白\n",
    "            # 1*28*28 のデータ\n",
    "            nn.Conv2d(1, 64, kernel_size=2, stride=1, padding=1), # 1*28*28 => 64*29*29\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), #64*29*29 => 64*27*27 46656\n",
    "            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1), # 64*27*27 => 192*27*27 139968\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3), # 192*27*27 => 192*9*9 15552\n",
    "            nn.Conv2d(192, 384, kernel_size=2, stride=1, padding=1), # 192*9*9 => 384*10*10\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1), # 384*10*10 => 256*10*10\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), # 256*10*10 => 256*10*10\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 256*10*10 => 256*5*5\n",
    "        )\n",
    "            \n",
    "        self.classifier = nn.Linear(256*5*5, 10)\n",
    "#         self.classifier = nn.Linear(28*28, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    return {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroDLCnn(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ZeroDLCnn, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # kernel_size:畳み込む行列のサイズ\n",
    "            # stride:スライドするときのサイズ\n",
    "            # padding:上下左右に追加する空白\n",
    "            # 1*28*28 のデータ\n",
    "            nn.Conv2d(1, 30, kernel_size=5, stride=1, padding=0), # 1*28*28 => 30*24*24\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #30*24*24 => 30*12*12\n",
    "        )\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(30*12*12, 30*12*12),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(30*12*12, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20\n",
    "history = {\n",
    "    'train_loss':[],\n",
    "    'test_loss':[],\n",
    "    'test_acc':[]\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = ZeroDLCnn().to(device)\n",
    "loaders = load_MNIST()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (128 / 60000 train. data). Loss: -0.10040080547332764\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: -0.44055724143981934\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: -0.6125679016113281\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: -0.6613986492156982\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: -0.7226717472076416\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: -0.6618556976318359\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: -0.6905960440635681\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: -0.6745857000350952\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: -0.6474494338035583\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: -0.634105920791626\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: -0.6451714634895325\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: -0.5978111624717712\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: -0.6618903875350952\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: -0.6218517422676086\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: -0.6410644054412842\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: -0.6725608706474304\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: -0.6836560964584351\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: -0.7228739857673645\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: -0.5738097429275513\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: -0.5974345803260803\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: -0.6980409622192383\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: -0.6981624960899353\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: -0.7470155358314514\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: -0.712293267250061\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: -0.6846717000007629\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: -0.6810023188591003\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: -0.7060733437538147\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: -0.7016641497612\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: -0.7136194109916687\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: -0.6753132939338684\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: -0.7294509410858154\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: -0.7174001932144165\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: -0.736295759677887\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: -0.7107234001159668\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: -0.6350482702255249\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: -0.6800356507301331\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: -0.7171519994735718\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: -0.7222327589988708\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: -0.7187851667404175\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: -0.8058769106864929\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: -0.7535436153411865\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: -0.7993720769882202\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: -0.8186354637145996\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: -0.8189188241958618\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: -0.8316681981086731\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: -0.8955398797988892\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: -0.8860659003257751\n",
      "Test loss (avg): -0.8676578647613525, Accuracy: 0.8695\n",
      "Training log: 2 epoch (128 / 60000 train. data). Loss: -0.861586332321167\n",
      "Training log: 2 epoch (1408 / 60000 train. data). Loss: -0.9039537906646729\n",
      "Training log: 2 epoch (2688 / 60000 train. data). Loss: -0.8794764280319214\n",
      "Training log: 2 epoch (3968 / 60000 train. data). Loss: -0.8619956970214844\n",
      "Training log: 2 epoch (5248 / 60000 train. data). Loss: -0.8565816283226013\n",
      "Training log: 2 epoch (6528 / 60000 train. data). Loss: -0.9140289425849915\n",
      "Training log: 2 epoch (7808 / 60000 train. data). Loss: -0.96589595079422\n",
      "Training log: 2 epoch (9088 / 60000 train. data). Loss: -0.9618642330169678\n",
      "Training log: 2 epoch (10368 / 60000 train. data). Loss: -0.9562743902206421\n",
      "Training log: 2 epoch (11648 / 60000 train. data). Loss: -0.9565795660018921\n",
      "Training log: 2 epoch (12928 / 60000 train. data). Loss: -0.9341157674789429\n",
      "Training log: 2 epoch (14208 / 60000 train. data). Loss: -0.974862813949585\n",
      "Training log: 2 epoch (15488 / 60000 train. data). Loss: -0.9401283264160156\n",
      "Training log: 2 epoch (16768 / 60000 train. data). Loss: -0.9680361747741699\n",
      "Training log: 2 epoch (18048 / 60000 train. data). Loss: -0.9673509001731873\n",
      "Training log: 2 epoch (19328 / 60000 train. data). Loss: -0.9617092609405518\n",
      "Training log: 2 epoch (20608 / 60000 train. data). Loss: -0.974403977394104\n",
      "Training log: 2 epoch (21888 / 60000 train. data). Loss: -0.9845439195632935\n",
      "Training log: 2 epoch (23168 / 60000 train. data). Loss: -0.9656468629837036\n",
      "Training log: 2 epoch (24448 / 60000 train. data). Loss: -0.9479376077651978\n",
      "Training log: 2 epoch (25728 / 60000 train. data). Loss: -0.982978880405426\n",
      "Training log: 2 epoch (27008 / 60000 train. data). Loss: -0.9724821448326111\n",
      "Training log: 2 epoch (28288 / 60000 train. data). Loss: -0.962737500667572\n",
      "Training log: 2 epoch (29568 / 60000 train. data). Loss: -0.9802581071853638\n",
      "Training log: 2 epoch (30848 / 60000 train. data). Loss: -0.9372600317001343\n",
      "Training log: 2 epoch (32128 / 60000 train. data). Loss: -0.9967614412307739\n",
      "Training log: 2 epoch (33408 / 60000 train. data). Loss: -0.9697830080986023\n",
      "Training log: 2 epoch (34688 / 60000 train. data). Loss: -0.9723984599113464\n",
      "Training log: 2 epoch (35968 / 60000 train. data). Loss: -0.9658447504043579\n",
      "Training log: 2 epoch (37248 / 60000 train. data). Loss: -0.9733253717422485\n",
      "Training log: 2 epoch (38528 / 60000 train. data). Loss: -0.980967104434967\n",
      "Training log: 2 epoch (39808 / 60000 train. data). Loss: -0.9808382987976074\n",
      "Training log: 2 epoch (41088 / 60000 train. data). Loss: -0.9688815474510193\n",
      "Training log: 2 epoch (42368 / 60000 train. data). Loss: -0.9721792340278625\n",
      "Training log: 2 epoch (43648 / 60000 train. data). Loss: -0.9355033040046692\n",
      "Training log: 2 epoch (44928 / 60000 train. data). Loss: -0.9767293334007263\n",
      "Training log: 2 epoch (46208 / 60000 train. data). Loss: -0.9817520976066589\n",
      "Training log: 2 epoch (47488 / 60000 train. data). Loss: -0.9727383852005005\n",
      "Training log: 2 epoch (48768 / 60000 train. data). Loss: -0.9818111062049866\n",
      "Training log: 2 epoch (50048 / 60000 train. data). Loss: -0.9789639711380005\n",
      "Training log: 2 epoch (51328 / 60000 train. data). Loss: -0.9995139837265015\n",
      "Training log: 2 epoch (52608 / 60000 train. data). Loss: -0.9741801023483276\n",
      "Training log: 2 epoch (53888 / 60000 train. data). Loss: -0.9690292477607727\n",
      "Training log: 2 epoch (55168 / 60000 train. data). Loss: -0.9797918796539307\n",
      "Training log: 2 epoch (56448 / 60000 train. data). Loss: -0.9991470575332642\n",
      "Training log: 2 epoch (57728 / 60000 train. data). Loss: -0.9836135506629944\n",
      "Training log: 2 epoch (59008 / 60000 train. data). Loss: -0.965086817741394\n",
      "Test loss (avg): -0.9793869574546814, Accuracy: 0.981\n",
      "Training log: 3 epoch (128 / 60000 train. data). Loss: -0.9856078624725342\n",
      "Training log: 3 epoch (1408 / 60000 train. data). Loss: -0.9828137159347534\n",
      "Training log: 3 epoch (2688 / 60000 train. data). Loss: -0.9802177548408508\n",
      "Training log: 3 epoch (3968 / 60000 train. data). Loss: -0.9909998774528503\n",
      "Training log: 3 epoch (5248 / 60000 train. data). Loss: -0.9675122499465942\n",
      "Training log: 3 epoch (6528 / 60000 train. data). Loss: -0.9679478406906128\n",
      "Training log: 3 epoch (7808 / 60000 train. data). Loss: -0.9808269143104553\n",
      "Training log: 3 epoch (9088 / 60000 train. data). Loss: -0.9700325727462769\n",
      "Training log: 3 epoch (10368 / 60000 train. data). Loss: -0.9841651916503906\n",
      "Training log: 3 epoch (11648 / 60000 train. data). Loss: -0.9930321574211121\n",
      "Training log: 3 epoch (12928 / 60000 train. data). Loss: -0.9926002621650696\n",
      "Training log: 3 epoch (14208 / 60000 train. data). Loss: -0.9998558759689331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 3 epoch (15488 / 60000 train. data). Loss: -0.9895012974739075\n",
      "Training log: 3 epoch (16768 / 60000 train. data). Loss: -0.976248562335968\n",
      "Training log: 3 epoch (18048 / 60000 train. data). Loss: -0.9563840627670288\n",
      "Training log: 3 epoch (19328 / 60000 train. data). Loss: -0.9862301349639893\n",
      "Training log: 3 epoch (20608 / 60000 train. data). Loss: -0.9844238758087158\n",
      "Training log: 3 epoch (21888 / 60000 train. data). Loss: -0.9787388443946838\n",
      "Training log: 3 epoch (23168 / 60000 train. data). Loss: -0.984586238861084\n",
      "Training log: 3 epoch (24448 / 60000 train. data). Loss: -0.9994561672210693\n",
      "Training log: 3 epoch (25728 / 60000 train. data). Loss: -0.9999067187309265\n",
      "Training log: 3 epoch (27008 / 60000 train. data). Loss: -0.9847069978713989\n",
      "Training log: 3 epoch (28288 / 60000 train. data). Loss: -0.9672856330871582\n",
      "Training log: 3 epoch (29568 / 60000 train. data). Loss: -0.981871485710144\n",
      "Training log: 3 epoch (30848 / 60000 train. data). Loss: -0.9572495222091675\n",
      "Training log: 3 epoch (32128 / 60000 train. data). Loss: -0.978959321975708\n",
      "Training log: 3 epoch (33408 / 60000 train. data). Loss: -0.9968246817588806\n",
      "Training log: 3 epoch (34688 / 60000 train. data). Loss: -0.9945526719093323\n",
      "Training log: 3 epoch (35968 / 60000 train. data). Loss: -0.9821324348449707\n",
      "Training log: 3 epoch (37248 / 60000 train. data). Loss: -0.9756563305854797\n",
      "Training log: 3 epoch (38528 / 60000 train. data). Loss: -0.9779937863349915\n",
      "Training log: 3 epoch (39808 / 60000 train. data). Loss: -0.9827108383178711\n",
      "Training log: 3 epoch (41088 / 60000 train. data). Loss: -0.9798541069030762\n",
      "Training log: 3 epoch (42368 / 60000 train. data). Loss: -0.978452742099762\n",
      "Training log: 3 epoch (43648 / 60000 train. data). Loss: -0.9921672940254211\n",
      "Training log: 3 epoch (44928 / 60000 train. data). Loss: -0.9777143597602844\n",
      "Training log: 3 epoch (46208 / 60000 train. data). Loss: -0.9918838739395142\n",
      "Training log: 3 epoch (47488 / 60000 train. data). Loss: -0.9932886362075806\n",
      "Training log: 3 epoch (48768 / 60000 train. data). Loss: -0.9859947562217712\n",
      "Training log: 3 epoch (50048 / 60000 train. data). Loss: -0.9623783230781555\n",
      "Training log: 3 epoch (51328 / 60000 train. data). Loss: -0.9918769598007202\n",
      "Training log: 3 epoch (52608 / 60000 train. data). Loss: -0.9991012811660767\n",
      "Training log: 3 epoch (53888 / 60000 train. data). Loss: -0.9794893264770508\n",
      "Training log: 3 epoch (55168 / 60000 train. data). Loss: -0.9892743229866028\n",
      "Training log: 3 epoch (56448 / 60000 train. data). Loss: -0.9837595224380493\n",
      "Training log: 3 epoch (57728 / 60000 train. data). Loss: -0.9745044112205505\n",
      "Training log: 3 epoch (59008 / 60000 train. data). Loss: -0.991134524345398\n",
      "Test loss (avg): -0.9781899199485778, Accuracy: 0.979\n",
      "Training log: 4 epoch (128 / 60000 train. data). Loss: -0.9951339364051819\n",
      "Training log: 4 epoch (1408 / 60000 train. data). Loss: -0.9685168862342834\n",
      "Training log: 4 epoch (2688 / 60000 train. data). Loss: -0.9920263290405273\n",
      "Training log: 4 epoch (3968 / 60000 train. data). Loss: -0.9999199509620667\n",
      "Training log: 4 epoch (5248 / 60000 train. data). Loss: -0.9817943572998047\n",
      "Training log: 4 epoch (6528 / 60000 train. data). Loss: -0.9741560816764832\n",
      "Training log: 4 epoch (7808 / 60000 train. data). Loss: -0.956031858921051\n",
      "Training log: 4 epoch (9088 / 60000 train. data). Loss: -0.9878131151199341\n",
      "Training log: 4 epoch (10368 / 60000 train. data). Loss: -0.9956492185592651\n",
      "Training log: 4 epoch (11648 / 60000 train. data). Loss: -0.986378014087677\n",
      "Training log: 4 epoch (12928 / 60000 train. data). Loss: -0.9841915369033813\n",
      "Training log: 4 epoch (14208 / 60000 train. data). Loss: -0.9924442768096924\n",
      "Training log: 4 epoch (15488 / 60000 train. data). Loss: -0.9638167023658752\n",
      "Training log: 4 epoch (16768 / 60000 train. data). Loss: -0.9778851270675659\n",
      "Training log: 4 epoch (18048 / 60000 train. data). Loss: -0.975949764251709\n",
      "Training log: 4 epoch (19328 / 60000 train. data). Loss: -0.9692922830581665\n",
      "Training log: 4 epoch (20608 / 60000 train. data). Loss: -0.986384391784668\n",
      "Training log: 4 epoch (21888 / 60000 train. data). Loss: -0.9842453598976135\n",
      "Training log: 4 epoch (23168 / 60000 train. data). Loss: -0.9997522830963135\n",
      "Training log: 4 epoch (24448 / 60000 train. data). Loss: -0.9941492676734924\n",
      "Training log: 4 epoch (25728 / 60000 train. data). Loss: -0.991827666759491\n",
      "Training log: 4 epoch (27008 / 60000 train. data). Loss: -0.9866465926170349\n",
      "Training log: 4 epoch (28288 / 60000 train. data). Loss: -0.9922388792037964\n",
      "Training log: 4 epoch (29568 / 60000 train. data). Loss: -0.9782077670097351\n",
      "Training log: 4 epoch (30848 / 60000 train. data). Loss: -0.9826847314834595\n",
      "Training log: 4 epoch (32128 / 60000 train. data). Loss: -0.9922043681144714\n",
      "Training log: 4 epoch (33408 / 60000 train. data). Loss: -0.9898313879966736\n",
      "Training log: 4 epoch (34688 / 60000 train. data). Loss: -0.97690749168396\n",
      "Training log: 4 epoch (35968 / 60000 train. data). Loss: -0.999308705329895\n",
      "Training log: 4 epoch (37248 / 60000 train. data). Loss: -0.9771856069564819\n",
      "Training log: 4 epoch (38528 / 60000 train. data). Loss: -0.9933935403823853\n",
      "Training log: 4 epoch (39808 / 60000 train. data). Loss: -0.9792556166648865\n",
      "Training log: 4 epoch (41088 / 60000 train. data). Loss: -0.9837179183959961\n",
      "Training log: 4 epoch (42368 / 60000 train. data). Loss: -0.9783974885940552\n",
      "Training log: 4 epoch (43648 / 60000 train. data). Loss: -0.9839453101158142\n",
      "Training log: 4 epoch (44928 / 60000 train. data). Loss: -0.9702158570289612\n",
      "Training log: 4 epoch (46208 / 60000 train. data). Loss: -0.9920355677604675\n",
      "Training log: 4 epoch (47488 / 60000 train. data). Loss: -0.9983623027801514\n",
      "Training log: 4 epoch (48768 / 60000 train. data). Loss: -0.999138355255127\n",
      "Training log: 4 epoch (50048 / 60000 train. data). Loss: -0.9806385040283203\n",
      "Training log: 4 epoch (51328 / 60000 train. data). Loss: -0.9769363403320312\n",
      "Training log: 4 epoch (52608 / 60000 train. data). Loss: -0.998972475528717\n",
      "Training log: 4 epoch (53888 / 60000 train. data). Loss: -0.9887731671333313\n",
      "Training log: 4 epoch (55168 / 60000 train. data). Loss: -0.9823319315910339\n",
      "Training log: 4 epoch (56448 / 60000 train. data). Loss: -0.9891589283943176\n",
      "Training log: 4 epoch (57728 / 60000 train. data). Loss: -0.981834888458252\n",
      "Training log: 4 epoch (59008 / 60000 train. data). Loss: -0.999991774559021\n",
      "Test loss (avg): -0.9842680318832397, Accuracy: 0.9852\n",
      "Training log: 5 epoch (128 / 60000 train. data). Loss: -0.961051881313324\n",
      "Training log: 5 epoch (1408 / 60000 train. data). Loss: -0.9843376874923706\n",
      "Training log: 5 epoch (2688 / 60000 train. data). Loss: -0.966683566570282\n",
      "Training log: 5 epoch (3968 / 60000 train. data). Loss: -0.9966821670532227\n",
      "Training log: 5 epoch (5248 / 60000 train. data). Loss: -0.9746268391609192\n",
      "Training log: 5 epoch (6528 / 60000 train. data). Loss: -0.999978244304657\n",
      "Training log: 5 epoch (7808 / 60000 train. data). Loss: -0.9921274185180664\n",
      "Training log: 5 epoch (9088 / 60000 train. data). Loss: -0.9865802526473999\n",
      "Training log: 5 epoch (10368 / 60000 train. data). Loss: -0.9884633421897888\n",
      "Training log: 5 epoch (11648 / 60000 train. data). Loss: -0.9886125326156616\n",
      "Training log: 5 epoch (12928 / 60000 train. data). Loss: -0.9919651746749878\n",
      "Training log: 5 epoch (14208 / 60000 train. data). Loss: -0.9974121451377869\n",
      "Training log: 5 epoch (15488 / 60000 train. data). Loss: -0.9860451221466064\n",
      "Training log: 5 epoch (16768 / 60000 train. data). Loss: -0.9921894669532776\n",
      "Training log: 5 epoch (18048 / 60000 train. data). Loss: -0.9798745512962341\n",
      "Training log: 5 epoch (19328 / 60000 train. data). Loss: -0.9834158420562744\n",
      "Training log: 5 epoch (20608 / 60000 train. data). Loss: -0.9975531697273254\n",
      "Training log: 5 epoch (21888 / 60000 train. data). Loss: -0.9897252321243286\n",
      "Training log: 5 epoch (23168 / 60000 train. data). Loss: -0.9924505352973938\n",
      "Training log: 5 epoch (24448 / 60000 train. data). Loss: -0.9776800274848938\n",
      "Training log: 5 epoch (25728 / 60000 train. data). Loss: -0.9879051446914673\n",
      "Training log: 5 epoch (27008 / 60000 train. data). Loss: -0.9995201230049133\n",
      "Training log: 5 epoch (28288 / 60000 train. data). Loss: -0.988362729549408\n",
      "Training log: 5 epoch (29568 / 60000 train. data). Loss: -0.9869981408119202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 5 epoch (30848 / 60000 train. data). Loss: -0.9912493228912354\n",
      "Training log: 5 epoch (32128 / 60000 train. data). Loss: -0.9998592138290405\n",
      "Training log: 5 epoch (33408 / 60000 train. data). Loss: -0.9689576029777527\n",
      "Training log: 5 epoch (34688 / 60000 train. data). Loss: -0.9753574132919312\n",
      "Training log: 5 epoch (35968 / 60000 train. data). Loss: -0.9993498921394348\n",
      "Training log: 5 epoch (37248 / 60000 train. data). Loss: -0.9999514222145081\n",
      "Training log: 5 epoch (38528 / 60000 train. data). Loss: -0.9797213077545166\n",
      "Training log: 5 epoch (39808 / 60000 train. data). Loss: -0.9779539704322815\n",
      "Training log: 5 epoch (41088 / 60000 train. data). Loss: -0.9970047473907471\n",
      "Training log: 5 epoch (42368 / 60000 train. data). Loss: -0.9967511296272278\n",
      "Training log: 5 epoch (43648 / 60000 train. data). Loss: -0.9890555739402771\n",
      "Training log: 5 epoch (44928 / 60000 train. data). Loss: -0.9887833595275879\n",
      "Training log: 5 epoch (46208 / 60000 train. data). Loss: -0.9805943965911865\n",
      "Training log: 5 epoch (47488 / 60000 train. data). Loss: -0.9916767477989197\n",
      "Training log: 5 epoch (48768 / 60000 train. data). Loss: -0.9891635179519653\n",
      "Training log: 5 epoch (50048 / 60000 train. data). Loss: -0.9842659831047058\n",
      "Training log: 5 epoch (51328 / 60000 train. data). Loss: -0.9923194646835327\n",
      "Training log: 5 epoch (52608 / 60000 train. data). Loss: -0.9762067198753357\n",
      "Training log: 5 epoch (53888 / 60000 train. data). Loss: -0.9991379976272583\n",
      "Training log: 5 epoch (55168 / 60000 train. data). Loss: -0.9881172776222229\n",
      "Training log: 5 epoch (56448 / 60000 train. data). Loss: -0.9969243407249451\n",
      "Training log: 5 epoch (57728 / 60000 train. data). Loss: -0.9783743619918823\n",
      "Training log: 5 epoch (59008 / 60000 train. data). Loss: -0.9784867763519287\n",
      "Test loss (avg): -0.9841402331352234, Accuracy: 0.9843\n",
      "Training log: 6 epoch (128 / 60000 train. data). Loss: -0.989241898059845\n",
      "Training log: 6 epoch (1408 / 60000 train. data). Loss: -0.9892171025276184\n",
      "Training log: 6 epoch (2688 / 60000 train. data). Loss: -0.9847089052200317\n",
      "Training log: 6 epoch (3968 / 60000 train. data). Loss: -0.9850163459777832\n",
      "Training log: 6 epoch (5248 / 60000 train. data). Loss: -0.9999324083328247\n",
      "Training log: 6 epoch (6528 / 60000 train. data). Loss: -0.9893302321434021\n",
      "Training log: 6 epoch (7808 / 60000 train. data). Loss: -0.984962522983551\n",
      "Training log: 6 epoch (9088 / 60000 train. data). Loss: -0.9996155500411987\n",
      "Training log: 6 epoch (10368 / 60000 train. data). Loss: -0.9979588389396667\n",
      "Training log: 6 epoch (11648 / 60000 train. data). Loss: -0.999997079372406\n",
      "Training log: 6 epoch (12928 / 60000 train. data). Loss: -0.992178738117218\n",
      "Training log: 6 epoch (14208 / 60000 train. data). Loss: -0.9826661944389343\n",
      "Training log: 6 epoch (15488 / 60000 train. data). Loss: -0.977751612663269\n",
      "Training log: 6 epoch (16768 / 60000 train. data). Loss: -0.9903329610824585\n",
      "Training log: 6 epoch (18048 / 60000 train. data). Loss: -0.9878731966018677\n",
      "Training log: 6 epoch (19328 / 60000 train. data). Loss: -0.9922051429748535\n",
      "Training log: 6 epoch (20608 / 60000 train. data). Loss: -0.9793643355369568\n",
      "Training log: 6 epoch (21888 / 60000 train. data). Loss: -0.9909539818763733\n",
      "Training log: 6 epoch (23168 / 60000 train. data). Loss: -0.9892997741699219\n",
      "Training log: 6 epoch (24448 / 60000 train. data). Loss: -0.976749062538147\n",
      "Training log: 6 epoch (25728 / 60000 train. data). Loss: -0.9882432222366333\n",
      "Training log: 6 epoch (27008 / 60000 train. data). Loss: -0.9732189178466797\n",
      "Training log: 6 epoch (28288 / 60000 train. data). Loss: -0.9850736260414124\n",
      "Training log: 6 epoch (29568 / 60000 train. data). Loss: -0.9855564832687378\n",
      "Training log: 6 epoch (30848 / 60000 train. data). Loss: -0.9821462631225586\n",
      "Training log: 6 epoch (32128 / 60000 train. data). Loss: -0.9993999600410461\n",
      "Training log: 6 epoch (33408 / 60000 train. data). Loss: -0.9803639650344849\n",
      "Training log: 6 epoch (34688 / 60000 train. data). Loss: -0.9920026659965515\n",
      "Training log: 6 epoch (35968 / 60000 train. data). Loss: -0.9797366261482239\n",
      "Training log: 6 epoch (37248 / 60000 train. data). Loss: -0.9999896287918091\n",
      "Training log: 6 epoch (38528 / 60000 train. data). Loss: -0.9921711087226868\n",
      "Training log: 6 epoch (39808 / 60000 train. data). Loss: -0.9858824014663696\n",
      "Training log: 6 epoch (41088 / 60000 train. data). Loss: -0.981967568397522\n",
      "Training log: 6 epoch (42368 / 60000 train. data). Loss: -0.9786255359649658\n",
      "Training log: 6 epoch (43648 / 60000 train. data). Loss: -0.9995124936103821\n",
      "Training log: 6 epoch (44928 / 60000 train. data). Loss: -0.9803594946861267\n",
      "Training log: 6 epoch (46208 / 60000 train. data). Loss: -0.997718334197998\n",
      "Training log: 6 epoch (47488 / 60000 train. data). Loss: -0.9918848276138306\n",
      "Training log: 6 epoch (48768 / 60000 train. data). Loss: -0.9882806539535522\n",
      "Training log: 6 epoch (50048 / 60000 train. data). Loss: -0.9830490350723267\n",
      "Training log: 6 epoch (51328 / 60000 train. data). Loss: -0.9839008450508118\n",
      "Training log: 6 epoch (52608 / 60000 train. data). Loss: -0.9985583424568176\n",
      "Training log: 6 epoch (53888 / 60000 train. data). Loss: -0.9921605587005615\n",
      "Training log: 6 epoch (55168 / 60000 train. data). Loss: -0.991439163684845\n",
      "Training log: 6 epoch (56448 / 60000 train. data). Loss: -0.9920614361763\n",
      "Training log: 6 epoch (57728 / 60000 train. data). Loss: -0.9718230962753296\n",
      "Training log: 6 epoch (59008 / 60000 train. data). Loss: -0.9841849207878113\n",
      "Test loss (avg): -0.9871273254394531, Accuracy: 0.9875\n",
      "Training log: 7 epoch (128 / 60000 train. data). Loss: -0.999980628490448\n",
      "Training log: 7 epoch (1408 / 60000 train. data). Loss: -0.9960342645645142\n",
      "Training log: 7 epoch (2688 / 60000 train. data). Loss: -0.9999135136604309\n",
      "Training log: 7 epoch (3968 / 60000 train. data). Loss: -0.9853495955467224\n",
      "Training log: 7 epoch (5248 / 60000 train. data). Loss: -0.9793641567230225\n",
      "Training log: 7 epoch (6528 / 60000 train. data). Loss: -0.988119900226593\n",
      "Training log: 7 epoch (7808 / 60000 train. data). Loss: -0.985308825969696\n",
      "Training log: 7 epoch (9088 / 60000 train. data). Loss: -0.9919894933700562\n",
      "Training log: 7 epoch (10368 / 60000 train. data). Loss: -0.9908747673034668\n",
      "Training log: 7 epoch (11648 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 7 epoch (12928 / 60000 train. data). Loss: -0.9905059337615967\n",
      "Training log: 7 epoch (14208 / 60000 train. data). Loss: -0.9997682571411133\n",
      "Training log: 7 epoch (15488 / 60000 train. data). Loss: -0.999963104724884\n",
      "Training log: 7 epoch (16768 / 60000 train. data). Loss: -0.9781191945075989\n",
      "Training log: 7 epoch (18048 / 60000 train. data). Loss: -0.9994360208511353\n",
      "Training log: 7 epoch (19328 / 60000 train. data). Loss: -0.9996169805526733\n",
      "Training log: 7 epoch (20608 / 60000 train. data). Loss: -0.9827919006347656\n",
      "Training log: 7 epoch (21888 / 60000 train. data). Loss: -0.99986332654953\n",
      "Training log: 7 epoch (23168 / 60000 train. data). Loss: -0.9915972352027893\n",
      "Training log: 7 epoch (24448 / 60000 train. data). Loss: -0.9975084066390991\n",
      "Training log: 7 epoch (25728 / 60000 train. data). Loss: -0.9997506141662598\n",
      "Training log: 7 epoch (27008 / 60000 train. data). Loss: -0.99998539686203\n",
      "Training log: 7 epoch (28288 / 60000 train. data). Loss: -0.9999242424964905\n",
      "Training log: 7 epoch (29568 / 60000 train. data). Loss: -0.9894788861274719\n",
      "Training log: 7 epoch (30848 / 60000 train. data). Loss: -0.9999859929084778\n",
      "Training log: 7 epoch (32128 / 60000 train. data). Loss: -0.9838994145393372\n",
      "Training log: 7 epoch (33408 / 60000 train. data). Loss: -0.985114336013794\n",
      "Training log: 7 epoch (34688 / 60000 train. data). Loss: -0.9912793040275574\n",
      "Training log: 7 epoch (35968 / 60000 train. data). Loss: -0.9999996423721313\n",
      "Training log: 7 epoch (37248 / 60000 train. data). Loss: -0.9835386276245117\n",
      "Training log: 7 epoch (38528 / 60000 train. data). Loss: -0.9921061396598816\n",
      "Training log: 7 epoch (39808 / 60000 train. data). Loss: -0.9945056438446045\n",
      "Training log: 7 epoch (41088 / 60000 train. data). Loss: -0.9775770306587219\n",
      "Training log: 7 epoch (42368 / 60000 train. data). Loss: -0.9946509599685669\n",
      "Training log: 7 epoch (43648 / 60000 train. data). Loss: -0.9994854927062988\n",
      "Training log: 7 epoch (44928 / 60000 train. data). Loss: -0.992183268070221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 7 epoch (46208 / 60000 train. data). Loss: -0.9865005016326904\n",
      "Training log: 7 epoch (47488 / 60000 train. data). Loss: -0.9891312718391418\n",
      "Training log: 7 epoch (48768 / 60000 train. data). Loss: -0.9919866919517517\n",
      "Training log: 7 epoch (50048 / 60000 train. data). Loss: -0.9977084398269653\n",
      "Training log: 7 epoch (51328 / 60000 train. data). Loss: -0.9881899356842041\n",
      "Training log: 7 epoch (52608 / 60000 train. data). Loss: -0.9720696210861206\n",
      "Training log: 7 epoch (53888 / 60000 train. data). Loss: -0.9830096960067749\n",
      "Training log: 7 epoch (55168 / 60000 train. data). Loss: -0.9920899271965027\n",
      "Training log: 7 epoch (56448 / 60000 train. data). Loss: -0.9994844198226929\n",
      "Training log: 7 epoch (57728 / 60000 train. data). Loss: -0.9929376840591431\n",
      "Training log: 7 epoch (59008 / 60000 train. data). Loss: -0.9921717643737793\n",
      "Test loss (avg): -0.9861274024963379, Accuracy: 0.9865\n",
      "Training log: 8 epoch (128 / 60000 train. data). Loss: -0.9921848773956299\n",
      "Training log: 8 epoch (1408 / 60000 train. data). Loss: -0.9843147993087769\n",
      "Training log: 8 epoch (2688 / 60000 train. data). Loss: -0.9689597487449646\n",
      "Training log: 8 epoch (3968 / 60000 train. data). Loss: -0.9999773502349854\n",
      "Training log: 8 epoch (5248 / 60000 train. data). Loss: -0.9893569946289062\n",
      "Training log: 8 epoch (6528 / 60000 train. data). Loss: -0.9827988743782043\n",
      "Training log: 8 epoch (7808 / 60000 train. data). Loss: -0.9967114925384521\n",
      "Training log: 8 epoch (9088 / 60000 train. data). Loss: -0.9985678791999817\n",
      "Training log: 8 epoch (10368 / 60000 train. data). Loss: -0.9841247797012329\n",
      "Training log: 8 epoch (11648 / 60000 train. data). Loss: -0.9921864867210388\n",
      "Training log: 8 epoch (12928 / 60000 train. data). Loss: -0.997151792049408\n",
      "Training log: 8 epoch (14208 / 60000 train. data). Loss: -0.9842890501022339\n",
      "Training log: 8 epoch (15488 / 60000 train. data). Loss: -0.9999257326126099\n",
      "Training log: 8 epoch (16768 / 60000 train. data). Loss: -0.9935639500617981\n",
      "Training log: 8 epoch (18048 / 60000 train. data). Loss: -0.9917492866516113\n",
      "Training log: 8 epoch (19328 / 60000 train. data). Loss: -0.9999933242797852\n",
      "Training log: 8 epoch (20608 / 60000 train. data). Loss: -0.9921035170555115\n",
      "Training log: 8 epoch (21888 / 60000 train. data). Loss: -0.9950472116470337\n",
      "Training log: 8 epoch (23168 / 60000 train. data). Loss: -0.9858271479606628\n",
      "Training log: 8 epoch (24448 / 60000 train. data). Loss: -0.9949379563331604\n",
      "Training log: 8 epoch (25728 / 60000 train. data). Loss: -0.9921773672103882\n",
      "Training log: 8 epoch (27008 / 60000 train. data). Loss: -0.9924352765083313\n",
      "Training log: 8 epoch (28288 / 60000 train. data). Loss: -0.9918569922447205\n",
      "Training log: 8 epoch (29568 / 60000 train. data). Loss: -0.9999638199806213\n",
      "Training log: 8 epoch (30848 / 60000 train. data). Loss: -0.9921872615814209\n",
      "Training log: 8 epoch (32128 / 60000 train. data). Loss: -0.9921867251396179\n",
      "Training log: 8 epoch (33408 / 60000 train. data). Loss: -0.9917945861816406\n",
      "Training log: 8 epoch (34688 / 60000 train. data). Loss: -0.9992592930793762\n",
      "Training log: 8 epoch (35968 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 8 epoch (37248 / 60000 train. data). Loss: -0.9889594912528992\n",
      "Training log: 8 epoch (38528 / 60000 train. data). Loss: -0.9844021201133728\n",
      "Training log: 8 epoch (39808 / 60000 train. data). Loss: -0.9998485445976257\n",
      "Training log: 8 epoch (41088 / 60000 train. data). Loss: -0.9899953007698059\n",
      "Training log: 8 epoch (42368 / 60000 train. data). Loss: -0.9834244847297668\n",
      "Training log: 8 epoch (43648 / 60000 train. data). Loss: -0.9708488583564758\n",
      "Training log: 8 epoch (44928 / 60000 train. data). Loss: -0.9931350946426392\n",
      "Training log: 8 epoch (46208 / 60000 train. data). Loss: -0.9918513298034668\n",
      "Training log: 8 epoch (47488 / 60000 train. data). Loss: -0.9880316257476807\n",
      "Training log: 8 epoch (48768 / 60000 train. data). Loss: -0.9921801090240479\n",
      "Training log: 8 epoch (50048 / 60000 train. data). Loss: -0.9843430519104004\n",
      "Training log: 8 epoch (51328 / 60000 train. data). Loss: -0.992671012878418\n",
      "Training log: 8 epoch (52608 / 60000 train. data). Loss: -0.9920472502708435\n",
      "Training log: 8 epoch (53888 / 60000 train. data). Loss: -0.9839855432510376\n",
      "Training log: 8 epoch (55168 / 60000 train. data). Loss: -0.9920884966850281\n",
      "Training log: 8 epoch (56448 / 60000 train. data). Loss: -0.9762144088745117\n",
      "Training log: 8 epoch (57728 / 60000 train. data). Loss: -0.9955801963806152\n",
      "Training log: 8 epoch (59008 / 60000 train. data). Loss: -0.9993929862976074\n",
      "Test loss (avg): -0.984758364868164, Accuracy: 0.9848\n",
      "Training log: 9 epoch (128 / 60000 train. data). Loss: -0.9889993667602539\n",
      "Training log: 9 epoch (1408 / 60000 train. data). Loss: -0.9925527572631836\n",
      "Training log: 9 epoch (2688 / 60000 train. data). Loss: -0.9921594262123108\n",
      "Training log: 9 epoch (3968 / 60000 train. data). Loss: -0.9999980926513672\n",
      "Training log: 9 epoch (5248 / 60000 train. data). Loss: -0.9860130548477173\n",
      "Training log: 9 epoch (6528 / 60000 train. data). Loss: -0.9997239112854004\n",
      "Training log: 9 epoch (7808 / 60000 train. data). Loss: -0.9915035963058472\n",
      "Training log: 9 epoch (9088 / 60000 train. data). Loss: -0.9826515316963196\n",
      "Training log: 9 epoch (10368 / 60000 train. data). Loss: -0.9799817204475403\n",
      "Training log: 9 epoch (11648 / 60000 train. data). Loss: -0.9966016411781311\n",
      "Training log: 9 epoch (12928 / 60000 train. data). Loss: -0.9900494813919067\n",
      "Training log: 9 epoch (14208 / 60000 train. data). Loss: -0.9999992251396179\n",
      "Training log: 9 epoch (15488 / 60000 train. data). Loss: -0.9921825528144836\n",
      "Training log: 9 epoch (16768 / 60000 train. data). Loss: -0.9999208450317383\n",
      "Training log: 9 epoch (18048 / 60000 train. data). Loss: -0.9879873394966125\n",
      "Training log: 9 epoch (19328 / 60000 train. data). Loss: -0.9872336983680725\n",
      "Training log: 9 epoch (20608 / 60000 train. data). Loss: -0.9999932646751404\n",
      "Training log: 9 epoch (21888 / 60000 train. data). Loss: -0.9922960996627808\n",
      "Training log: 9 epoch (23168 / 60000 train. data). Loss: -0.9998916983604431\n",
      "Training log: 9 epoch (24448 / 60000 train. data). Loss: -0.9999933242797852\n",
      "Training log: 9 epoch (25728 / 60000 train. data). Loss: -0.9899517297744751\n",
      "Training log: 9 epoch (27008 / 60000 train. data). Loss: -0.9967292547225952\n",
      "Training log: 9 epoch (28288 / 60000 train. data). Loss: -0.9999467730522156\n",
      "Training log: 9 epoch (29568 / 60000 train. data). Loss: -0.9837685823440552\n",
      "Training log: 9 epoch (30848 / 60000 train. data). Loss: -0.9999998211860657\n",
      "Training log: 9 epoch (32128 / 60000 train. data). Loss: -0.9990398287773132\n",
      "Training log: 9 epoch (33408 / 60000 train. data). Loss: -0.9913573265075684\n",
      "Training log: 9 epoch (34688 / 60000 train. data). Loss: -0.9836662411689758\n",
      "Training log: 9 epoch (35968 / 60000 train. data). Loss: -0.9969691634178162\n",
      "Training log: 9 epoch (37248 / 60000 train. data). Loss: -0.999239981174469\n",
      "Training log: 9 epoch (38528 / 60000 train. data). Loss: -0.9858302474021912\n",
      "Training log: 9 epoch (39808 / 60000 train. data). Loss: -0.9914546012878418\n",
      "Training log: 9 epoch (41088 / 60000 train. data). Loss: -0.9992180466651917\n",
      "Training log: 9 epoch (42368 / 60000 train. data). Loss: -0.9933088421821594\n",
      "Training log: 9 epoch (43648 / 60000 train. data). Loss: -0.995568037033081\n",
      "Training log: 9 epoch (44928 / 60000 train. data). Loss: -0.9919503927230835\n",
      "Training log: 9 epoch (46208 / 60000 train. data). Loss: -0.9996850490570068\n",
      "Training log: 9 epoch (47488 / 60000 train. data). Loss: -0.9842021465301514\n",
      "Training log: 9 epoch (48768 / 60000 train. data). Loss: -0.9921874403953552\n",
      "Training log: 9 epoch (50048 / 60000 train. data). Loss: -0.9917081594467163\n",
      "Training log: 9 epoch (51328 / 60000 train. data). Loss: -0.9991900324821472\n",
      "Training log: 9 epoch (52608 / 60000 train. data). Loss: -0.9819545149803162\n",
      "Training log: 9 epoch (53888 / 60000 train. data). Loss: -0.968418538570404\n",
      "Training log: 9 epoch (55168 / 60000 train. data). Loss: -0.9998733401298523\n",
      "Training log: 9 epoch (56448 / 60000 train. data). Loss: -0.991994321346283\n",
      "Training log: 9 epoch (57728 / 60000 train. data). Loss: -0.9971224069595337\n",
      "Training log: 9 epoch (59008 / 60000 train. data). Loss: -0.9921875\n",
      "Test loss (avg): -0.9865497139930726, Accuracy: 0.9865\n",
      "Training log: 10 epoch (128 / 60000 train. data). Loss: -0.9921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 10 epoch (1408 / 60000 train. data). Loss: -0.9999213218688965\n",
      "Training log: 10 epoch (2688 / 60000 train. data). Loss: -0.9863852858543396\n",
      "Training log: 10 epoch (3968 / 60000 train. data). Loss: -0.9979387521743774\n",
      "Training log: 10 epoch (5248 / 60000 train. data). Loss: -0.9864654541015625\n",
      "Training log: 10 epoch (6528 / 60000 train. data). Loss: -0.9999997615814209\n",
      "Training log: 10 epoch (7808 / 60000 train. data). Loss: -0.987764835357666\n",
      "Training log: 10 epoch (9088 / 60000 train. data). Loss: -0.9849758744239807\n",
      "Training log: 10 epoch (10368 / 60000 train. data). Loss: -0.9993088245391846\n",
      "Training log: 10 epoch (11648 / 60000 train. data). Loss: -0.9886927604675293\n",
      "Training log: 10 epoch (12928 / 60000 train. data). Loss: -0.9956716299057007\n",
      "Training log: 10 epoch (14208 / 60000 train. data). Loss: -0.9999953508377075\n",
      "Training log: 10 epoch (15488 / 60000 train. data). Loss: -0.9999964237213135\n",
      "Training log: 10 epoch (16768 / 60000 train. data). Loss: -0.9922671318054199\n",
      "Training log: 10 epoch (18048 / 60000 train. data). Loss: -0.9852418303489685\n",
      "Training log: 10 epoch (19328 / 60000 train. data). Loss: -0.9921863675117493\n",
      "Training log: 10 epoch (20608 / 60000 train. data). Loss: -0.9992877840995789\n",
      "Training log: 10 epoch (21888 / 60000 train. data). Loss: -0.9999995827674866\n",
      "Training log: 10 epoch (23168 / 60000 train. data). Loss: -0.9998824596405029\n",
      "Training log: 10 epoch (24448 / 60000 train. data). Loss: -0.9842753410339355\n",
      "Training log: 10 epoch (25728 / 60000 train. data). Loss: -0.9849424362182617\n",
      "Training log: 10 epoch (27008 / 60000 train. data). Loss: -0.9856605529785156\n",
      "Training log: 10 epoch (28288 / 60000 train. data). Loss: -0.9999630451202393\n",
      "Training log: 10 epoch (29568 / 60000 train. data). Loss: -0.9921782612800598\n",
      "Training log: 10 epoch (30848 / 60000 train. data). Loss: -0.9998891353607178\n",
      "Training log: 10 epoch (32128 / 60000 train. data). Loss: -0.9894334673881531\n",
      "Training log: 10 epoch (33408 / 60000 train. data). Loss: -0.9934021234512329\n",
      "Training log: 10 epoch (34688 / 60000 train. data). Loss: -0.9922211170196533\n",
      "Training log: 10 epoch (35968 / 60000 train. data). Loss: -0.9999613761901855\n",
      "Training log: 10 epoch (37248 / 60000 train. data). Loss: -0.9999916553497314\n",
      "Training log: 10 epoch (38528 / 60000 train. data). Loss: -0.9845359325408936\n",
      "Training log: 10 epoch (39808 / 60000 train. data). Loss: -0.9763165712356567\n",
      "Training log: 10 epoch (41088 / 60000 train. data). Loss: -0.9840825796127319\n",
      "Training log: 10 epoch (42368 / 60000 train. data). Loss: -0.9937722682952881\n",
      "Training log: 10 epoch (43648 / 60000 train. data). Loss: -0.9929127097129822\n",
      "Training log: 10 epoch (44928 / 60000 train. data). Loss: -0.9999982118606567\n",
      "Training log: 10 epoch (46208 / 60000 train. data). Loss: -0.9999927878379822\n",
      "Training log: 10 epoch (47488 / 60000 train. data). Loss: -0.9921945333480835\n",
      "Training log: 10 epoch (48768 / 60000 train. data). Loss: -0.9999989867210388\n",
      "Training log: 10 epoch (50048 / 60000 train. data). Loss: -0.9921847581863403\n",
      "Training log: 10 epoch (51328 / 60000 train. data). Loss: -0.9899625778198242\n",
      "Training log: 10 epoch (52608 / 60000 train. data). Loss: -0.9949602484703064\n",
      "Training log: 10 epoch (53888 / 60000 train. data). Loss: -0.9921528697013855\n",
      "Training log: 10 epoch (55168 / 60000 train. data). Loss: -0.9921863079071045\n",
      "Training log: 10 epoch (56448 / 60000 train. data). Loss: -0.9918044805526733\n",
      "Training log: 10 epoch (57728 / 60000 train. data). Loss: -0.9842498302459717\n",
      "Training log: 10 epoch (59008 / 60000 train. data). Loss: -0.9993224143981934\n",
      "Test loss (avg): -0.9857587719917298, Accuracy: 0.9866\n",
      "Training log: 11 epoch (128 / 60000 train. data). Loss: -0.9921194314956665\n",
      "Training log: 11 epoch (1408 / 60000 train. data). Loss: -0.9926584362983704\n",
      "Training log: 11 epoch (2688 / 60000 train. data). Loss: -0.9843769669532776\n",
      "Training log: 11 epoch (3968 / 60000 train. data). Loss: -0.9999764561653137\n",
      "Training log: 11 epoch (5248 / 60000 train. data). Loss: -0.9918960332870483\n",
      "Training log: 11 epoch (6528 / 60000 train. data). Loss: -0.9921849966049194\n",
      "Training log: 11 epoch (7808 / 60000 train. data). Loss: -0.9999971985816956\n",
      "Training log: 11 epoch (9088 / 60000 train. data). Loss: -1.0\n",
      "Training log: 11 epoch (10368 / 60000 train. data). Loss: -0.9921918511390686\n",
      "Training log: 11 epoch (11648 / 60000 train. data). Loss: -0.9999995231628418\n",
      "Training log: 11 epoch (12928 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 11 epoch (14208 / 60000 train. data). Loss: -0.9999984502792358\n",
      "Training log: 11 epoch (15488 / 60000 train. data). Loss: -0.9843407273292542\n",
      "Training log: 11 epoch (16768 / 60000 train. data). Loss: -0.9919306635856628\n",
      "Training log: 11 epoch (18048 / 60000 train. data). Loss: -0.9837871193885803\n",
      "Training log: 11 epoch (19328 / 60000 train. data). Loss: -0.992188572883606\n",
      "Training log: 11 epoch (20608 / 60000 train. data). Loss: -0.9984304308891296\n",
      "Training log: 11 epoch (21888 / 60000 train. data). Loss: -0.9999939203262329\n",
      "Training log: 11 epoch (23168 / 60000 train. data). Loss: -0.9843257069587708\n",
      "Training log: 11 epoch (24448 / 60000 train. data). Loss: -0.9843661189079285\n",
      "Training log: 11 epoch (25728 / 60000 train. data). Loss: -0.9962151050567627\n",
      "Training log: 11 epoch (27008 / 60000 train. data). Loss: -0.9765646457672119\n",
      "Training log: 11 epoch (28288 / 60000 train. data). Loss: -0.9999967217445374\n",
      "Training log: 11 epoch (29568 / 60000 train. data). Loss: -0.9999914169311523\n",
      "Training log: 11 epoch (30848 / 60000 train. data). Loss: -0.9773118495941162\n",
      "Training log: 11 epoch (32128 / 60000 train. data). Loss: -0.9849318265914917\n",
      "Training log: 11 epoch (33408 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 11 epoch (34688 / 60000 train. data). Loss: -0.9676623344421387\n",
      "Training log: 11 epoch (35968 / 60000 train. data). Loss: -1.0\n",
      "Training log: 11 epoch (37248 / 60000 train. data). Loss: -0.9999901652336121\n",
      "Training log: 11 epoch (38528 / 60000 train. data). Loss: -0.994937539100647\n",
      "Training log: 11 epoch (39808 / 60000 train. data). Loss: -0.9894322752952576\n",
      "Training log: 11 epoch (41088 / 60000 train. data). Loss: -0.9863632321357727\n",
      "Training log: 11 epoch (42368 / 60000 train. data). Loss: -0.9999971389770508\n",
      "Training log: 11 epoch (43648 / 60000 train. data). Loss: -0.9999608993530273\n",
      "Training log: 11 epoch (44928 / 60000 train. data). Loss: -0.9999980330467224\n",
      "Training log: 11 epoch (46208 / 60000 train. data). Loss: -0.9923309683799744\n",
      "Training log: 11 epoch (47488 / 60000 train. data). Loss: -0.9920079112052917\n",
      "Training log: 11 epoch (48768 / 60000 train. data). Loss: -0.9843580722808838\n",
      "Training log: 11 epoch (50048 / 60000 train. data). Loss: -0.9850085973739624\n",
      "Training log: 11 epoch (51328 / 60000 train. data). Loss: -0.9995473027229309\n",
      "Training log: 11 epoch (52608 / 60000 train. data). Loss: -0.9926519989967346\n",
      "Training log: 11 epoch (53888 / 60000 train. data). Loss: -0.9998828768730164\n",
      "Training log: 11 epoch (55168 / 60000 train. data). Loss: -0.992274820804596\n",
      "Training log: 11 epoch (56448 / 60000 train. data). Loss: -1.0\n",
      "Training log: 11 epoch (57728 / 60000 train. data). Loss: -0.9929859638214111\n",
      "Training log: 11 epoch (59008 / 60000 train. data). Loss: -0.9871665835380554\n",
      "Test loss (avg): -0.9874884323120117, Accuracy: 0.9879\n",
      "Training log: 12 epoch (128 / 60000 train. data). Loss: -0.987128734588623\n",
      "Training log: 12 epoch (1408 / 60000 train. data). Loss: -0.992901086807251\n",
      "Training log: 12 epoch (2688 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 12 epoch (3968 / 60000 train. data). Loss: -0.991826057434082\n",
      "Training log: 12 epoch (5248 / 60000 train. data). Loss: -0.992194414138794\n",
      "Training log: 12 epoch (6528 / 60000 train. data). Loss: -0.9998174905776978\n",
      "Training log: 12 epoch (7808 / 60000 train. data). Loss: -0.9956674575805664\n",
      "Training log: 12 epoch (9088 / 60000 train. data). Loss: -0.9998131990432739\n",
      "Training log: 12 epoch (10368 / 60000 train. data). Loss: -0.9999977350234985\n",
      "Training log: 12 epoch (11648 / 60000 train. data). Loss: -0.999995768070221\n",
      "Training log: 12 epoch (12928 / 60000 train. data). Loss: -0.9999022483825684\n",
      "Training log: 12 epoch (14208 / 60000 train. data). Loss: -0.9873262047767639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 12 epoch (15488 / 60000 train. data). Loss: -0.992184042930603\n",
      "Training log: 12 epoch (16768 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 12 epoch (18048 / 60000 train. data). Loss: -0.987550675868988\n",
      "Training log: 12 epoch (19328 / 60000 train. data). Loss: -0.992163360118866\n",
      "Training log: 12 epoch (20608 / 60000 train. data). Loss: -0.9999972581863403\n",
      "Training log: 12 epoch (21888 / 60000 train. data). Loss: -0.9999996423721313\n",
      "Training log: 12 epoch (23168 / 60000 train. data). Loss: -0.9949806928634644\n",
      "Training log: 12 epoch (24448 / 60000 train. data). Loss: -0.9994202852249146\n",
      "Training log: 12 epoch (25728 / 60000 train. data). Loss: -0.9946629405021667\n",
      "Training log: 12 epoch (27008 / 60000 train. data). Loss: -0.9998666048049927\n",
      "Training log: 12 epoch (28288 / 60000 train. data). Loss: -0.999575138092041\n",
      "Training log: 12 epoch (29568 / 60000 train. data). Loss: -0.9921720623970032\n",
      "Training log: 12 epoch (30848 / 60000 train. data). Loss: -0.9992160797119141\n",
      "Training log: 12 epoch (32128 / 60000 train. data). Loss: -0.9959568977355957\n",
      "Training log: 12 epoch (33408 / 60000 train. data). Loss: -0.9999764561653137\n",
      "Training log: 12 epoch (34688 / 60000 train. data). Loss: -0.9852872490882874\n",
      "Training log: 12 epoch (35968 / 60000 train. data). Loss: -0.9917960166931152\n",
      "Training log: 12 epoch (37248 / 60000 train. data). Loss: -0.999974250793457\n",
      "Training log: 12 epoch (38528 / 60000 train. data). Loss: -0.9922933578491211\n",
      "Training log: 12 epoch (39808 / 60000 train. data). Loss: -0.9999976754188538\n",
      "Training log: 12 epoch (41088 / 60000 train. data). Loss: -0.9872238636016846\n",
      "Training log: 12 epoch (42368 / 60000 train. data). Loss: -0.9999982714653015\n",
      "Training log: 12 epoch (43648 / 60000 train. data). Loss: -0.9925135970115662\n",
      "Training log: 12 epoch (44928 / 60000 train. data). Loss: -0.9963418245315552\n",
      "Training log: 12 epoch (46208 / 60000 train. data). Loss: -0.99388188123703\n",
      "Training log: 12 epoch (47488 / 60000 train. data). Loss: -0.9936771392822266\n",
      "Training log: 12 epoch (48768 / 60000 train. data). Loss: -0.9919854402542114\n",
      "Training log: 12 epoch (50048 / 60000 train. data). Loss: -0.992198646068573\n",
      "Training log: 12 epoch (51328 / 60000 train. data). Loss: -0.9999909400939941\n",
      "Training log: 12 epoch (52608 / 60000 train. data). Loss: -0.9917914867401123\n",
      "Training log: 12 epoch (53888 / 60000 train. data). Loss: -0.9999988079071045\n",
      "Training log: 12 epoch (55168 / 60000 train. data). Loss: -0.9921897053718567\n",
      "Training log: 12 epoch (56448 / 60000 train. data). Loss: -0.9697875380516052\n",
      "Training log: 12 epoch (57728 / 60000 train. data). Loss: -0.9999523758888245\n",
      "Training log: 12 epoch (59008 / 60000 train. data). Loss: -0.9765587449073792\n",
      "Test loss (avg): -0.9867130447387695, Accuracy: 0.9867\n",
      "Training log: 13 epoch (128 / 60000 train. data). Loss: -0.9921485185623169\n",
      "Training log: 13 epoch (1408 / 60000 train. data). Loss: -0.9998195767402649\n",
      "Training log: 13 epoch (2688 / 60000 train. data). Loss: -0.9926350712776184\n",
      "Training log: 13 epoch (3968 / 60000 train. data). Loss: -0.9844207167625427\n",
      "Training log: 13 epoch (5248 / 60000 train. data). Loss: -0.9921866059303284\n",
      "Training log: 13 epoch (6528 / 60000 train. data). Loss: -0.9999746084213257\n",
      "Training log: 13 epoch (7808 / 60000 train. data). Loss: -0.9981364011764526\n",
      "Training log: 13 epoch (9088 / 60000 train. data). Loss: -0.9998294711112976\n",
      "Training log: 13 epoch (10368 / 60000 train. data). Loss: -0.9998512864112854\n",
      "Training log: 13 epoch (11648 / 60000 train. data). Loss: -0.9927470684051514\n",
      "Training log: 13 epoch (12928 / 60000 train. data). Loss: -0.9874653816223145\n",
      "Training log: 13 epoch (14208 / 60000 train. data). Loss: -0.9999970197677612\n",
      "Training log: 13 epoch (15488 / 60000 train. data). Loss: -0.9997862577438354\n",
      "Training log: 13 epoch (16768 / 60000 train. data). Loss: -0.997449517250061\n",
      "Training log: 13 epoch (18048 / 60000 train. data). Loss: -0.9940844774246216\n",
      "Training log: 13 epoch (19328 / 60000 train. data). Loss: -0.9995048642158508\n",
      "Training log: 13 epoch (20608 / 60000 train. data). Loss: -0.9918892979621887\n",
      "Training log: 13 epoch (21888 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 13 epoch (23168 / 60000 train. data). Loss: -0.9921259880065918\n",
      "Training log: 13 epoch (24448 / 60000 train. data). Loss: -0.9758374094963074\n",
      "Training log: 13 epoch (25728 / 60000 train. data). Loss: -0.9998146891593933\n",
      "Training log: 13 epoch (27008 / 60000 train. data). Loss: -0.9765983819961548\n",
      "Training log: 13 epoch (28288 / 60000 train. data). Loss: -0.9936977624893188\n",
      "Training log: 13 epoch (29568 / 60000 train. data). Loss: -0.9918093681335449\n",
      "Training log: 13 epoch (30848 / 60000 train. data). Loss: -0.9974863529205322\n",
      "Training log: 13 epoch (32128 / 60000 train. data). Loss: -0.9995446801185608\n",
      "Training log: 13 epoch (33408 / 60000 train. data). Loss: -0.9921867847442627\n",
      "Training log: 13 epoch (34688 / 60000 train. data). Loss: -0.9840074777603149\n",
      "Training log: 13 epoch (35968 / 60000 train. data). Loss: -0.9922125339508057\n",
      "Training log: 13 epoch (37248 / 60000 train. data). Loss: -0.9999975562095642\n",
      "Training log: 13 epoch (38528 / 60000 train. data). Loss: -0.9999983310699463\n",
      "Training log: 13 epoch (39808 / 60000 train. data). Loss: -0.9997406005859375\n",
      "Training log: 13 epoch (41088 / 60000 train. data). Loss: -0.9933823347091675\n",
      "Training log: 13 epoch (42368 / 60000 train. data). Loss: -0.9921873211860657\n",
      "Training log: 13 epoch (43648 / 60000 train. data). Loss: -1.0\n",
      "Training log: 13 epoch (44928 / 60000 train. data). Loss: -1.0\n",
      "Training log: 13 epoch (46208 / 60000 train. data). Loss: -0.999988853931427\n",
      "Training log: 13 epoch (47488 / 60000 train. data). Loss: -0.9908789992332458\n",
      "Training log: 13 epoch (48768 / 60000 train. data). Loss: -0.9974298477172852\n",
      "Training log: 13 epoch (50048 / 60000 train. data). Loss: -0.9923006296157837\n",
      "Training log: 13 epoch (51328 / 60000 train. data). Loss: -1.0\n",
      "Training log: 13 epoch (52608 / 60000 train. data). Loss: -0.9999805092811584\n",
      "Training log: 13 epoch (53888 / 60000 train. data). Loss: -0.9999082684516907\n",
      "Training log: 13 epoch (55168 / 60000 train. data). Loss: -0.9921712875366211\n",
      "Training log: 13 epoch (56448 / 60000 train. data). Loss: -0.9999963641166687\n",
      "Training log: 13 epoch (57728 / 60000 train. data). Loss: -0.9940574169158936\n",
      "Training log: 13 epoch (59008 / 60000 train. data). Loss: -0.9941864013671875\n",
      "Test loss (avg): -0.9876458847045898, Accuracy: 0.9878\n",
      "Training log: 14 epoch (128 / 60000 train. data). Loss: -0.9994543194770813\n",
      "Training log: 14 epoch (1408 / 60000 train. data). Loss: -0.9998850226402283\n",
      "Training log: 14 epoch (2688 / 60000 train. data). Loss: -0.9981760382652283\n",
      "Training log: 14 epoch (3968 / 60000 train. data). Loss: -0.9921844601631165\n",
      "Training log: 14 epoch (5248 / 60000 train. data). Loss: -0.9999996423721313\n",
      "Training log: 14 epoch (6528 / 60000 train. data). Loss: -0.9900503158569336\n",
      "Training log: 14 epoch (7808 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 14 epoch (9088 / 60000 train. data). Loss: -0.9999997019767761\n",
      "Training log: 14 epoch (10368 / 60000 train. data). Loss: -0.985831618309021\n",
      "Training log: 14 epoch (11648 / 60000 train. data). Loss: -0.9922117590904236\n",
      "Training log: 14 epoch (12928 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 14 epoch (14208 / 60000 train. data). Loss: -0.9943659901618958\n",
      "Training log: 14 epoch (15488 / 60000 train. data). Loss: -1.0\n",
      "Training log: 14 epoch (16768 / 60000 train. data). Loss: -0.9843775629997253\n",
      "Training log: 14 epoch (18048 / 60000 train. data). Loss: -0.9921902418136597\n",
      "Training log: 14 epoch (19328 / 60000 train. data). Loss: -0.9843690395355225\n",
      "Training log: 14 epoch (20608 / 60000 train. data). Loss: -1.0\n",
      "Training log: 14 epoch (21888 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 14 epoch (23168 / 60000 train. data). Loss: -0.9892480969429016\n",
      "Training log: 14 epoch (24448 / 60000 train. data). Loss: -0.9997391104698181\n",
      "Training log: 14 epoch (25728 / 60000 train. data). Loss: -1.0\n",
      "Training log: 14 epoch (27008 / 60000 train. data). Loss: -0.9999764561653137\n",
      "Training log: 14 epoch (28288 / 60000 train. data). Loss: -0.9998452067375183\n",
      "Training log: 14 epoch (29568 / 60000 train. data). Loss: -0.9983335733413696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 14 epoch (30848 / 60000 train. data). Loss: -0.9998567700386047\n",
      "Training log: 14 epoch (32128 / 60000 train. data). Loss: -0.9982353448867798\n",
      "Training log: 14 epoch (33408 / 60000 train. data). Loss: -0.9921695590019226\n",
      "Training log: 14 epoch (34688 / 60000 train. data). Loss: -0.9999994039535522\n",
      "Training log: 14 epoch (35968 / 60000 train. data). Loss: -0.9766167998313904\n",
      "Training log: 14 epoch (37248 / 60000 train. data). Loss: -0.984373927116394\n",
      "Training log: 14 epoch (38528 / 60000 train. data). Loss: -0.9921453595161438\n",
      "Training log: 14 epoch (39808 / 60000 train. data). Loss: -0.9999989867210388\n",
      "Training log: 14 epoch (41088 / 60000 train. data). Loss: -0.9845402836799622\n",
      "Training log: 14 epoch (42368 / 60000 train. data). Loss: -0.9957717657089233\n",
      "Training log: 14 epoch (43648 / 60000 train. data). Loss: -0.9847620725631714\n",
      "Training log: 14 epoch (44928 / 60000 train. data). Loss: -0.9876826405525208\n",
      "Training log: 14 epoch (46208 / 60000 train. data). Loss: -0.9921799302101135\n",
      "Training log: 14 epoch (47488 / 60000 train. data). Loss: -0.9999989867210388\n",
      "Training log: 14 epoch (48768 / 60000 train. data). Loss: -0.9843748807907104\n",
      "Training log: 14 epoch (50048 / 60000 train. data). Loss: -1.0\n",
      "Training log: 14 epoch (51328 / 60000 train. data). Loss: -0.9914206862449646\n",
      "Training log: 14 epoch (52608 / 60000 train. data). Loss: -0.9845716953277588\n",
      "Training log: 14 epoch (53888 / 60000 train. data). Loss: -0.9999676942825317\n",
      "Training log: 14 epoch (55168 / 60000 train. data). Loss: -0.9922106266021729\n",
      "Training log: 14 epoch (56448 / 60000 train. data). Loss: -0.9875568151473999\n",
      "Training log: 14 epoch (57728 / 60000 train. data). Loss: -0.9995419979095459\n",
      "Training log: 14 epoch (59008 / 60000 train. data). Loss: -0.9998211860656738\n",
      "Test loss (avg): -0.9861761192321777, Accuracy: 0.9861\n",
      "Training log: 15 epoch (128 / 60000 train. data). Loss: -0.9999221563339233\n",
      "Training log: 15 epoch (1408 / 60000 train. data). Loss: -0.9999762773513794\n",
      "Training log: 15 epoch (2688 / 60000 train. data). Loss: -0.9843317866325378\n",
      "Training log: 15 epoch (3968 / 60000 train. data). Loss: -0.9999951720237732\n",
      "Training log: 15 epoch (5248 / 60000 train. data). Loss: -0.9966747164726257\n",
      "Training log: 15 epoch (6528 / 60000 train. data). Loss: -0.9994704723358154\n",
      "Training log: 15 epoch (7808 / 60000 train. data). Loss: -1.0\n",
      "Training log: 15 epoch (9088 / 60000 train. data). Loss: -0.9921874403953552\n",
      "Training log: 15 epoch (10368 / 60000 train. data). Loss: -0.9921777248382568\n",
      "Training log: 15 epoch (11648 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 15 epoch (12928 / 60000 train. data). Loss: -0.9999368786811829\n",
      "Training log: 15 epoch (14208 / 60000 train. data). Loss: -0.9996955394744873\n",
      "Training log: 15 epoch (15488 / 60000 train. data). Loss: -0.9970278739929199\n",
      "Training log: 15 epoch (16768 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 15 epoch (18048 / 60000 train. data). Loss: -0.984375\n",
      "Training log: 15 epoch (19328 / 60000 train. data). Loss: -0.9922163486480713\n",
      "Training log: 15 epoch (20608 / 60000 train. data). Loss: -1.0\n",
      "Training log: 15 epoch (21888 / 60000 train. data). Loss: -0.9917343258857727\n",
      "Training log: 15 epoch (23168 / 60000 train. data). Loss: -0.9921970367431641\n",
      "Training log: 15 epoch (24448 / 60000 train. data). Loss: -0.9999952912330627\n",
      "Training log: 15 epoch (25728 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 15 epoch (27008 / 60000 train. data). Loss: -0.9921874403953552\n",
      "Training log: 15 epoch (28288 / 60000 train. data). Loss: -0.9999988079071045\n",
      "Training log: 15 epoch (29568 / 60000 train. data). Loss: -0.999976634979248\n",
      "Training log: 15 epoch (30848 / 60000 train. data). Loss: -0.9925562739372253\n",
      "Training log: 15 epoch (32128 / 60000 train. data). Loss: -0.9921565055847168\n",
      "Training log: 15 epoch (33408 / 60000 train. data). Loss: -1.0\n",
      "Training log: 15 epoch (34688 / 60000 train. data). Loss: -1.0\n",
      "Training log: 15 epoch (35968 / 60000 train. data). Loss: -0.9998928904533386\n",
      "Training log: 15 epoch (37248 / 60000 train. data). Loss: -0.9921868443489075\n",
      "Training log: 15 epoch (38528 / 60000 train. data). Loss: -1.0\n",
      "Training log: 15 epoch (39808 / 60000 train. data). Loss: -0.9834995865821838\n",
      "Training log: 15 epoch (41088 / 60000 train. data). Loss: -0.9919071197509766\n",
      "Training log: 15 epoch (42368 / 60000 train. data). Loss: -0.9944140315055847\n",
      "Training log: 15 epoch (43648 / 60000 train. data). Loss: -0.9947408437728882\n",
      "Training log: 15 epoch (44928 / 60000 train. data). Loss: -0.999515175819397\n",
      "Training log: 15 epoch (46208 / 60000 train. data). Loss: -0.9999995231628418\n",
      "Training log: 15 epoch (47488 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 15 epoch (48768 / 60000 train. data). Loss: -0.984535813331604\n",
      "Training log: 15 epoch (50048 / 60000 train. data). Loss: -0.9918374419212341\n",
      "Training log: 15 epoch (51328 / 60000 train. data). Loss: -0.9923430681228638\n",
      "Training log: 15 epoch (52608 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 15 epoch (53888 / 60000 train. data). Loss: -0.9921914935112\n",
      "Training log: 15 epoch (55168 / 60000 train. data). Loss: -0.9999611973762512\n",
      "Training log: 15 epoch (56448 / 60000 train. data). Loss: -0.9999988675117493\n",
      "Training log: 15 epoch (57728 / 60000 train. data). Loss: -0.9862704873085022\n",
      "Training log: 15 epoch (59008 / 60000 train. data). Loss: -0.9915724992752075\n",
      "Test loss (avg): -0.9874426963806152, Accuracy: 0.9877\n",
      "Training log: 16 epoch (128 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 16 epoch (1408 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (2688 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (3968 / 60000 train. data). Loss: -0.9994693994522095\n",
      "Training log: 16 epoch (5248 / 60000 train. data). Loss: -0.9917147755622864\n",
      "Training log: 16 epoch (6528 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 16 epoch (7808 / 60000 train. data). Loss: -0.9994409084320068\n",
      "Training log: 16 epoch (9088 / 60000 train. data). Loss: -0.995011568069458\n",
      "Training log: 16 epoch (10368 / 60000 train. data). Loss: -0.9850015044212341\n",
      "Training log: 16 epoch (11648 / 60000 train. data). Loss: -0.9831671118736267\n",
      "Training log: 16 epoch (12928 / 60000 train. data). Loss: -0.9999997019767761\n",
      "Training log: 16 epoch (14208 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (15488 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (16768 / 60000 train. data). Loss: -0.9921754002571106\n",
      "Training log: 16 epoch (18048 / 60000 train. data). Loss: -0.9905498623847961\n",
      "Training log: 16 epoch (19328 / 60000 train. data). Loss: -0.9843659996986389\n",
      "Training log: 16 epoch (20608 / 60000 train. data). Loss: -0.9999584555625916\n",
      "Training log: 16 epoch (21888 / 60000 train. data). Loss: -0.9999963045120239\n",
      "Training log: 16 epoch (23168 / 60000 train. data). Loss: -0.9999716281890869\n",
      "Training log: 16 epoch (24448 / 60000 train. data). Loss: -0.9999998211860657\n",
      "Training log: 16 epoch (25728 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (27008 / 60000 train. data). Loss: -0.9921859502792358\n",
      "Training log: 16 epoch (28288 / 60000 train. data). Loss: -0.9832189083099365\n",
      "Training log: 16 epoch (29568 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 16 epoch (30848 / 60000 train. data). Loss: -0.9921813011169434\n",
      "Training log: 16 epoch (32128 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 16 epoch (33408 / 60000 train. data). Loss: -0.9951990246772766\n",
      "Training log: 16 epoch (34688 / 60000 train. data). Loss: -0.9921872615814209\n",
      "Training log: 16 epoch (35968 / 60000 train. data). Loss: -0.9994245767593384\n",
      "Training log: 16 epoch (37248 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (38528 / 60000 train. data). Loss: -0.992493748664856\n",
      "Training log: 16 epoch (39808 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (41088 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 16 epoch (42368 / 60000 train. data). Loss: -0.99150550365448\n",
      "Training log: 16 epoch (43648 / 60000 train. data). Loss: -0.9986419081687927\n",
      "Training log: 16 epoch (44928 / 60000 train. data). Loss: -0.9999926090240479\n",
      "Training log: 16 epoch (46208 / 60000 train. data). Loss: -0.9996280074119568\n",
      "Training log: 16 epoch (47488 / 60000 train. data). Loss: -0.9843659400939941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 16 epoch (48768 / 60000 train. data). Loss: -0.9999990463256836\n",
      "Training log: 16 epoch (50048 / 60000 train. data). Loss: -0.9910332560539246\n",
      "Training log: 16 epoch (51328 / 60000 train. data). Loss: -0.9999991655349731\n",
      "Training log: 16 epoch (52608 / 60000 train. data). Loss: -0.999287486076355\n",
      "Training log: 16 epoch (53888 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 16 epoch (55168 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (56448 / 60000 train. data). Loss: -0.9943706393241882\n",
      "Training log: 16 epoch (57728 / 60000 train. data). Loss: -1.0\n",
      "Training log: 16 epoch (59008 / 60000 train. data). Loss: -0.9946767091751099\n",
      "Test loss (avg): -0.9869108078956604, Accuracy: 0.9871\n",
      "Training log: 17 epoch (128 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 17 epoch (1408 / 60000 train. data). Loss: -0.9843704104423523\n",
      "Training log: 17 epoch (2688 / 60000 train. data). Loss: -0.9843763709068298\n",
      "Training log: 17 epoch (3968 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 17 epoch (5248 / 60000 train. data). Loss: -0.9997411966323853\n",
      "Training log: 17 epoch (6528 / 60000 train. data). Loss: -0.9923576712608337\n",
      "Training log: 17 epoch (7808 / 60000 train. data). Loss: -0.9999997615814209\n",
      "Training log: 17 epoch (9088 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 17 epoch (10368 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 17 epoch (11648 / 60000 train. data). Loss: -0.9844251871109009\n",
      "Training log: 17 epoch (12928 / 60000 train. data). Loss: -0.9999517202377319\n",
      "Training log: 17 epoch (14208 / 60000 train. data). Loss: -0.9921873807907104\n",
      "Training log: 17 epoch (15488 / 60000 train. data). Loss: -0.9921873807907104\n",
      "Training log: 17 epoch (16768 / 60000 train. data). Loss: -1.0\n",
      "Training log: 17 epoch (18048 / 60000 train. data). Loss: -0.9919462203979492\n",
      "Training log: 17 epoch (19328 / 60000 train. data). Loss: -0.9937939643859863\n",
      "Training log: 17 epoch (20608 / 60000 train. data). Loss: -0.9845818281173706\n",
      "Training log: 17 epoch (21888 / 60000 train. data). Loss: -0.99998939037323\n",
      "Training log: 17 epoch (23168 / 60000 train. data). Loss: -1.0\n",
      "Training log: 17 epoch (24448 / 60000 train. data). Loss: -0.9999989867210388\n",
      "Training log: 17 epoch (25728 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Training log: 17 epoch (27008 / 60000 train. data). Loss: -0.9911003112792969\n",
      "Training log: 17 epoch (28288 / 60000 train. data). Loss: -0.9999763369560242\n",
      "Training log: 17 epoch (29568 / 60000 train. data). Loss: -0.9921850562095642\n",
      "Training log: 17 epoch (30848 / 60000 train. data). Loss: -0.9921765923500061\n",
      "Training log: 17 epoch (32128 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 17 epoch (33408 / 60000 train. data). Loss: -0.9761284589767456\n",
      "Training log: 17 epoch (34688 / 60000 train. data). Loss: -1.0\n",
      "Training log: 17 epoch (35968 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 17 epoch (37248 / 60000 train. data). Loss: -1.0\n",
      "Training log: 17 epoch (38528 / 60000 train. data). Loss: -0.992255449295044\n",
      "Training log: 17 epoch (39808 / 60000 train. data). Loss: -1.0\n",
      "Training log: 17 epoch (41088 / 60000 train. data). Loss: -0.9839255213737488\n",
      "Training log: 17 epoch (42368 / 60000 train. data). Loss: -0.9979174733161926\n",
      "Training log: 17 epoch (43648 / 60000 train. data). Loss: -0.9921596646308899\n",
      "Training log: 17 epoch (44928 / 60000 train. data). Loss: -0.9806532859802246\n",
      "Training log: 17 epoch (46208 / 60000 train. data). Loss: -0.9844906330108643\n",
      "Training log: 17 epoch (47488 / 60000 train. data). Loss: -0.999549150466919\n",
      "Training log: 17 epoch (48768 / 60000 train. data). Loss: -0.9999266862869263\n",
      "Training log: 17 epoch (50048 / 60000 train. data). Loss: -0.9999952912330627\n",
      "Training log: 17 epoch (51328 / 60000 train. data). Loss: -0.99990314245224\n",
      "Training log: 17 epoch (52608 / 60000 train. data). Loss: -0.9999966621398926\n",
      "Training log: 17 epoch (53888 / 60000 train. data). Loss: -0.9999957084655762\n",
      "Training log: 17 epoch (55168 / 60000 train. data). Loss: -1.0\n",
      "Training log: 17 epoch (56448 / 60000 train. data). Loss: -0.9867949485778809\n",
      "Training log: 17 epoch (57728 / 60000 train. data). Loss: -0.9999868869781494\n",
      "Training log: 17 epoch (59008 / 60000 train. data). Loss: -0.9954881072044373\n",
      "Test loss (avg): -0.9846950593948365, Accuracy: 0.9845\n",
      "Training log: 18 epoch (128 / 60000 train. data). Loss: -0.9877071380615234\n",
      "Training log: 18 epoch (1408 / 60000 train. data). Loss: -0.9999994039535522\n",
      "Training log: 18 epoch (2688 / 60000 train. data). Loss: -0.984375\n",
      "Training log: 18 epoch (3968 / 60000 train. data). Loss: -0.9999915957450867\n",
      "Training log: 18 epoch (5248 / 60000 train. data). Loss: -0.9924959540367126\n",
      "Training log: 18 epoch (6528 / 60000 train. data). Loss: -0.9958372712135315\n",
      "Training log: 18 epoch (7808 / 60000 train. data). Loss: -0.9999977946281433\n",
      "Training log: 18 epoch (9088 / 60000 train. data). Loss: -0.9867678880691528\n",
      "Training log: 18 epoch (10368 / 60000 train. data). Loss: -0.990134596824646\n",
      "Training log: 18 epoch (11648 / 60000 train. data). Loss: -1.0\n",
      "Training log: 18 epoch (12928 / 60000 train. data). Loss: -0.9721324443817139\n",
      "Training log: 18 epoch (14208 / 60000 train. data). Loss: -0.999998927116394\n",
      "Training log: 18 epoch (15488 / 60000 train. data). Loss: -0.9999997615814209\n",
      "Training log: 18 epoch (16768 / 60000 train. data). Loss: -0.9832742810249329\n",
      "Training log: 18 epoch (18048 / 60000 train. data). Loss: -0.9904203414916992\n",
      "Training log: 18 epoch (19328 / 60000 train. data). Loss: -0.9921973943710327\n",
      "Training log: 18 epoch (20608 / 60000 train. data). Loss: -0.9921337366104126\n",
      "Training log: 18 epoch (21888 / 60000 train. data). Loss: -0.9919941425323486\n",
      "Training log: 18 epoch (23168 / 60000 train. data). Loss: -1.0\n",
      "Training log: 18 epoch (24448 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 18 epoch (25728 / 60000 train. data). Loss: -0.9843924641609192\n",
      "Training log: 18 epoch (27008 / 60000 train. data). Loss: -0.9994255900382996\n",
      "Training log: 18 epoch (28288 / 60000 train. data). Loss: -0.9955349564552307\n",
      "Training log: 18 epoch (29568 / 60000 train. data). Loss: -0.9986529350280762\n",
      "Training log: 18 epoch (30848 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 18 epoch (32128 / 60000 train. data). Loss: -0.9921855330467224\n",
      "Training log: 18 epoch (33408 / 60000 train. data). Loss: -1.0\n",
      "Training log: 18 epoch (34688 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 18 epoch (35968 / 60000 train. data). Loss: -0.9995437264442444\n",
      "Training log: 18 epoch (37248 / 60000 train. data). Loss: -0.9843773245811462\n",
      "Training log: 18 epoch (38528 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 18 epoch (39808 / 60000 train. data). Loss: -0.9937126636505127\n",
      "Training log: 18 epoch (41088 / 60000 train. data). Loss: -1.0\n",
      "Training log: 18 epoch (42368 / 60000 train. data). Loss: -1.0\n",
      "Training log: 18 epoch (43648 / 60000 train. data). Loss: -0.9936838746070862\n",
      "Training log: 18 epoch (44928 / 60000 train. data). Loss: -0.9919995069503784\n",
      "Training log: 18 epoch (46208 / 60000 train. data). Loss: -0.9999998211860657\n",
      "Training log: 18 epoch (47488 / 60000 train. data). Loss: -0.990797221660614\n",
      "Training log: 18 epoch (48768 / 60000 train. data). Loss: -0.9956678152084351\n",
      "Training log: 18 epoch (50048 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 18 epoch (51328 / 60000 train. data). Loss: -1.0\n",
      "Training log: 18 epoch (52608 / 60000 train. data). Loss: -0.9765874147415161\n",
      "Training log: 18 epoch (53888 / 60000 train. data). Loss: -0.9999997019767761\n",
      "Training log: 18 epoch (55168 / 60000 train. data). Loss: -0.9999998211860657\n",
      "Training log: 18 epoch (56448 / 60000 train. data). Loss: -1.0\n",
      "Training log: 18 epoch (57728 / 60000 train. data). Loss: -0.9935940504074097\n",
      "Training log: 18 epoch (59008 / 60000 train. data). Loss: -0.9999997019767761\n",
      "Test loss (avg): -0.9866805681228638, Accuracy: 0.9869\n",
      "Training log: 19 epoch (128 / 60000 train. data). Loss: -0.9993153214454651\n",
      "Training log: 19 epoch (1408 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 19 epoch (2688 / 60000 train. data). Loss: -0.9843873977661133\n",
      "Training log: 19 epoch (3968 / 60000 train. data). Loss: -0.9917023777961731\n",
      "Training log: 19 epoch (5248 / 60000 train. data). Loss: -0.9843769073486328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 19 epoch (6528 / 60000 train. data). Loss: -0.992938220500946\n",
      "Training log: 19 epoch (7808 / 60000 train. data). Loss: -0.9962620735168457\n",
      "Training log: 19 epoch (9088 / 60000 train. data). Loss: -0.984375\n",
      "Training log: 19 epoch (10368 / 60000 train. data). Loss: -0.9999848008155823\n",
      "Training log: 19 epoch (11648 / 60000 train. data). Loss: -0.9999990463256836\n",
      "Training log: 19 epoch (12928 / 60000 train. data). Loss: -0.9921814203262329\n",
      "Training log: 19 epoch (14208 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (15488 / 60000 train. data). Loss: -0.9843754768371582\n",
      "Training log: 19 epoch (16768 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (18048 / 60000 train. data). Loss: -0.984375\n",
      "Training log: 19 epoch (19328 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (20608 / 60000 train. data). Loss: -0.9996657371520996\n",
      "Training log: 19 epoch (21888 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (23168 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (24448 / 60000 train. data). Loss: -0.999981701374054\n",
      "Training log: 19 epoch (25728 / 60000 train. data). Loss: -0.9938229322433472\n",
      "Training log: 19 epoch (27008 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (28288 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 19 epoch (29568 / 60000 train. data). Loss: -0.9921647906303406\n",
      "Training log: 19 epoch (30848 / 60000 train. data). Loss: -0.9834221601486206\n",
      "Training log: 19 epoch (32128 / 60000 train. data). Loss: -0.9921872019767761\n",
      "Training log: 19 epoch (33408 / 60000 train. data). Loss: -0.9952860474586487\n",
      "Training log: 19 epoch (34688 / 60000 train. data). Loss: -0.9921848773956299\n",
      "Training log: 19 epoch (35968 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 19 epoch (37248 / 60000 train. data). Loss: -0.9999984502792358\n",
      "Training log: 19 epoch (38528 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (39808 / 60000 train. data). Loss: -0.9921870231628418\n",
      "Training log: 19 epoch (41088 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 19 epoch (42368 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (43648 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (44928 / 60000 train. data). Loss: -0.9999725818634033\n",
      "Training log: 19 epoch (46208 / 60000 train. data). Loss: -0.9999985694885254\n",
      "Training log: 19 epoch (47488 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 19 epoch (48768 / 60000 train. data). Loss: -0.9844347238540649\n",
      "Training log: 19 epoch (50048 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (51328 / 60000 train. data). Loss: -1.0\n",
      "Training log: 19 epoch (52608 / 60000 train. data). Loss: -0.9945923089981079\n",
      "Training log: 19 epoch (53888 / 60000 train. data). Loss: -0.9997832775115967\n",
      "Training log: 19 epoch (55168 / 60000 train. data). Loss: -0.9921994209289551\n",
      "Training log: 19 epoch (56448 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 19 epoch (57728 / 60000 train. data). Loss: -0.9925025105476379\n",
      "Training log: 19 epoch (59008 / 60000 train. data). Loss: -0.9999999403953552\n",
      "Test loss (avg): -0.9893469528198242, Accuracy: 0.9897\n",
      "Training log: 20 epoch (128 / 60000 train. data). Loss: -0.9999996423721313\n",
      "Training log: 20 epoch (1408 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (2688 / 60000 train. data). Loss: -0.9999409914016724\n",
      "Training log: 20 epoch (3968 / 60000 train. data). Loss: -0.9928542971611023\n",
      "Training log: 20 epoch (5248 / 60000 train. data). Loss: -0.9999967217445374\n",
      "Training log: 20 epoch (6528 / 60000 train. data). Loss: -0.9921693205833435\n",
      "Training log: 20 epoch (7808 / 60000 train. data). Loss: -0.9921882152557373\n",
      "Training log: 20 epoch (9088 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (10368 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (11648 / 60000 train. data). Loss: -0.9924201965332031\n",
      "Training log: 20 epoch (12928 / 60000 train. data). Loss: -0.999567449092865\n",
      "Training log: 20 epoch (14208 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (15488 / 60000 train. data). Loss: -0.9999987483024597\n",
      "Training log: 20 epoch (16768 / 60000 train. data). Loss: -0.9924717545509338\n",
      "Training log: 20 epoch (18048 / 60000 train. data). Loss: -0.9999997615814209\n",
      "Training log: 20 epoch (19328 / 60000 train. data). Loss: -0.984375\n",
      "Training log: 20 epoch (20608 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (21888 / 60000 train. data). Loss: -0.9999232292175293\n",
      "Training log: 20 epoch (23168 / 60000 train. data). Loss: -0.9971822500228882\n",
      "Training log: 20 epoch (24448 / 60000 train. data). Loss: -0.9887433052062988\n",
      "Training log: 20 epoch (25728 / 60000 train. data). Loss: -0.9999966621398926\n",
      "Training log: 20 epoch (27008 / 60000 train. data). Loss: -0.9999997019767761\n",
      "Training log: 20 epoch (28288 / 60000 train. data). Loss: -0.9999979734420776\n",
      "Training log: 20 epoch (29568 / 60000 train. data). Loss: -0.9921658635139465\n",
      "Training log: 20 epoch (30848 / 60000 train. data). Loss: -0.9861971139907837\n",
      "Training log: 20 epoch (32128 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 20 epoch (33408 / 60000 train. data). Loss: -0.9921875\n",
      "Training log: 20 epoch (34688 / 60000 train. data). Loss: -0.9999502301216125\n",
      "Training log: 20 epoch (35968 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (37248 / 60000 train. data). Loss: -0.9915109276771545\n",
      "Training log: 20 epoch (38528 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (39808 / 60000 train. data). Loss: -0.9999978542327881\n",
      "Training log: 20 epoch (41088 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (42368 / 60000 train. data). Loss: -0.9999998807907104\n",
      "Training log: 20 epoch (43648 / 60000 train. data). Loss: -0.999999463558197\n",
      "Training log: 20 epoch (44928 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (46208 / 60000 train. data). Loss: -0.9843748807907104\n",
      "Training log: 20 epoch (47488 / 60000 train. data). Loss: -0.9935082793235779\n",
      "Training log: 20 epoch (48768 / 60000 train. data). Loss: -0.9999995827674866\n",
      "Training log: 20 epoch (50048 / 60000 train. data). Loss: -0.9899557828903198\n",
      "Training log: 20 epoch (51328 / 60000 train. data). Loss: -0.9999997615814209\n",
      "Training log: 20 epoch (52608 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (53888 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (55168 / 60000 train. data). Loss: -0.984363853931427\n",
      "Training log: 20 epoch (56448 / 60000 train. data). Loss: -0.999988317489624\n",
      "Training log: 20 epoch (57728 / 60000 train. data). Loss: -1.0\n",
      "Training log: 20 epoch (59008 / 60000 train. data). Loss: -0.998358964920044\n",
      "Test loss (avg): -0.9860123840332031, Accuracy: 0.9862\n"
     ]
    }
   ],
   "source": [
    "for i_epoch in range(num_epoch):\n",
    "    loss = None\n",
    "    \n",
    "    net.train(True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for i, (data, target) in enumerate(loaders['train']):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        \n",
    "#         print(output.shape)\n",
    "        \n",
    "        loss = f.nll_loss(output, target)\n",
    "#         loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(i_epoch+1, (i+1)*128, loss.item()))\n",
    "    \n",
    "    history['train_loss'].append(loss.item())\n",
    "    \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = net(data)\n",
    "#             loss = criterion(output, target)\n",
    "#             test_loss += loss.item()\n",
    "            test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= 10000\n",
    "    print('Test loss (avg): {}, Accuracy: {}'.format(test_loss, correct / 10000))\n",
    "    \n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(correct / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [-0.8916932940483093, -0.9968054890632629, -0.9972373843193054, -0.9999368786811829, -0.9919577240943909, -0.9517616629600525, -0.9803010821342468, -0.9751377701759338, -0.9919478893280029, -0.9971242547035217, -0.9998693466186523, -1.0, -0.9999993443489075, -0.9999923706054688, -1.0, -0.9980999827384949, -0.9991399645805359, -0.9995797276496887, -0.9898127913475037, -1.0], 'test_loss': [-0.8676578647613525, -0.9793869574546814, -0.9781899199485778, -0.9842680318832397, -0.9841402331352234, -0.9871273254394531, -0.9861274024963379, -0.984758364868164, -0.9865497139930726, -0.9857587719917298, -0.9874884323120117, -0.9867130447387695, -0.9876458847045898, -0.9861761192321777, -0.9874426963806152, -0.9869108078956604, -0.9846950593948365, -0.9866805681228638, -0.9893469528198242, -0.9860123840332031], 'test_acc': [0.8695, 0.981, 0.979, 0.9852, 0.9843, 0.9875, 0.9865, 0.9848, 0.9865, 0.9866, 0.9879, 0.9867, 0.9878, 0.9861, 0.9877, 0.9871, 0.9845, 0.9869, 0.9897, 0.9862]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epoch\n",
    "print(history)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['train_loss'])), history['train_loss'], label='train_loss')\n",
    "plt.plot(range(len(history['test_loss'])), history['test_loss'], label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('cnnloss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['test_acc'])), history['test_acc'])\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('cnn_test_acc2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
