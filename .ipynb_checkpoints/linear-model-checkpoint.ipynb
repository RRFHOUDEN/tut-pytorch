{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rightcode.co.jp/blog/information-technology/pytorch-mnist-learning\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return f.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    return {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20\n",
    "history = {\n",
    "    'train_lostt':[],\n",
    "    'test_lostt':[],\n",
    "    'test_acc':[]\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = MyNet().to(device)\n",
    "loaders = load_MNIST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.4158226251602173\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.26530420780181885\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.18521977961063385\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.2624163031578064\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.2742323577404022\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.22390985488891602\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.2535843551158905\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.17968234419822693\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.25364431738853455\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.25252461433410645\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.2698568105697632\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.34840089082717896\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.3127133250236511\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.28985846042633057\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.2589007019996643\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.2093351036310196\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.2955227494239807\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.1993769109249115\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.33777323365211487\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.21637850999832153\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.3430846631526947\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.25412899255752563\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.1904136836528778\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.19401170313358307\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.24528363347053528\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.30368152260780334\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.2129647433757782\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.3269288241863251\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.302963525056839\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.2394384890794754\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.23691418766975403\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.13929396867752075\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.202358677983284\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.14325042068958282\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.23292607069015503\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.16465625166893005\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.21193335950374603\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.21405810117721558\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.21986836194992065\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.24112404882907867\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.22224891185760498\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.17594672739505768\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.19916032254695892\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.23434509336948395\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.27063658833503723\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.19319038093090057\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.27747461199760437\n",
      "Test loss (avg): 0.20303052320480347, Accuracy: 0.939\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.21345220506191254\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.2502881586551666\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.20299090445041656\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.2714773416519165\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.18130464851856232\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.1795448213815689\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.16375383734703064\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.178656205534935\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.27711227536201477\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.1386358141899109\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.2382739782333374\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.23233434557914734\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.3139856457710266\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.12598945200443268\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.2543676495552063\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.1692630648612976\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.2810041308403015\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.16844803094863892\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.2168334275484085\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.2339572012424469\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.20051611959934235\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.13450801372528076\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.15844321250915527\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.20830421149730682\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.2797732651233673\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.13805420696735382\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.1837444007396698\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.27734002470970154\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.198654443025589\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.18413271009922028\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.17109444737434387\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.13086038827896118\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.15324190258979797\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.15503957867622375\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.07630521059036255\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.2737022340297699\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.11625941097736359\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.168455570936203\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.09936758875846863\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.14948469400405884\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.08346311748027802\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.14279554784297943\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.07910770922899246\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.23411975800991058\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.2147277593612671\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.10249916464090347\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.09966164827346802\n",
      "Test loss (avg): 0.157972212767601, Accuracy: 0.9526\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.1444682776927948\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.23228134214878082\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.11405162513256073\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.06723182648420334\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.141037717461586\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.11180656403303146\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.1379304677248001\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.12077067792415619\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.10136853903532028\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.0971292033791542\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.11050912737846375\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.17997410893440247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.10529999434947968\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.08904582262039185\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.1362162083387375\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.20100165903568268\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.12579093873500824\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.14602243900299072\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.13205070793628693\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.2688688039779663\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.208889901638031\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.1285778284072876\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.21743649244308472\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.09719101339578629\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.2546705901622772\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.13218681514263153\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.14808616042137146\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.08061926066875458\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.11205649375915527\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.2437284141778946\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.13089662790298462\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.11124707758426666\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.1693994253873825\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.1885581910610199\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.17592424154281616\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.05481521040201187\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.09885092824697495\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.1641070544719696\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.07168328016996384\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.0799366682767868\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.1837533414363861\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.21962982416152954\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.0805310308933258\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.13279461860656738\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.12537404894828796\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.11585048586130142\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.09041070193052292\n",
      "Test loss (avg): 0.12903273396492004, Accuracy: 0.961\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.13301807641983032\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.11919554322957993\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.10455326735973358\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.19328728318214417\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.09672861546278\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.1695106476545334\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.15722425282001495\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.09840179234743118\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.0717618465423584\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.11205537617206573\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.25672614574432373\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.09712977707386017\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.10861524939537048\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.21572205424308777\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.08041907846927643\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.10138847678899765\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.16761194169521332\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.07743077725172043\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.053436994552612305\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.08448097109794617\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.06417684257030487\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.09068068861961365\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.07059735059738159\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.05179525911808014\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.04915622994303703\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.18011532723903656\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.13684909045696259\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.08042602986097336\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.16139255464076996\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.12628552317619324\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.10399361699819565\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.09637195616960526\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.11261483281850815\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.05879785493016243\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.09683230519294739\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.047251518815755844\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.07185569405555725\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.05808597430586815\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.06027239188551903\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.053513575345277786\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.04508991166949272\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.09058649092912674\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.07152308523654938\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.1533232033252716\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.07594913244247437\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.10390716046094894\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.12493348121643066\n",
      "Test loss (avg): 0.10742663340568542, Accuracy: 0.9667\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.08703378587961197\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.1350977122783661\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.1272558569908142\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.09119697660207748\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.13541273772716522\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.05260802432894707\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.03806498274207115\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.20742851495742798\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.10043454170227051\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.0481056310236454\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.12657687067985535\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.12901818752288818\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.05493578314781189\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.09870730340480804\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.06802088022232056\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.14023691415786743\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.05967655032873154\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.11739902198314667\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.08596109598875046\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.1086498498916626\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.1479635238647461\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.08860579133033752\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.05577817186713219\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.056519247591495514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.0916919931769371\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.12245848029851913\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.1115497499704361\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.09155251085758209\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.13643325865268707\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.04263182356953621\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.14817668497562408\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.1144748330116272\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.07222903519868851\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.15750174224376678\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.057315509766340256\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.07716985791921616\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.03702161833643913\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.1334773600101471\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.0866694226861\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.14754939079284668\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.08154551684856415\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.056935589760541916\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.10490810871124268\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.05707480013370514\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.11356106400489807\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.08056657016277313\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.030504882335662842\n",
      "Test loss (avg): 0.0977000301361084, Accuracy: 0.9704\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.0693744644522667\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.06301473826169968\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.15133905410766602\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.08599657565355301\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.1572032868862152\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.05213487893342972\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.07125116884708405\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.07489250600337982\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.07873396575450897\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.044789981096982956\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.05137825757265091\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.07315093278884888\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.06201927736401558\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.10837501287460327\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.0443415641784668\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.06921567022800446\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.14856038987636566\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.11031474173069\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.09038977324962616\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.04771663248538971\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.13921576738357544\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.09611694514751434\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.07386192679405212\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.06029130518436432\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.04062027111649513\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.22677694261074066\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.12157132476568222\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.05599556118249893\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.12750856578350067\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.08562453091144562\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.06767894327640533\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.04812701418995857\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.02020316943526268\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.037139616906642914\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.1246052086353302\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.07168899476528168\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.06088339537382126\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.08111806213855743\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.08342516422271729\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.08820471167564392\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.09510265290737152\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.024436596781015396\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.02300732582807541\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.0595763623714447\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.17130441963672638\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.06019183248281479\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.06886932998895645\n",
      "Test loss (avg): 0.08491053676605224, Accuracy: 0.9741\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.04935755208134651\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.05568668246269226\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.06700611114501953\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.053955040872097015\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.08487892150878906\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.0484473891556263\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.04326248914003372\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.07001065462827682\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.07931826263666153\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.1579715758562088\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.04916372895240784\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.027166925370693207\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.03086547739803791\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.0383203960955143\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.06809082627296448\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.07422032952308655\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.08285129070281982\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.033812325447797775\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.04650411382317543\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.11061474680900574\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.06104397028684616\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.03576286882162094\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.05936230719089508\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.06498884409666061\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.10534663498401642\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.03928421437740326\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.07346449047327042\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.07044049352407455\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.0188785120844841\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.04597058892250061\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.040638960897922516\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.03886077180504799\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.07151222229003906\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.05266902223229408\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.059839583933353424\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.04806114360690117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.08612725883722305\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.02367500774562359\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.07802297174930573\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.022665612399578094\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.0585208423435688\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.03363838791847229\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.058786679059267044\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.04328719154000282\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.030294539406895638\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.027320314198732376\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.07116088271141052\n",
      "Test loss (avg): 0.07920893597602845, Accuracy: 0.9755\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.06524910032749176\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.03499491140246391\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.06767121702432632\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.03522704541683197\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.08794862031936646\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.06835722923278809\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.07726435363292694\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.026563627645373344\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.035427384078502655\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.02169124037027359\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.03038864955306053\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.07896500080823898\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.04655466228723526\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.03729603439569473\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.05760198459029198\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.04095951467752457\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.04767581447958946\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.08717989176511765\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.061315033584833145\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.06529341638088226\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.02628803811967373\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.023524446412920952\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.037507135421037674\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.049605920910835266\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.030889809131622314\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.07643954455852509\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.037387002259492874\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.05159899592399597\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.04349839687347412\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.04131922498345375\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.04610385000705719\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.0371505431830883\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.034456755965948105\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.06761527061462402\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.05308837443590164\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.09957624226808548\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.03159916773438454\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.034236177802085876\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.06021585687994957\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.062321994453668594\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.10194762051105499\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.06943003833293915\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.05771072581410408\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.025099430233240128\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.053711045533418655\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.033852893859148026\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.09575806558132172\n",
      "Test loss (avg): 0.073805513215065, Accuracy: 0.9778\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.01493927650153637\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.10201539844274521\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.06397087126970291\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.031010810285806656\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.03252584487199783\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.04961845651268959\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.016703514382243156\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.01824403740465641\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.047531090676784515\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.07114943861961365\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.05610277131199837\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.03742343932390213\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.03442498296499252\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.049153171479701996\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.035728346556425095\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.02308465540409088\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.010956769809126854\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.02084686979651451\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.019966276362538338\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.05309581756591797\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.0173298642039299\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.09174768626689911\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.058790408074855804\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.11854380369186401\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.017356816679239273\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.040371138602495193\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.04436368867754936\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.07232516258955002\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.047919951379299164\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.060312073677778244\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.02672617696225643\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.028509341180324554\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.04743834212422371\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.023418685421347618\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.035272881388664246\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.0464743971824646\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.04633953049778938\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.07660422474145889\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.04300958290696144\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.02797391638159752\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.06853297352790833\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.022299934178590775\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.039422642439603806\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.031073393300175667\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.01874683052301407\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.01996893249452114\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.030518028885126114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (avg): 0.07104523124694824, Accuracy: 0.9785\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.07565540075302124\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.03482634946703911\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.018932346254587173\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.03036625124514103\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.015787631273269653\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.0248931385576725\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.0834043025970459\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.027848700061440468\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.010488346219062805\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.02517235279083252\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.060690198093652725\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.0410967692732811\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.05676621198654175\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.05571913346648216\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.021079063415527344\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.02872348390519619\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.015825025737285614\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.0681820809841156\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.02135077677667141\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.03367241844534874\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.029467877000570297\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.010192951187491417\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.03129585459828377\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.03282444551587105\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.04742078855633736\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.0520249642431736\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.05722388997673988\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.05392986163496971\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.041924864053726196\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.05870529264211655\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.03242190554738045\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.010630670934915543\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.032462719827890396\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.026180889457464218\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.018925709649920464\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.014659728854894638\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.013379961252212524\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.07057380676269531\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.0926067903637886\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.0594354122877121\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.015368159860372543\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.03833994269371033\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.07414451241493225\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.02409028261899948\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.044282492250204086\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.02002860978245735\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.02306416444480419\n",
      "Test loss (avg): 0.06924872674942016, Accuracy: 0.9787\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.05493709817528725\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.014125540852546692\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.03947993367910385\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.01750236004590988\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.018781490623950958\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.025046681985259056\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.024541547521948814\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.015321750193834305\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.03902026265859604\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.028171813115477562\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.04196811467409134\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.031354278326034546\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.011735914275050163\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.007965575903654099\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.011668169870972633\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.016894768923521042\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.023167265579104424\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.018886927515268326\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.01434723474085331\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.02507074922323227\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.023160243406891823\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.07907211780548096\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.03370282053947449\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.013283872976899147\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.01827256940305233\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.01128331944346428\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.030352625995874405\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.09024731814861298\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.017214294523000717\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.04654679447412491\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.027709344401955605\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.05455046147108078\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.017458587884902954\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.04067056626081467\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.027773605659604073\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.05134780332446098\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.01758730597794056\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.02501918375492096\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.0144950021058321\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.009484220296144485\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.015788821503520012\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.048490218818187714\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.05864802375435829\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.04072320833802223\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.0206463485956192\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.033225055783987045\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.03505685552954674\n",
      "Test loss (avg): 0.0660652242898941, Accuracy: 0.9799\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.024306228384375572\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.006954783573746681\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.014519726857542992\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.015276657417416573\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.02165674977004528\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.01979585364460945\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.015544513240456581\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.0204185601323843\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.024327082559466362\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.011315111070871353\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.03784164413809776\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.016522223129868507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.034004345536231995\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.02238140068948269\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.06612639129161835\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.005288112908601761\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.012125924229621887\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.015342611819505692\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.0453348234295845\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.013133756816387177\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.023934241384267807\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.007147114723920822\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.019258318468928337\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.05197352170944214\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.0115995854139328\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.013433597981929779\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.06075048819184303\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.01789887063205242\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.017920255661010742\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.005557749420404434\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.02747792936861515\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.006333019584417343\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.046549178659915924\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.010265903547406197\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.006255008280277252\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.014199448749423027\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.042009953409433365\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.02329806610941887\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.00788385421037674\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.0075307488441467285\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.007865168154239655\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.02837132103741169\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.01578039675951004\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.06541895866394043\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.019519217312335968\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.02168428525328636\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.010457390919327736\n",
      "Test loss (avg): 0.06631335380077362, Accuracy: 0.9795\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.00893900915980339\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.017674356698989868\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.014705510810017586\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.025737304240465164\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.010747738182544708\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.0052400920540094376\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.016848886385560036\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.0315340980887413\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.003102067857980728\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.010439462959766388\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.013823110610246658\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.019385356456041336\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.013802170753479004\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.04806046187877655\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.03128839284181595\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.056970175355672836\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.03718851879239082\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.014287197962403297\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.009334582835435867\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.007343651726841927\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.012261494994163513\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.025343535467982292\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.02429366670548916\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.02211548574268818\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.016131829470396042\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.029282961040735245\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.024641308933496475\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.00733964703977108\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.020483238622546196\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.018621666356921196\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.016528306528925896\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.011071957647800446\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.01486305333673954\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.007319658994674683\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.010859820991754532\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.011778406798839569\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.019634291529655457\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.011969432234764099\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.006059927865862846\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.01606460101902485\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.01260922197252512\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.013056628406047821\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.026090413331985474\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.008268410339951515\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.01778763346374035\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.011990990489721298\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.020686546340584755\n",
      "Test loss (avg): 0.06458996207714081, Accuracy: 0.9796\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.006748918443918228\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.018097246065735817\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.012750647962093353\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.03038417361676693\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.00957983173429966\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.058656901121139526\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.012173199094831944\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.0070520248264074326\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.015844017267227173\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.004143368452787399\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.012051723897457123\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.03583260253071785\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.015073863789439201\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.006348714232444763\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.006765902042388916\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.007109280675649643\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.006454311311244965\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.01160450465977192\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.02232043817639351\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.0107844527810812\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.016670627519488335\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.006552638486027718\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.024753564968705177\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.009309511631727219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.03625280782580376\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.013527030125260353\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.009777802973985672\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.01732722856104374\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.004560886882245541\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.01613355427980423\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.047499045729637146\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.013475015759468079\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.016598323360085487\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.009396325796842575\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.004580248147249222\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.013245567679405212\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.007239706814289093\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.007016848772764206\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.004095673561096191\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.005971060134470463\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.03457286208868027\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.015632199123501778\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.011770248413085938\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.020493268966674805\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.020407143980264664\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.016670487821102142\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.012295812368392944\n",
      "Test loss (avg): 0.0647345454454422, Accuracy: 0.9801\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.00925644300878048\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.009208397939801216\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.009036034345626831\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.0059716105461120605\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.008059271611273289\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.004089087247848511\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.009978746995329857\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.009421676397323608\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.009598523378372192\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.00423659011721611\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.003467308357357979\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.014247924089431763\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.011598536744713783\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.0030854232609272003\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.014342697337269783\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.004114206880331039\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.00744248740375042\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.0034155435860157013\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.00786815956234932\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.012211516499519348\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.009444870054721832\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.00645638071000576\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.005611523985862732\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.010327817872166634\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.0067275408655405045\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.006052924320101738\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.00852988287806511\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.00923847034573555\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.011783408001065254\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.01857738196849823\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.015569368377327919\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.020461857318878174\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.003466913476586342\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.002817310392856598\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.014000393450260162\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.009673032909631729\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.015177775174379349\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.007945079356431961\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.00813145563006401\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.023350413888692856\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.017403321340680122\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.012328624725341797\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.012994332239031792\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.011724389158189297\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.005517888814210892\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.011932706460356712\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.01097181811928749\n",
      "Test loss (avg): 0.07037913846969604, Accuracy: 0.9802\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.0055168624967336655\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.020815540105104446\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.0029979534447193146\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.009074699133634567\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.005031120032072067\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.005390003323554993\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.005109105259180069\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.018062464892864227\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.004951510578393936\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.004612155258655548\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.004387741908431053\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.035789601504802704\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.01060439646244049\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.019895393401384354\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.0021236855536699295\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.011871971189975739\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.012353017926216125\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.006054475903511047\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.004891868680715561\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.004757098853588104\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.004676636308431625\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.015796642750501633\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.005062548443675041\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.006287183612585068\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.01014370284974575\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.00825371965765953\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.03348531201481819\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.005572838708758354\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.004698697477579117\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.015153630636632442\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.01321680098772049\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.01746426522731781\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.002420133911073208\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.004327710717916489\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.012558922171592712\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.006265638396143913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.013707596808671951\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.006486576050519943\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.011394156143069267\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.007230620831251144\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.010272817686200142\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.013107981532812119\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.012179816141724586\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.013984989374876022\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.016424167901277542\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.009068944491446018\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.02980935014784336\n",
      "Test loss (avg): 0.06771950492858887, Accuracy: 0.9803\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.0031424593180418015\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.0037834085524082184\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.0027041956782341003\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.003201369196176529\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.0070412252098321915\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.0031604282557964325\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.002826940268278122\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.00808301568031311\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.015527741983532906\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.008928850293159485\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.0026768893003463745\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.005256541073322296\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.006698990240693092\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.009445533156394958\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.007407648488879204\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.004551095888018608\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.05036762356758118\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.010361392050981522\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.009420229122042656\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.0053765252232551575\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.004927419126033783\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.002948550507426262\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.007769608870148659\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.018528616055846214\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.0029902569949626923\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.009060271084308624\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.012450458481907845\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.003033790737390518\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.009380417875945568\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.002514997497200966\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.008025050163269043\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.0006635524332523346\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.041294895112514496\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.0073217106983065605\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.00857560709118843\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.006753545254468918\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.0022133029997348785\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.012565601617097855\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.013738504610955715\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.0014685336500406265\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.0037064645439386368\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.006196519359946251\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.01010725274682045\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.005354674533009529\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.02132340334355831\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.01486407034099102\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.005166541785001755\n",
      "Test loss (avg): 0.06311030225753785, Accuracy: 0.9806\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.003735274076461792\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.0035301707684993744\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.005436405539512634\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.004104571416974068\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.007034000009298325\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.003407759591937065\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.0023367591202259064\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.014030415564775467\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.003459800034761429\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.0022964701056480408\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.004397014155983925\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.0037883929908275604\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.002836449071764946\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.007445361465215683\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.0033154208213090897\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.0022326037287712097\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.004878906533122063\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.009478911757469177\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.007958494126796722\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.0017957314848899841\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.0164546649903059\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.006792457774281502\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.004450175911188126\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.0024626217782497406\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.003920849412679672\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.007107073441147804\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.0031679589301347733\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.003745114430785179\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.0023588277399539948\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.015850475057959557\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.006492452695965767\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.005565004423260689\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.00907098688185215\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.0027636662125587463\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.0034778639674186707\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.0031410157680511475\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.0026532690972089767\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.0032700952142477036\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.0034273266792297363\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.004741933196783066\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.001713823527097702\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.0048925429582595825\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.01188725233078003\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.006882453337311745\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.001385049894452095\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.0026475712656974792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.006643971428275108\n",
      "Test loss (avg): 0.061458275818824765, Accuracy: 0.9824\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.0023815222084522247\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.0015838034451007843\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.0035527730360627174\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.0025253165513277054\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.002935558557510376\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.005135387182235718\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.0012197699397802353\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.002854645252227783\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.0031817741692066193\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.0018568262457847595\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.007052727043628693\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.00144132599234581\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.006262553855776787\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.006192322820425034\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.003293050453066826\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.004928635433316231\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.004856858402490616\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.004437027499079704\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.001301182433962822\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.002330971881747246\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.00741228275001049\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.003359885886311531\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.005901213735342026\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.004788156598806381\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.007198275066912174\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.005320638418197632\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.005661693401634693\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.005170593038201332\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.007215045392513275\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.0016453005373477936\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.0028554797172546387\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.012477175332605839\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.004500748589634895\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.0046813469380140305\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.004478249698877335\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.018311066552996635\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.0016549713909626007\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.007102208212018013\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.0015908963978290558\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.0015249401330947876\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.003130948171019554\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.015575358644127846\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.004970930516719818\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.011196337640285492\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.006473766639828682\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.002673368901014328\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.00491899810731411\n",
      "Test loss (avg): 0.06207135330438614, Accuracy: 0.9823\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 0.004084479063749313\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 0.0016663707792758942\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 0.002094179391860962\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 0.013176828622817993\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.0024801231920719147\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.005613531917333603\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.004343034699559212\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.0067458488047122955\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.0031608976423740387\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.0011033304035663605\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.009922979399561882\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.00715081300586462\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.0162460058927536\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.004290588200092316\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.005632976070046425\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.00440300814807415\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.0035755448043346405\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.0021170340478420258\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.0028249993920326233\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.0026506781578063965\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.0029486622661352158\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.0014354661107063293\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.006481548771262169\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.004586311057209969\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.001769181340932846\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.0017329119145870209\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.003976784646511078\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.0026231296360492706\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.0011451244354248047\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.004658116027712822\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.004316462203860283\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.0035936301574110985\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.0013402476906776428\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.0032554566860198975\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.005687244236469269\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.0064425719901919365\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.007044762372970581\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.0032674185931682587\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.0009729936718940735\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.00340430811047554\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.002775726839900017\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.00419546477496624\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.0029956139624118805\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.006358150392770767\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.0033553875982761383\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.00593496672809124\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.003765154629945755\n",
      "Test loss (avg): 0.06628001140356064, Accuracy: 0.9816\n",
      "{'train_loss': [tensor(0.3325, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)], 'test_loss': [0.20303052320480347, 0.157972212767601, 0.12903273396492004, 0.10742663340568542, 0.0977000301361084, 0.08491053676605224, 0.07920893597602845, 0.073805513215065, 0.07104523124694824, 0.06924872674942016, 0.0660652242898941, 0.06631335380077362, 0.06458996207714081, 0.0647345454454422, 0.07037913846969604, 0.06771950492858887, 0.06311030225753785, 0.061458275818824765, 0.06207135330438614, 0.06628001140356064], 'test_acc': [0.939, 0.9526, 0.961, 0.9667, 0.9704, 0.9741, 0.9755, 0.9778, 0.9785, 0.9787, 0.9799, 0.9795, 0.9796, 0.9801, 0.9802, 0.9803, 0.9806, 0.9824, 0.9823, 0.9816]}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (21,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-af09f1fe43e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2794\u001b[0m     return gca().plot(\n\u001b[0;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2796\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m         \"\"\"\n\u001b[0;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1665\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 270\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (21,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i_epoch in range(num_epoch):\n",
    "    loss = None\n",
    "    \n",
    "    net.train(True)\n",
    "    \n",
    "    for i, (data, target) in enumerate(loaders['train']):\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = f.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(i_epoch+1, (i+1)*128, loss.item()))\n",
    "    \n",
    "    history['train_loss'].append(loss)\n",
    "    \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = net(data)\n",
    "            test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= 10000\n",
    "    print('Test loss (avg): {}, Accuracy: {}'.format(test_loss, correct / 10000))\n",
    "    \n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(correct / 10000)\n",
    "    \n",
    "print(history)\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epoch+1), history['train_loss'], label='train_loss')\n",
    "plt.plot(range(1, num_epoch+1), history['test_loss'], label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, epoch+1), history['test_acc'])\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('test_acc.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1660'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
