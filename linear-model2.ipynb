{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rightcode.co.jp/blog/information-technology/pytorch-mnist-learning\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 5000)\n",
    "        self.fc3 = torch.nn.Linear(5000, 3000)\n",
    "        self.fc4 = torch.nn.Linear(3000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return f.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    return {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20\n",
    "history = {\n",
    "    'train_loss':[],\n",
    "    'test_loss':[],\n",
    "    'test_acc':[]\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = MyNet().to(device)\n",
    "loaders = load_MNIST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 2.297132730484009\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 3.287682294845581\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 2.4148969650268555\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 2.379462957382202\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 2.2093350887298584\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 2.098137378692627\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 1.9702472686767578\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 1.8906311988830566\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 1.8152360916137695\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 1.7271825075149536\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 1.797523021697998\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 1.5991472005844116\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 1.4201130867004395\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 1.0363794565200806\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 1.1706453561782837\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 1.1129826307296753\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 1.061490774154663\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.9496355652809143\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.885100245475769\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.965194821357727\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.7758522033691406\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.9162748456001282\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.6145122051239014\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.5157938003540039\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.5621121525764465\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.4541383981704712\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.5166499614715576\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.5465009212493896\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.47051048278808594\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.6308885216712952\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.4225333333015442\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.5252609252929688\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.42420119047164917\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.4064209461212158\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.35575464367866516\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.3440849184989929\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.34229469299316406\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.31438517570495605\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.3901442885398865\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.40947791934013367\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.48846280574798584\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.2042517364025116\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.4643319845199585\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.17885416746139526\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.26669615507125854\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.18604817986488342\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.3024592399597168\n",
      "Test loss (avg): 0.29929737558364866, Accuracy: 0.9109\n",
      "Training log: 2 epoch (128 / 60000 train. data). Loss: 0.34972962737083435\n",
      "Training log: 2 epoch (1408 / 60000 train. data). Loss: 0.20225124061107635\n",
      "Training log: 2 epoch (2688 / 60000 train. data). Loss: 0.2574499547481537\n",
      "Training log: 2 epoch (3968 / 60000 train. data). Loss: 0.18504583835601807\n",
      "Training log: 2 epoch (5248 / 60000 train. data). Loss: 0.35749033093452454\n",
      "Training log: 2 epoch (6528 / 60000 train. data). Loss: 0.1741453856229782\n",
      "Training log: 2 epoch (7808 / 60000 train. data). Loss: 0.3059924840927124\n",
      "Training log: 2 epoch (9088 / 60000 train. data). Loss: 0.3005651831626892\n",
      "Training log: 2 epoch (10368 / 60000 train. data). Loss: 0.2962620258331299\n",
      "Training log: 2 epoch (11648 / 60000 train. data). Loss: 0.3411642909049988\n",
      "Training log: 2 epoch (12928 / 60000 train. data). Loss: 0.23359087109565735\n",
      "Training log: 2 epoch (14208 / 60000 train. data). Loss: 0.23355895280838013\n",
      "Training log: 2 epoch (15488 / 60000 train. data). Loss: 0.3306201994419098\n",
      "Training log: 2 epoch (16768 / 60000 train. data). Loss: 0.2617737054824829\n",
      "Training log: 2 epoch (18048 / 60000 train. data). Loss: 0.28822124004364014\n",
      "Training log: 2 epoch (19328 / 60000 train. data). Loss: 0.37143567204475403\n",
      "Training log: 2 epoch (20608 / 60000 train. data). Loss: 0.19618040323257446\n",
      "Training log: 2 epoch (21888 / 60000 train. data). Loss: 0.16272388398647308\n",
      "Training log: 2 epoch (23168 / 60000 train. data). Loss: 0.31754404306411743\n",
      "Training log: 2 epoch (24448 / 60000 train. data). Loss: 0.2751730978488922\n",
      "Training log: 2 epoch (25728 / 60000 train. data). Loss: 0.21145129203796387\n",
      "Training log: 2 epoch (27008 / 60000 train. data). Loss: 0.10600391030311584\n",
      "Training log: 2 epoch (28288 / 60000 train. data). Loss: 0.2441895306110382\n",
      "Training log: 2 epoch (29568 / 60000 train. data). Loss: 0.3076933026313782\n",
      "Training log: 2 epoch (30848 / 60000 train. data). Loss: 0.31046587228775024\n",
      "Training log: 2 epoch (32128 / 60000 train. data). Loss: 0.2571420669555664\n",
      "Training log: 2 epoch (33408 / 60000 train. data). Loss: 0.2889235317707062\n",
      "Training log: 2 epoch (34688 / 60000 train. data). Loss: 0.23323284089565277\n",
      "Training log: 2 epoch (35968 / 60000 train. data). Loss: 0.1887234002351761\n",
      "Training log: 2 epoch (37248 / 60000 train. data). Loss: 0.27928879857063293\n",
      "Training log: 2 epoch (38528 / 60000 train. data). Loss: 0.19453313946723938\n",
      "Training log: 2 epoch (39808 / 60000 train. data). Loss: 0.16379497945308685\n",
      "Training log: 2 epoch (41088 / 60000 train. data). Loss: 0.3086656928062439\n",
      "Training log: 2 epoch (42368 / 60000 train. data). Loss: 0.15393611788749695\n",
      "Training log: 2 epoch (43648 / 60000 train. data). Loss: 0.22403106093406677\n",
      "Training log: 2 epoch (44928 / 60000 train. data). Loss: 0.19490820169448853\n",
      "Training log: 2 epoch (46208 / 60000 train. data). Loss: 0.22794170677661896\n",
      "Training log: 2 epoch (47488 / 60000 train. data). Loss: 0.18527022004127502\n",
      "Training log: 2 epoch (48768 / 60000 train. data). Loss: 0.17598065733909607\n",
      "Training log: 2 epoch (50048 / 60000 train. data). Loss: 0.10520613938570023\n",
      "Training log: 2 epoch (51328 / 60000 train. data). Loss: 0.28345733880996704\n",
      "Training log: 2 epoch (52608 / 60000 train. data). Loss: 0.19030699133872986\n",
      "Training log: 2 epoch (53888 / 60000 train. data). Loss: 0.16058073937892914\n",
      "Training log: 2 epoch (55168 / 60000 train. data). Loss: 0.2695689797401428\n",
      "Training log: 2 epoch (56448 / 60000 train. data). Loss: 0.3312820494174957\n",
      "Training log: 2 epoch (57728 / 60000 train. data). Loss: 0.358795702457428\n",
      "Training log: 2 epoch (59008 / 60000 train. data). Loss: 0.1651047021150589\n",
      "Test loss (avg): 0.2110747040271759, Accuracy: 0.9351\n",
      "Training log: 3 epoch (128 / 60000 train. data). Loss: 0.29623937606811523\n",
      "Training log: 3 epoch (1408 / 60000 train. data). Loss: 0.1556171476840973\n",
      "Training log: 3 epoch (2688 / 60000 train. data). Loss: 0.17431822419166565\n",
      "Training log: 3 epoch (3968 / 60000 train. data). Loss: 0.11947416514158249\n",
      "Training log: 3 epoch (5248 / 60000 train. data). Loss: 0.11069352924823761\n",
      "Training log: 3 epoch (6528 / 60000 train. data). Loss: 0.1184745728969574\n",
      "Training log: 3 epoch (7808 / 60000 train. data). Loss: 0.1195867508649826\n",
      "Training log: 3 epoch (9088 / 60000 train. data). Loss: 0.20601220428943634\n",
      "Training log: 3 epoch (10368 / 60000 train. data). Loss: 0.27641838788986206\n",
      "Training log: 3 epoch (11648 / 60000 train. data). Loss: 0.27724599838256836\n",
      "Training log: 3 epoch (12928 / 60000 train. data). Loss: 0.12611545622348785\n",
      "Training log: 3 epoch (14208 / 60000 train. data). Loss: 0.10479141771793365\n",
      "Training log: 3 epoch (15488 / 60000 train. data). Loss: 0.07109200954437256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 3 epoch (16768 / 60000 train. data). Loss: 0.30994734168052673\n",
      "Training log: 3 epoch (18048 / 60000 train. data). Loss: 0.1272897571325302\n",
      "Training log: 3 epoch (19328 / 60000 train. data). Loss: 0.1817823350429535\n",
      "Training log: 3 epoch (20608 / 60000 train. data). Loss: 0.053818102926015854\n",
      "Training log: 3 epoch (21888 / 60000 train. data). Loss: 0.20448726415634155\n",
      "Training log: 3 epoch (23168 / 60000 train. data). Loss: 0.1272248774766922\n",
      "Training log: 3 epoch (24448 / 60000 train. data). Loss: 0.04768768325448036\n",
      "Training log: 3 epoch (25728 / 60000 train. data). Loss: 0.1858079433441162\n",
      "Training log: 3 epoch (27008 / 60000 train. data). Loss: 0.15829643607139587\n",
      "Training log: 3 epoch (28288 / 60000 train. data). Loss: 0.1826021820306778\n",
      "Training log: 3 epoch (29568 / 60000 train. data). Loss: 0.28339141607284546\n",
      "Training log: 3 epoch (30848 / 60000 train. data). Loss: 0.14161476492881775\n",
      "Training log: 3 epoch (32128 / 60000 train. data). Loss: 0.18175865709781647\n",
      "Training log: 3 epoch (33408 / 60000 train. data). Loss: 0.12277025729417801\n",
      "Training log: 3 epoch (34688 / 60000 train. data). Loss: 0.2122846096754074\n",
      "Training log: 3 epoch (35968 / 60000 train. data). Loss: 0.140648752450943\n",
      "Training log: 3 epoch (37248 / 60000 train. data). Loss: 0.18210962414741516\n",
      "Training log: 3 epoch (38528 / 60000 train. data). Loss: 0.1277971863746643\n",
      "Training log: 3 epoch (39808 / 60000 train. data). Loss: 0.19558635354042053\n",
      "Training log: 3 epoch (41088 / 60000 train. data). Loss: 0.17574265599250793\n",
      "Training log: 3 epoch (42368 / 60000 train. data). Loss: 0.2306857407093048\n",
      "Training log: 3 epoch (43648 / 60000 train. data). Loss: 0.14136630296707153\n",
      "Training log: 3 epoch (44928 / 60000 train. data). Loss: 0.1457732766866684\n",
      "Training log: 3 epoch (46208 / 60000 train. data). Loss: 0.12139866501092911\n",
      "Training log: 3 epoch (47488 / 60000 train. data). Loss: 0.11653578281402588\n",
      "Training log: 3 epoch (48768 / 60000 train. data). Loss: 0.1428593397140503\n",
      "Training log: 3 epoch (50048 / 60000 train. data). Loss: 0.23398035764694214\n",
      "Training log: 3 epoch (51328 / 60000 train. data). Loss: 0.09030231833457947\n",
      "Training log: 3 epoch (52608 / 60000 train. data). Loss: 0.17930778861045837\n",
      "Training log: 3 epoch (53888 / 60000 train. data). Loss: 0.11343582719564438\n",
      "Training log: 3 epoch (55168 / 60000 train. data). Loss: 0.10100129246711731\n",
      "Training log: 3 epoch (56448 / 60000 train. data). Loss: 0.08775309473276138\n",
      "Training log: 3 epoch (57728 / 60000 train. data). Loss: 0.21186619997024536\n",
      "Training log: 3 epoch (59008 / 60000 train. data). Loss: 0.11580608040094376\n",
      "Test loss (avg): 0.14177264289855956, Accuracy: 0.9564\n",
      "Training log: 4 epoch (128 / 60000 train. data). Loss: 0.21078330278396606\n",
      "Training log: 4 epoch (1408 / 60000 train. data). Loss: 0.1587384045124054\n",
      "Training log: 4 epoch (2688 / 60000 train. data). Loss: 0.17105691134929657\n",
      "Training log: 4 epoch (3968 / 60000 train. data). Loss: 0.14089125394821167\n",
      "Training log: 4 epoch (5248 / 60000 train. data). Loss: 0.146820068359375\n",
      "Training log: 4 epoch (6528 / 60000 train. data). Loss: 0.20196309685707092\n",
      "Training log: 4 epoch (7808 / 60000 train. data). Loss: 0.14370064437389374\n",
      "Training log: 4 epoch (9088 / 60000 train. data). Loss: 0.1730799376964569\n",
      "Training log: 4 epoch (10368 / 60000 train. data). Loss: 0.062188245356082916\n",
      "Training log: 4 epoch (11648 / 60000 train. data). Loss: 0.03305467590689659\n",
      "Training log: 4 epoch (12928 / 60000 train. data). Loss: 0.09143801033496857\n",
      "Training log: 4 epoch (14208 / 60000 train. data). Loss: 0.19624221324920654\n",
      "Training log: 4 epoch (15488 / 60000 train. data). Loss: 0.025609496980905533\n",
      "Training log: 4 epoch (16768 / 60000 train. data). Loss: 0.10025642812252045\n",
      "Training log: 4 epoch (18048 / 60000 train. data). Loss: 0.0623069629073143\n",
      "Training log: 4 epoch (19328 / 60000 train. data). Loss: 0.14393988251686096\n",
      "Training log: 4 epoch (20608 / 60000 train. data). Loss: 0.07581150531768799\n",
      "Training log: 4 epoch (21888 / 60000 train. data). Loss: 0.09698319435119629\n",
      "Training log: 4 epoch (23168 / 60000 train. data). Loss: 0.11268715560436249\n",
      "Training log: 4 epoch (24448 / 60000 train. data). Loss: 0.07391396909952164\n",
      "Training log: 4 epoch (25728 / 60000 train. data). Loss: 0.123893141746521\n",
      "Training log: 4 epoch (27008 / 60000 train. data). Loss: 0.09782660007476807\n",
      "Training log: 4 epoch (28288 / 60000 train. data). Loss: 0.09820850938558578\n",
      "Training log: 4 epoch (29568 / 60000 train. data). Loss: 0.09943577647209167\n",
      "Training log: 4 epoch (30848 / 60000 train. data). Loss: 0.08986964076757431\n",
      "Training log: 4 epoch (32128 / 60000 train. data). Loss: 0.10246463119983673\n",
      "Training log: 4 epoch (33408 / 60000 train. data). Loss: 0.15066616237163544\n",
      "Training log: 4 epoch (34688 / 60000 train. data). Loss: 0.25297337770462036\n",
      "Training log: 4 epoch (35968 / 60000 train. data). Loss: 0.20671093463897705\n",
      "Training log: 4 epoch (37248 / 60000 train. data). Loss: 0.1733800172805786\n",
      "Training log: 4 epoch (38528 / 60000 train. data). Loss: 0.14570140838623047\n",
      "Training log: 4 epoch (39808 / 60000 train. data). Loss: 0.0761023461818695\n",
      "Training log: 4 epoch (41088 / 60000 train. data). Loss: 0.04324539378285408\n",
      "Training log: 4 epoch (42368 / 60000 train. data). Loss: 0.10793489217758179\n",
      "Training log: 4 epoch (43648 / 60000 train. data). Loss: 0.10510924458503723\n",
      "Training log: 4 epoch (44928 / 60000 train. data). Loss: 0.08676784485578537\n",
      "Training log: 4 epoch (46208 / 60000 train. data). Loss: 0.10462486743927002\n",
      "Training log: 4 epoch (47488 / 60000 train. data). Loss: 0.05602002143859863\n",
      "Training log: 4 epoch (48768 / 60000 train. data). Loss: 0.13613493740558624\n",
      "Training log: 4 epoch (50048 / 60000 train. data). Loss: 0.15108813345432281\n",
      "Training log: 4 epoch (51328 / 60000 train. data). Loss: 0.08533962815999985\n",
      "Training log: 4 epoch (52608 / 60000 train. data). Loss: 0.1023177057504654\n",
      "Training log: 4 epoch (53888 / 60000 train. data). Loss: 0.07533062249422073\n",
      "Training log: 4 epoch (55168 / 60000 train. data). Loss: 0.10906355082988739\n",
      "Training log: 4 epoch (56448 / 60000 train. data). Loss: 0.0965200811624527\n",
      "Training log: 4 epoch (57728 / 60000 train. data). Loss: 0.11282584071159363\n",
      "Training log: 4 epoch (59008 / 60000 train. data). Loss: 0.14247819781303406\n",
      "Test loss (avg): 0.1473788492202759, Accuracy: 0.9573\n",
      "Training log: 5 epoch (128 / 60000 train. data). Loss: 0.21198031306266785\n",
      "Training log: 5 epoch (1408 / 60000 train. data). Loss: 0.18166223168373108\n",
      "Training log: 5 epoch (2688 / 60000 train. data). Loss: 0.142476886510849\n",
      "Training log: 5 epoch (3968 / 60000 train. data). Loss: 0.09670093655586243\n",
      "Training log: 5 epoch (5248 / 60000 train. data). Loss: 0.09954547882080078\n",
      "Training log: 5 epoch (6528 / 60000 train. data). Loss: 0.06446853280067444\n",
      "Training log: 5 epoch (7808 / 60000 train. data). Loss: 0.1121673732995987\n",
      "Training log: 5 epoch (9088 / 60000 train. data). Loss: 0.10801529884338379\n",
      "Training log: 5 epoch (10368 / 60000 train. data). Loss: 0.1808907836675644\n",
      "Training log: 5 epoch (11648 / 60000 train. data). Loss: 0.170058012008667\n",
      "Training log: 5 epoch (12928 / 60000 train. data). Loss: 0.022327642887830734\n",
      "Training log: 5 epoch (14208 / 60000 train. data). Loss: 0.08993461728096008\n",
      "Training log: 5 epoch (15488 / 60000 train. data). Loss: 0.06275252252817154\n",
      "Training log: 5 epoch (16768 / 60000 train. data). Loss: 0.1351560801267624\n",
      "Training log: 5 epoch (18048 / 60000 train. data). Loss: 0.07601210474967957\n",
      "Training log: 5 epoch (19328 / 60000 train. data). Loss: 0.07308079302310944\n",
      "Training log: 5 epoch (20608 / 60000 train. data). Loss: 0.10991600155830383\n",
      "Training log: 5 epoch (21888 / 60000 train. data). Loss: 0.07159081846475601\n",
      "Training log: 5 epoch (23168 / 60000 train. data). Loss: 0.04952992871403694\n",
      "Training log: 5 epoch (24448 / 60000 train. data). Loss: 0.19729259610176086\n",
      "Training log: 5 epoch (25728 / 60000 train. data). Loss: 0.13354085385799408\n",
      "Training log: 5 epoch (27008 / 60000 train. data). Loss: 0.07987895607948303\n",
      "Training log: 5 epoch (28288 / 60000 train. data). Loss: 0.041079577058553696\n",
      "Training log: 5 epoch (29568 / 60000 train. data). Loss: 0.13519814610481262\n",
      "Training log: 5 epoch (30848 / 60000 train. data). Loss: 0.0559607595205307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 5 epoch (32128 / 60000 train. data). Loss: 0.24437780678272247\n",
      "Training log: 5 epoch (33408 / 60000 train. data). Loss: 0.12574370205402374\n",
      "Training log: 5 epoch (34688 / 60000 train. data). Loss: 0.13489016890525818\n",
      "Training log: 5 epoch (35968 / 60000 train. data). Loss: 0.0463426373898983\n",
      "Training log: 5 epoch (37248 / 60000 train. data). Loss: 0.16868168115615845\n",
      "Training log: 5 epoch (38528 / 60000 train. data). Loss: 0.16694751381874084\n",
      "Training log: 5 epoch (39808 / 60000 train. data). Loss: 0.04495033994317055\n",
      "Training log: 5 epoch (41088 / 60000 train. data). Loss: 0.03542277216911316\n",
      "Training log: 5 epoch (42368 / 60000 train. data). Loss: 0.059707507491111755\n",
      "Training log: 5 epoch (43648 / 60000 train. data). Loss: 0.03304187208414078\n",
      "Training log: 5 epoch (44928 / 60000 train. data). Loss: 0.0834626629948616\n",
      "Training log: 5 epoch (46208 / 60000 train. data). Loss: 0.11035919934511185\n",
      "Training log: 5 epoch (47488 / 60000 train. data). Loss: 0.09799538552761078\n",
      "Training log: 5 epoch (48768 / 60000 train. data). Loss: 0.07025225460529327\n",
      "Training log: 5 epoch (50048 / 60000 train. data). Loss: 0.11064084619283676\n",
      "Training log: 5 epoch (51328 / 60000 train. data). Loss: 0.05542287975549698\n",
      "Training log: 5 epoch (52608 / 60000 train. data). Loss: 0.09594175219535828\n",
      "Training log: 5 epoch (53888 / 60000 train. data). Loss: 0.1273149698972702\n",
      "Training log: 5 epoch (55168 / 60000 train. data). Loss: 0.1469346284866333\n",
      "Training log: 5 epoch (56448 / 60000 train. data). Loss: 0.07449273765087128\n",
      "Training log: 5 epoch (57728 / 60000 train. data). Loss: 0.047521159052848816\n",
      "Training log: 5 epoch (59008 / 60000 train. data). Loss: 0.12025853246450424\n",
      "Test loss (avg): 0.11964009656906129, Accuracy: 0.9632\n",
      "Training log: 6 epoch (128 / 60000 train. data). Loss: 0.06076199933886528\n",
      "Training log: 6 epoch (1408 / 60000 train. data). Loss: 0.08582805097103119\n",
      "Training log: 6 epoch (2688 / 60000 train. data). Loss: 0.19754637777805328\n",
      "Training log: 6 epoch (3968 / 60000 train. data). Loss: 0.1386137306690216\n",
      "Training log: 6 epoch (5248 / 60000 train. data). Loss: 0.06495551019906998\n",
      "Training log: 6 epoch (6528 / 60000 train. data). Loss: 0.03413773328065872\n",
      "Training log: 6 epoch (7808 / 60000 train. data). Loss: 0.04038462042808533\n",
      "Training log: 6 epoch (9088 / 60000 train. data). Loss: 0.053269341588020325\n",
      "Training log: 6 epoch (10368 / 60000 train. data). Loss: 0.08412490785121918\n",
      "Training log: 6 epoch (11648 / 60000 train. data). Loss: 0.1248980164527893\n",
      "Training log: 6 epoch (12928 / 60000 train. data). Loss: 0.06941159814596176\n",
      "Training log: 6 epoch (14208 / 60000 train. data). Loss: 0.08624173700809479\n",
      "Training log: 6 epoch (15488 / 60000 train. data). Loss: 0.07466768473386765\n",
      "Training log: 6 epoch (16768 / 60000 train. data). Loss: 0.06431811302900314\n",
      "Training log: 6 epoch (18048 / 60000 train. data). Loss: 0.17012372612953186\n",
      "Training log: 6 epoch (19328 / 60000 train. data). Loss: 0.10447214543819427\n",
      "Training log: 6 epoch (20608 / 60000 train. data). Loss: 0.060407090932130814\n",
      "Training log: 6 epoch (21888 / 60000 train. data). Loss: 0.1283019781112671\n",
      "Training log: 6 epoch (23168 / 60000 train. data). Loss: 0.12409865856170654\n",
      "Training log: 6 epoch (24448 / 60000 train. data). Loss: 0.06830187141895294\n",
      "Training log: 6 epoch (25728 / 60000 train. data). Loss: 0.15647244453430176\n",
      "Training log: 6 epoch (27008 / 60000 train. data). Loss: 0.06259769201278687\n",
      "Training log: 6 epoch (28288 / 60000 train. data). Loss: 0.02831658348441124\n",
      "Training log: 6 epoch (29568 / 60000 train. data). Loss: 0.06268246471881866\n",
      "Training log: 6 epoch (30848 / 60000 train. data). Loss: 0.09467010945081711\n",
      "Training log: 6 epoch (32128 / 60000 train. data). Loss: 0.11460673063993454\n",
      "Training log: 6 epoch (33408 / 60000 train. data). Loss: 0.2619210481643677\n",
      "Training log: 6 epoch (34688 / 60000 train. data). Loss: 0.1545579433441162\n",
      "Training log: 6 epoch (35968 / 60000 train. data). Loss: 0.05487615987658501\n",
      "Training log: 6 epoch (37248 / 60000 train. data). Loss: 0.07128644734621048\n",
      "Training log: 6 epoch (38528 / 60000 train. data). Loss: 0.07281770557165146\n",
      "Training log: 6 epoch (39808 / 60000 train. data). Loss: 0.10295058786869049\n",
      "Training log: 6 epoch (41088 / 60000 train. data). Loss: 0.0652661994099617\n",
      "Training log: 6 epoch (42368 / 60000 train. data). Loss: 0.15863613784313202\n",
      "Training log: 6 epoch (43648 / 60000 train. data). Loss: 0.05258415639400482\n",
      "Training log: 6 epoch (44928 / 60000 train. data). Loss: 0.15572747588157654\n",
      "Training log: 6 epoch (46208 / 60000 train. data). Loss: 0.013902515172958374\n",
      "Training log: 6 epoch (47488 / 60000 train. data). Loss: 0.1957523226737976\n",
      "Training log: 6 epoch (48768 / 60000 train. data). Loss: 0.09256137162446976\n",
      "Training log: 6 epoch (50048 / 60000 train. data). Loss: 0.03130822256207466\n",
      "Training log: 6 epoch (51328 / 60000 train. data). Loss: 0.05181014537811279\n",
      "Training log: 6 epoch (52608 / 60000 train. data). Loss: 0.06939809024333954\n",
      "Training log: 6 epoch (53888 / 60000 train. data). Loss: 0.04115724563598633\n",
      "Training log: 6 epoch (55168 / 60000 train. data). Loss: 0.05611923336982727\n",
      "Training log: 6 epoch (56448 / 60000 train. data). Loss: 0.0796126127243042\n",
      "Training log: 6 epoch (57728 / 60000 train. data). Loss: 0.025668125599622726\n",
      "Training log: 6 epoch (59008 / 60000 train. data). Loss: 0.0859658420085907\n",
      "Test loss (avg): 0.11983619952201843, Accuracy: 0.9641\n",
      "Training log: 7 epoch (128 / 60000 train. data). Loss: 0.11558538675308228\n",
      "Training log: 7 epoch (1408 / 60000 train. data). Loss: 0.06596847623586655\n",
      "Training log: 7 epoch (2688 / 60000 train. data). Loss: 0.05872045457363129\n",
      "Training log: 7 epoch (3968 / 60000 train. data). Loss: 0.103345587849617\n",
      "Training log: 7 epoch (5248 / 60000 train. data). Loss: 0.08959536999464035\n",
      "Training log: 7 epoch (6528 / 60000 train. data). Loss: 0.05645918473601341\n",
      "Training log: 7 epoch (7808 / 60000 train. data). Loss: 0.032111894339323044\n",
      "Training log: 7 epoch (9088 / 60000 train. data). Loss: 0.0683000385761261\n",
      "Training log: 7 epoch (10368 / 60000 train. data). Loss: 0.10683424025774002\n",
      "Training log: 7 epoch (11648 / 60000 train. data). Loss: 0.12068650126457214\n",
      "Training log: 7 epoch (12928 / 60000 train. data). Loss: 0.03553198650479317\n",
      "Training log: 7 epoch (14208 / 60000 train. data). Loss: 0.0703454464673996\n",
      "Training log: 7 epoch (15488 / 60000 train. data). Loss: 0.07262320071458817\n",
      "Training log: 7 epoch (16768 / 60000 train. data). Loss: 0.06663338840007782\n",
      "Training log: 7 epoch (18048 / 60000 train. data). Loss: 0.03451836109161377\n",
      "Training log: 7 epoch (19328 / 60000 train. data). Loss: 0.06726546585559845\n",
      "Training log: 7 epoch (20608 / 60000 train. data). Loss: 0.06612733751535416\n",
      "Training log: 7 epoch (21888 / 60000 train. data). Loss: 0.12975117564201355\n",
      "Training log: 7 epoch (23168 / 60000 train. data). Loss: 0.0750972181558609\n",
      "Training log: 7 epoch (24448 / 60000 train. data). Loss: 0.01959812268614769\n",
      "Training log: 7 epoch (25728 / 60000 train. data). Loss: 0.07619178295135498\n",
      "Training log: 7 epoch (27008 / 60000 train. data). Loss: 0.05888741463422775\n",
      "Training log: 7 epoch (28288 / 60000 train. data). Loss: 0.12759487330913544\n",
      "Training log: 7 epoch (29568 / 60000 train. data). Loss: 0.10012629628181458\n",
      "Training log: 7 epoch (30848 / 60000 train. data). Loss: 0.05827284976840019\n",
      "Training log: 7 epoch (32128 / 60000 train. data). Loss: 0.12852106988430023\n",
      "Training log: 7 epoch (33408 / 60000 train. data). Loss: 0.043811555951833725\n",
      "Training log: 7 epoch (34688 / 60000 train. data). Loss: 0.07683473825454712\n",
      "Training log: 7 epoch (35968 / 60000 train. data). Loss: 0.06487852334976196\n",
      "Training log: 7 epoch (37248 / 60000 train. data). Loss: 0.03324566408991814\n",
      "Training log: 7 epoch (38528 / 60000 train. data). Loss: 0.019177377223968506\n",
      "Training log: 7 epoch (39808 / 60000 train. data). Loss: 0.09102493524551392\n",
      "Training log: 7 epoch (41088 / 60000 train. data). Loss: 0.08962108194828033\n",
      "Training log: 7 epoch (42368 / 60000 train. data). Loss: 0.06737056374549866\n",
      "Training log: 7 epoch (43648 / 60000 train. data). Loss: 0.08200951665639877\n",
      "Training log: 7 epoch (44928 / 60000 train. data). Loss: 0.019958805292844772\n",
      "Training log: 7 epoch (46208 / 60000 train. data). Loss: 0.011805355548858643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 7 epoch (47488 / 60000 train. data). Loss: 0.02677346020936966\n",
      "Training log: 7 epoch (48768 / 60000 train. data). Loss: 0.05138743668794632\n",
      "Training log: 7 epoch (50048 / 60000 train. data). Loss: 0.055919744074344635\n",
      "Training log: 7 epoch (51328 / 60000 train. data). Loss: 0.07390838116407394\n",
      "Training log: 7 epoch (52608 / 60000 train. data). Loss: 0.05414098873734474\n",
      "Training log: 7 epoch (53888 / 60000 train. data). Loss: 0.1026444137096405\n",
      "Training log: 7 epoch (55168 / 60000 train. data). Loss: 0.1079290509223938\n",
      "Training log: 7 epoch (56448 / 60000 train. data). Loss: 0.10670012980699539\n",
      "Training log: 7 epoch (57728 / 60000 train. data). Loss: 0.09251323342323303\n",
      "Training log: 7 epoch (59008 / 60000 train. data). Loss: 0.03837170451879501\n",
      "Test loss (avg): 0.09189538135528565, Accuracy: 0.972\n",
      "Training log: 8 epoch (128 / 60000 train. data). Loss: 0.12038622796535492\n",
      "Training log: 8 epoch (1408 / 60000 train. data). Loss: 0.06778887659311295\n",
      "Training log: 8 epoch (2688 / 60000 train. data). Loss: 0.03090064600110054\n",
      "Training log: 8 epoch (3968 / 60000 train. data). Loss: 0.012114033102989197\n",
      "Training log: 8 epoch (5248 / 60000 train. data). Loss: 0.03833766281604767\n",
      "Training log: 8 epoch (6528 / 60000 train. data). Loss: 0.024149712175130844\n",
      "Training log: 8 epoch (7808 / 60000 train. data). Loss: 0.02531798556447029\n",
      "Training log: 8 epoch (9088 / 60000 train. data). Loss: 0.02726738527417183\n",
      "Training log: 8 epoch (10368 / 60000 train. data). Loss: 0.09146562963724136\n",
      "Training log: 8 epoch (11648 / 60000 train. data). Loss: 0.02792821265757084\n",
      "Training log: 8 epoch (12928 / 60000 train. data). Loss: 0.059279799461364746\n",
      "Training log: 8 epoch (14208 / 60000 train. data). Loss: 0.043041348457336426\n",
      "Training log: 8 epoch (15488 / 60000 train. data). Loss: 0.008690781891345978\n",
      "Training log: 8 epoch (16768 / 60000 train. data). Loss: 0.17389510571956635\n",
      "Training log: 8 epoch (18048 / 60000 train. data). Loss: 0.05165209248661995\n",
      "Training log: 8 epoch (19328 / 60000 train. data). Loss: 0.10491202026605606\n",
      "Training log: 8 epoch (20608 / 60000 train. data). Loss: 0.10653466731309891\n",
      "Training log: 8 epoch (21888 / 60000 train. data). Loss: 0.056706078350543976\n",
      "Training log: 8 epoch (23168 / 60000 train. data). Loss: 0.020218949764966965\n",
      "Training log: 8 epoch (24448 / 60000 train. data). Loss: 0.07731179893016815\n",
      "Training log: 8 epoch (25728 / 60000 train. data). Loss: 0.05477006360888481\n",
      "Training log: 8 epoch (27008 / 60000 train. data). Loss: 0.03354763984680176\n",
      "Training log: 8 epoch (28288 / 60000 train. data). Loss: 0.03956114500761032\n",
      "Training log: 8 epoch (29568 / 60000 train. data). Loss: 0.10421368479728699\n",
      "Training log: 8 epoch (30848 / 60000 train. data). Loss: 0.06767474114894867\n",
      "Training log: 8 epoch (32128 / 60000 train. data). Loss: 0.051442500203847885\n",
      "Training log: 8 epoch (33408 / 60000 train. data). Loss: 0.03388837352395058\n",
      "Training log: 8 epoch (34688 / 60000 train. data). Loss: 0.05888404697179794\n",
      "Training log: 8 epoch (35968 / 60000 train. data). Loss: 0.035213060677051544\n",
      "Training log: 8 epoch (37248 / 60000 train. data). Loss: 0.02014225721359253\n",
      "Training log: 8 epoch (38528 / 60000 train. data). Loss: 0.045429836958646774\n",
      "Training log: 8 epoch (39808 / 60000 train. data). Loss: 0.061026688665151596\n",
      "Training log: 8 epoch (41088 / 60000 train. data). Loss: 0.02106523886322975\n",
      "Training log: 8 epoch (42368 / 60000 train. data). Loss: 0.07716671377420425\n",
      "Training log: 8 epoch (43648 / 60000 train. data). Loss: 0.03236454352736473\n",
      "Training log: 8 epoch (44928 / 60000 train. data). Loss: 0.0944230705499649\n",
      "Training log: 8 epoch (46208 / 60000 train. data). Loss: 0.11250483244657516\n",
      "Training log: 8 epoch (47488 / 60000 train. data). Loss: 0.02778594195842743\n",
      "Training log: 8 epoch (48768 / 60000 train. data). Loss: 0.013026047497987747\n",
      "Training log: 8 epoch (50048 / 60000 train. data). Loss: 0.03365645185112953\n",
      "Training log: 8 epoch (51328 / 60000 train. data). Loss: 0.08003637194633484\n",
      "Training log: 8 epoch (52608 / 60000 train. data). Loss: 0.07456161081790924\n",
      "Training log: 8 epoch (53888 / 60000 train. data). Loss: 0.04057817533612251\n",
      "Training log: 8 epoch (55168 / 60000 train. data). Loss: 0.045838363468647\n",
      "Training log: 8 epoch (56448 / 60000 train. data). Loss: 0.06915373355150223\n",
      "Training log: 8 epoch (57728 / 60000 train. data). Loss: 0.022911734879016876\n",
      "Training log: 8 epoch (59008 / 60000 train. data). Loss: 0.08457181602716446\n",
      "Test loss (avg): 0.09345129075050354, Accuracy: 0.9723\n",
      "Training log: 9 epoch (128 / 60000 train. data). Loss: 0.009071480482816696\n",
      "Training log: 9 epoch (1408 / 60000 train. data). Loss: 0.04931033402681351\n",
      "Training log: 9 epoch (2688 / 60000 train. data). Loss: 0.032869547605514526\n",
      "Training log: 9 epoch (3968 / 60000 train. data). Loss: 0.05217066779732704\n",
      "Training log: 9 epoch (5248 / 60000 train. data). Loss: 0.016393769532442093\n",
      "Training log: 9 epoch (6528 / 60000 train. data). Loss: 0.023899082094430923\n",
      "Training log: 9 epoch (7808 / 60000 train. data). Loss: 0.02744760736823082\n",
      "Training log: 9 epoch (9088 / 60000 train. data). Loss: 0.05987623333930969\n",
      "Training log: 9 epoch (10368 / 60000 train. data). Loss: 0.04569954052567482\n",
      "Training log: 9 epoch (11648 / 60000 train. data). Loss: 0.05202610790729523\n",
      "Training log: 9 epoch (12928 / 60000 train. data). Loss: 0.042954593896865845\n",
      "Training log: 9 epoch (14208 / 60000 train. data). Loss: 0.03756840154528618\n",
      "Training log: 9 epoch (15488 / 60000 train. data). Loss: 0.05879973620176315\n",
      "Training log: 9 epoch (16768 / 60000 train. data). Loss: 0.042739387601614\n",
      "Training log: 9 epoch (18048 / 60000 train. data). Loss: 0.07569418847560883\n",
      "Training log: 9 epoch (19328 / 60000 train. data). Loss: 0.03823390230536461\n",
      "Training log: 9 epoch (20608 / 60000 train. data). Loss: 0.09281992167234421\n",
      "Training log: 9 epoch (21888 / 60000 train. data). Loss: 0.10704570263624191\n",
      "Training log: 9 epoch (23168 / 60000 train. data). Loss: 0.02299555391073227\n",
      "Training log: 9 epoch (24448 / 60000 train. data). Loss: 0.031851865351200104\n",
      "Training log: 9 epoch (25728 / 60000 train. data). Loss: 0.005911126732826233\n",
      "Training log: 9 epoch (27008 / 60000 train. data). Loss: 0.04203646257519722\n",
      "Training log: 9 epoch (28288 / 60000 train. data). Loss: 0.008828520774841309\n",
      "Training log: 9 epoch (29568 / 60000 train. data). Loss: 0.07755587249994278\n",
      "Training log: 9 epoch (30848 / 60000 train. data). Loss: 0.04158645495772362\n",
      "Training log: 9 epoch (32128 / 60000 train. data). Loss: 0.03576540946960449\n",
      "Training log: 9 epoch (33408 / 60000 train. data). Loss: 0.03428263962268829\n",
      "Training log: 9 epoch (34688 / 60000 train. data). Loss: 0.004037927836179733\n",
      "Training log: 9 epoch (35968 / 60000 train. data). Loss: 0.009135231375694275\n",
      "Training log: 9 epoch (37248 / 60000 train. data). Loss: 0.040082745254039764\n",
      "Training log: 9 epoch (38528 / 60000 train. data). Loss: 0.009824875742197037\n",
      "Training log: 9 epoch (39808 / 60000 train. data). Loss: 0.047168832272291183\n",
      "Training log: 9 epoch (41088 / 60000 train. data). Loss: 0.005960296839475632\n",
      "Training log: 9 epoch (42368 / 60000 train. data). Loss: 0.05970451235771179\n",
      "Training log: 9 epoch (43648 / 60000 train. data). Loss: 0.015037108212709427\n",
      "Training log: 9 epoch (44928 / 60000 train. data). Loss: 0.010894052684307098\n",
      "Training log: 9 epoch (46208 / 60000 train. data). Loss: 0.040705811232328415\n",
      "Training log: 9 epoch (47488 / 60000 train. data). Loss: 0.057690005749464035\n",
      "Training log: 9 epoch (48768 / 60000 train. data). Loss: 0.012891199439764023\n",
      "Training log: 9 epoch (50048 / 60000 train. data). Loss: 0.012340981513261795\n",
      "Training log: 9 epoch (51328 / 60000 train. data). Loss: 0.02022561803460121\n",
      "Training log: 9 epoch (52608 / 60000 train. data). Loss: 0.08918175101280212\n",
      "Training log: 9 epoch (53888 / 60000 train. data). Loss: 0.055658936500549316\n",
      "Training log: 9 epoch (55168 / 60000 train. data). Loss: 0.0934138149023056\n",
      "Training log: 9 epoch (56448 / 60000 train. data). Loss: 0.046388644725084305\n",
      "Training log: 9 epoch (57728 / 60000 train. data). Loss: 0.05049746483564377\n",
      "Training log: 9 epoch (59008 / 60000 train. data). Loss: 0.11550623178482056\n",
      "Test loss (avg): 0.10163369121551513, Accuracy: 0.9711\n",
      "Training log: 10 epoch (128 / 60000 train. data). Loss: 0.012428104877471924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 10 epoch (1408 / 60000 train. data). Loss: 0.10315883904695511\n",
      "Training log: 10 epoch (2688 / 60000 train. data). Loss: 0.060748353600502014\n",
      "Training log: 10 epoch (3968 / 60000 train. data). Loss: 0.031182214617729187\n",
      "Training log: 10 epoch (5248 / 60000 train. data). Loss: 0.042178791016340256\n",
      "Training log: 10 epoch (6528 / 60000 train. data). Loss: 0.07906542718410492\n",
      "Training log: 10 epoch (7808 / 60000 train. data). Loss: 0.01043025217950344\n",
      "Training log: 10 epoch (9088 / 60000 train. data). Loss: 0.04035572335124016\n",
      "Training log: 10 epoch (10368 / 60000 train. data). Loss: 0.013130512088537216\n",
      "Training log: 10 epoch (11648 / 60000 train. data). Loss: 0.0035030245780944824\n",
      "Training log: 10 epoch (12928 / 60000 train. data). Loss: 0.11125534772872925\n",
      "Training log: 10 epoch (14208 / 60000 train. data). Loss: 0.11887209117412567\n",
      "Training log: 10 epoch (15488 / 60000 train. data). Loss: 0.011793320998549461\n",
      "Training log: 10 epoch (16768 / 60000 train. data). Loss: 0.02902100421488285\n",
      "Training log: 10 epoch (18048 / 60000 train. data). Loss: 0.07118941098451614\n",
      "Training log: 10 epoch (19328 / 60000 train. data). Loss: 0.014139845967292786\n",
      "Training log: 10 epoch (20608 / 60000 train. data). Loss: 0.009105686098337173\n",
      "Training log: 10 epoch (21888 / 60000 train. data). Loss: 0.0581120103597641\n",
      "Training log: 10 epoch (23168 / 60000 train. data). Loss: 0.019781246781349182\n",
      "Training log: 10 epoch (24448 / 60000 train. data). Loss: 0.051063284277915955\n",
      "Training log: 10 epoch (25728 / 60000 train. data). Loss: 0.030944757163524628\n",
      "Training log: 10 epoch (27008 / 60000 train. data). Loss: 0.020901218056678772\n",
      "Training log: 10 epoch (28288 / 60000 train. data). Loss: 0.0623132586479187\n",
      "Training log: 10 epoch (29568 / 60000 train. data). Loss: 0.0450449213385582\n",
      "Training log: 10 epoch (30848 / 60000 train. data). Loss: 0.04984206333756447\n",
      "Training log: 10 epoch (32128 / 60000 train. data). Loss: 0.026585590094327927\n",
      "Training log: 10 epoch (33408 / 60000 train. data). Loss: 0.005575317889451981\n",
      "Training log: 10 epoch (34688 / 60000 train. data). Loss: 0.04601898416876793\n",
      "Training log: 10 epoch (35968 / 60000 train. data). Loss: 0.038612861186265945\n",
      "Training log: 10 epoch (37248 / 60000 train. data). Loss: 0.11935612559318542\n",
      "Training log: 10 epoch (38528 / 60000 train. data). Loss: 0.04802050068974495\n",
      "Training log: 10 epoch (39808 / 60000 train. data). Loss: 0.06931944191455841\n",
      "Training log: 10 epoch (41088 / 60000 train. data). Loss: 0.07455184310674667\n",
      "Training log: 10 epoch (42368 / 60000 train. data). Loss: 0.11481961607933044\n",
      "Training log: 10 epoch (43648 / 60000 train. data). Loss: 0.03085467778146267\n",
      "Training log: 10 epoch (44928 / 60000 train. data). Loss: 0.0796218290925026\n",
      "Training log: 10 epoch (46208 / 60000 train. data). Loss: 0.04764491319656372\n",
      "Training log: 10 epoch (47488 / 60000 train. data). Loss: 0.012120181694626808\n",
      "Training log: 10 epoch (48768 / 60000 train. data). Loss: 0.023426953703165054\n",
      "Training log: 10 epoch (50048 / 60000 train. data). Loss: 0.0979280173778534\n",
      "Training log: 10 epoch (51328 / 60000 train. data). Loss: 0.027312543243169785\n",
      "Training log: 10 epoch (52608 / 60000 train. data). Loss: 0.011585302650928497\n",
      "Training log: 10 epoch (53888 / 60000 train. data). Loss: 0.06272035837173462\n",
      "Training log: 10 epoch (55168 / 60000 train. data). Loss: 0.028335988521575928\n",
      "Training log: 10 epoch (56448 / 60000 train. data). Loss: 0.04965446516871452\n",
      "Training log: 10 epoch (57728 / 60000 train. data). Loss: 0.007732175290584564\n",
      "Training log: 10 epoch (59008 / 60000 train. data). Loss: 0.06765514612197876\n",
      "Test loss (avg): 0.08311127707958221, Accuracy: 0.9764\n",
      "Training log: 11 epoch (128 / 60000 train. data). Loss: 0.018937060609459877\n",
      "Training log: 11 epoch (1408 / 60000 train. data). Loss: 0.016649320721626282\n",
      "Training log: 11 epoch (2688 / 60000 train. data). Loss: 0.06164415180683136\n",
      "Training log: 11 epoch (3968 / 60000 train. data). Loss: 0.032036375254392624\n",
      "Training log: 11 epoch (5248 / 60000 train. data). Loss: 0.02196776121854782\n",
      "Training log: 11 epoch (6528 / 60000 train. data). Loss: 0.032357361167669296\n",
      "Training log: 11 epoch (7808 / 60000 train. data). Loss: 0.08960757404565811\n",
      "Training log: 11 epoch (9088 / 60000 train. data). Loss: 0.011865388602018356\n",
      "Training log: 11 epoch (10368 / 60000 train. data). Loss: 0.025544345378875732\n",
      "Training log: 11 epoch (11648 / 60000 train. data). Loss: 0.013419225811958313\n",
      "Training log: 11 epoch (12928 / 60000 train. data). Loss: 0.013438452035188675\n",
      "Training log: 11 epoch (14208 / 60000 train. data). Loss: 0.011325201019644737\n",
      "Training log: 11 epoch (15488 / 60000 train. data). Loss: 0.01688404753804207\n",
      "Training log: 11 epoch (16768 / 60000 train. data). Loss: 0.02549019083380699\n",
      "Training log: 11 epoch (18048 / 60000 train. data). Loss: 0.0731874406337738\n",
      "Training log: 11 epoch (19328 / 60000 train. data). Loss: 0.03179708123207092\n",
      "Training log: 11 epoch (20608 / 60000 train. data). Loss: 0.12101947516202927\n",
      "Training log: 11 epoch (21888 / 60000 train. data). Loss: 0.07237841933965683\n",
      "Training log: 11 epoch (23168 / 60000 train. data). Loss: 0.009397350251674652\n",
      "Training log: 11 epoch (24448 / 60000 train. data). Loss: 0.009643159806728363\n",
      "Training log: 11 epoch (25728 / 60000 train. data). Loss: 0.012786708772182465\n",
      "Training log: 11 epoch (27008 / 60000 train. data). Loss: 0.060860224068164825\n",
      "Training log: 11 epoch (28288 / 60000 train. data). Loss: 0.00585193932056427\n",
      "Training log: 11 epoch (29568 / 60000 train. data). Loss: 0.02050715684890747\n",
      "Training log: 11 epoch (30848 / 60000 train. data). Loss: 0.01671900600194931\n",
      "Training log: 11 epoch (32128 / 60000 train. data). Loss: 0.03593020886182785\n",
      "Training log: 11 epoch (33408 / 60000 train. data). Loss: 0.0231885127723217\n",
      "Training log: 11 epoch (34688 / 60000 train. data). Loss: 0.013151992112398148\n",
      "Training log: 11 epoch (35968 / 60000 train. data). Loss: 0.012086423113942146\n",
      "Training log: 11 epoch (37248 / 60000 train. data). Loss: 0.00650700181722641\n",
      "Training log: 11 epoch (38528 / 60000 train. data). Loss: 0.058770548552274704\n",
      "Training log: 11 epoch (39808 / 60000 train. data). Loss: 0.043293096125125885\n",
      "Training log: 11 epoch (41088 / 60000 train. data). Loss: 0.049216024577617645\n",
      "Training log: 11 epoch (42368 / 60000 train. data). Loss: 0.04816930741071701\n",
      "Training log: 11 epoch (43648 / 60000 train. data). Loss: 0.055715806782245636\n",
      "Training log: 11 epoch (44928 / 60000 train. data). Loss: 0.05258363485336304\n",
      "Training log: 11 epoch (46208 / 60000 train. data). Loss: 0.021125901490449905\n",
      "Training log: 11 epoch (47488 / 60000 train. data). Loss: 0.016410797834396362\n",
      "Training log: 11 epoch (48768 / 60000 train. data). Loss: 0.03701813519001007\n",
      "Training log: 11 epoch (50048 / 60000 train. data). Loss: 0.06013202667236328\n",
      "Training log: 11 epoch (51328 / 60000 train. data). Loss: 0.027867242693901062\n",
      "Training log: 11 epoch (52608 / 60000 train. data). Loss: 0.03276903182268143\n",
      "Training log: 11 epoch (53888 / 60000 train. data). Loss: 0.0175943560898304\n",
      "Training log: 11 epoch (55168 / 60000 train. data). Loss: 0.006249338388442993\n",
      "Training log: 11 epoch (56448 / 60000 train. data). Loss: 0.04188298061490059\n",
      "Training log: 11 epoch (57728 / 60000 train. data). Loss: 0.03267640620470047\n",
      "Training log: 11 epoch (59008 / 60000 train. data). Loss: 0.023745495826005936\n",
      "Test loss (avg): 0.08250986528396606, Accuracy: 0.9769\n",
      "Training log: 12 epoch (128 / 60000 train. data). Loss: 0.024499665945768356\n",
      "Training log: 12 epoch (1408 / 60000 train. data). Loss: 0.05986497923731804\n",
      "Training log: 12 epoch (2688 / 60000 train. data). Loss: 0.06550317257642746\n",
      "Training log: 12 epoch (3968 / 60000 train. data). Loss: 0.04654921218752861\n",
      "Training log: 12 epoch (5248 / 60000 train. data). Loss: 0.021102651953697205\n",
      "Training log: 12 epoch (6528 / 60000 train. data). Loss: 0.033912502229213715\n",
      "Training log: 12 epoch (7808 / 60000 train. data). Loss: 0.06380106508731842\n",
      "Training log: 12 epoch (9088 / 60000 train. data). Loss: 0.01842370443046093\n",
      "Training log: 12 epoch (10368 / 60000 train. data). Loss: 0.030675191432237625\n",
      "Training log: 12 epoch (11648 / 60000 train. data). Loss: 0.02941785752773285\n",
      "Training log: 12 epoch (12928 / 60000 train. data). Loss: 0.01337665319442749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 12 epoch (14208 / 60000 train. data). Loss: 0.05583380535244942\n",
      "Training log: 12 epoch (15488 / 60000 train. data). Loss: 0.06016145274043083\n",
      "Training log: 12 epoch (16768 / 60000 train. data). Loss: 0.041442640125751495\n",
      "Training log: 12 epoch (18048 / 60000 train. data). Loss: 0.03440307825803757\n",
      "Training log: 12 epoch (19328 / 60000 train. data). Loss: 0.00223434716463089\n",
      "Training log: 12 epoch (20608 / 60000 train. data). Loss: 0.045601990073919296\n",
      "Training log: 12 epoch (21888 / 60000 train. data). Loss: 0.0032896846532821655\n",
      "Training log: 12 epoch (23168 / 60000 train. data). Loss: 0.009611498564481735\n",
      "Training log: 12 epoch (24448 / 60000 train. data). Loss: 0.0047171227633953094\n",
      "Training log: 12 epoch (25728 / 60000 train. data). Loss: 0.03409496694803238\n",
      "Training log: 12 epoch (27008 / 60000 train. data). Loss: 0.004252180457115173\n",
      "Training log: 12 epoch (28288 / 60000 train. data). Loss: 0.0075593143701553345\n",
      "Training log: 12 epoch (29568 / 60000 train. data). Loss: 0.0040655843913555145\n",
      "Training log: 12 epoch (30848 / 60000 train. data). Loss: 0.0047268345952034\n",
      "Training log: 12 epoch (32128 / 60000 train. data). Loss: 0.03323967754840851\n",
      "Training log: 12 epoch (33408 / 60000 train. data). Loss: 0.06052584946155548\n",
      "Training log: 12 epoch (34688 / 60000 train. data). Loss: 0.04513052850961685\n",
      "Training log: 12 epoch (35968 / 60000 train. data). Loss: 0.02242623269557953\n",
      "Training log: 12 epoch (37248 / 60000 train. data). Loss: 0.048547711223363876\n",
      "Training log: 12 epoch (38528 / 60000 train. data). Loss: 0.02286456525325775\n",
      "Training log: 12 epoch (39808 / 60000 train. data). Loss: 0.03889937326312065\n",
      "Training log: 12 epoch (41088 / 60000 train. data). Loss: 0.010176140815019608\n",
      "Training log: 12 epoch (42368 / 60000 train. data). Loss: 0.011445064097642899\n",
      "Training log: 12 epoch (43648 / 60000 train. data). Loss: 0.010086610913276672\n",
      "Training log: 12 epoch (44928 / 60000 train. data). Loss: 0.015996061265468597\n",
      "Training log: 12 epoch (46208 / 60000 train. data). Loss: 0.038911573588848114\n",
      "Training log: 12 epoch (47488 / 60000 train. data). Loss: 0.015032097697257996\n",
      "Training log: 12 epoch (48768 / 60000 train. data). Loss: 0.006281483918428421\n",
      "Training log: 12 epoch (50048 / 60000 train. data). Loss: 0.00843525305390358\n",
      "Training log: 12 epoch (51328 / 60000 train. data). Loss: 0.053816404193639755\n",
      "Training log: 12 epoch (52608 / 60000 train. data). Loss: 0.09176162630319595\n",
      "Training log: 12 epoch (53888 / 60000 train. data). Loss: 0.07880109548568726\n",
      "Training log: 12 epoch (55168 / 60000 train. data). Loss: 0.010419875383377075\n",
      "Training log: 12 epoch (56448 / 60000 train. data). Loss: 0.042302537709474564\n",
      "Training log: 12 epoch (57728 / 60000 train. data). Loss: 0.026134364306926727\n",
      "Training log: 12 epoch (59008 / 60000 train. data). Loss: 0.053389277309179306\n",
      "Test loss (avg): 0.08199368405342101, Accuracy: 0.9768\n",
      "Training log: 13 epoch (128 / 60000 train. data). Loss: 0.019954930990934372\n",
      "Training log: 13 epoch (1408 / 60000 train. data). Loss: 0.026950854808092117\n",
      "Training log: 13 epoch (2688 / 60000 train. data). Loss: 0.02632172778248787\n",
      "Training log: 13 epoch (3968 / 60000 train. data). Loss: 0.04389055073261261\n",
      "Training log: 13 epoch (5248 / 60000 train. data). Loss: 0.019380729645490646\n",
      "Training log: 13 epoch (6528 / 60000 train. data). Loss: 0.002571273595094681\n",
      "Training log: 13 epoch (7808 / 60000 train. data). Loss: 0.03869437426328659\n",
      "Training log: 13 epoch (9088 / 60000 train. data). Loss: 0.006830260157585144\n",
      "Training log: 13 epoch (10368 / 60000 train. data). Loss: 0.017850682139396667\n",
      "Training log: 13 epoch (11648 / 60000 train. data). Loss: 0.07030470669269562\n",
      "Training log: 13 epoch (12928 / 60000 train. data). Loss: 0.012741807848215103\n",
      "Training log: 13 epoch (14208 / 60000 train. data). Loss: 0.10017433017492294\n",
      "Training log: 13 epoch (15488 / 60000 train. data). Loss: 0.0007743164896965027\n",
      "Training log: 13 epoch (16768 / 60000 train. data). Loss: 0.002176787704229355\n",
      "Training log: 13 epoch (18048 / 60000 train. data). Loss: 0.022367319092154503\n",
      "Training log: 13 epoch (19328 / 60000 train. data). Loss: 0.011193893849849701\n",
      "Training log: 13 epoch (20608 / 60000 train. data). Loss: 0.018172841519117355\n",
      "Training log: 13 epoch (21888 / 60000 train. data). Loss: 0.030572719871997833\n",
      "Training log: 13 epoch (23168 / 60000 train. data). Loss: 0.006086457520723343\n",
      "Training log: 13 epoch (24448 / 60000 train. data). Loss: 0.0292179137468338\n",
      "Training log: 13 epoch (25728 / 60000 train. data). Loss: 0.0037056244909763336\n",
      "Training log: 13 epoch (27008 / 60000 train. data). Loss: 0.046085987240076065\n",
      "Training log: 13 epoch (28288 / 60000 train. data). Loss: 0.06001994386315346\n",
      "Training log: 13 epoch (29568 / 60000 train. data). Loss: 0.09089598059654236\n",
      "Training log: 13 epoch (30848 / 60000 train. data). Loss: 0.0281386636197567\n",
      "Training log: 13 epoch (32128 / 60000 train. data). Loss: 0.015250727534294128\n",
      "Training log: 13 epoch (33408 / 60000 train. data). Loss: 0.00676380842924118\n",
      "Training log: 13 epoch (34688 / 60000 train. data). Loss: 0.008676297962665558\n",
      "Training log: 13 epoch (35968 / 60000 train. data). Loss: 0.00760781392455101\n",
      "Training log: 13 epoch (37248 / 60000 train. data). Loss: 0.02136901579797268\n",
      "Training log: 13 epoch (38528 / 60000 train. data). Loss: 0.006224852055311203\n",
      "Training log: 13 epoch (39808 / 60000 train. data). Loss: 0.01659715734422207\n",
      "Training log: 13 epoch (41088 / 60000 train. data). Loss: 0.012522468343377113\n",
      "Training log: 13 epoch (42368 / 60000 train. data). Loss: 0.026634681969881058\n",
      "Training log: 13 epoch (43648 / 60000 train. data). Loss: 0.007207058370113373\n",
      "Training log: 13 epoch (44928 / 60000 train. data). Loss: 0.008042611181735992\n",
      "Training log: 13 epoch (46208 / 60000 train. data). Loss: 0.010338833555579185\n",
      "Training log: 13 epoch (47488 / 60000 train. data). Loss: 0.03969931602478027\n",
      "Training log: 13 epoch (48768 / 60000 train. data). Loss: 0.007073491811752319\n",
      "Training log: 13 epoch (50048 / 60000 train. data). Loss: 0.04024096950888634\n",
      "Training log: 13 epoch (51328 / 60000 train. data). Loss: 0.034012146294116974\n",
      "Training log: 13 epoch (52608 / 60000 train. data). Loss: 0.02505214884877205\n",
      "Training log: 13 epoch (53888 / 60000 train. data). Loss: 0.010474026203155518\n",
      "Training log: 13 epoch (55168 / 60000 train. data). Loss: 0.032822415232658386\n",
      "Training log: 13 epoch (56448 / 60000 train. data). Loss: 0.00917036086320877\n",
      "Training log: 13 epoch (57728 / 60000 train. data). Loss: 0.004459258168935776\n",
      "Training log: 13 epoch (59008 / 60000 train. data). Loss: 0.010379031300544739\n",
      "Test loss (avg): 0.08726600470542907, Accuracy: 0.9785\n",
      "Training log: 14 epoch (128 / 60000 train. data). Loss: 0.010874543339014053\n",
      "Training log: 14 epoch (1408 / 60000 train. data). Loss: 0.017819520086050034\n",
      "Training log: 14 epoch (2688 / 60000 train. data). Loss: 0.0062726326286792755\n",
      "Training log: 14 epoch (3968 / 60000 train. data). Loss: 0.00980105996131897\n",
      "Training log: 14 epoch (5248 / 60000 train. data). Loss: 0.015814833343029022\n",
      "Training log: 14 epoch (6528 / 60000 train. data). Loss: 0.020213009789586067\n",
      "Training log: 14 epoch (7808 / 60000 train. data). Loss: 0.007798679172992706\n",
      "Training log: 14 epoch (9088 / 60000 train. data). Loss: 0.035381417721509933\n",
      "Training log: 14 epoch (10368 / 60000 train. data). Loss: 0.002311430871486664\n",
      "Training log: 14 epoch (11648 / 60000 train. data). Loss: 0.00784371793270111\n",
      "Training log: 14 epoch (12928 / 60000 train. data). Loss: 0.0014163292944431305\n",
      "Training log: 14 epoch (14208 / 60000 train. data). Loss: 0.018412195146083832\n",
      "Training log: 14 epoch (15488 / 60000 train. data). Loss: 0.03268086537718773\n",
      "Training log: 14 epoch (16768 / 60000 train. data). Loss: 0.02651521936058998\n",
      "Training log: 14 epoch (18048 / 60000 train. data). Loss: 0.0075564198195934296\n",
      "Training log: 14 epoch (19328 / 60000 train. data). Loss: 0.006146091967821121\n",
      "Training log: 14 epoch (20608 / 60000 train. data). Loss: 0.002403847873210907\n",
      "Training log: 14 epoch (21888 / 60000 train. data). Loss: 0.0021522268652915955\n",
      "Training log: 14 epoch (23168 / 60000 train. data). Loss: 0.011274613440036774\n",
      "Training log: 14 epoch (24448 / 60000 train. data). Loss: 0.008676838129758835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 14 epoch (25728 / 60000 train. data). Loss: 0.011565017513930798\n",
      "Training log: 14 epoch (27008 / 60000 train. data). Loss: 0.0031452812254428864\n",
      "Training log: 14 epoch (28288 / 60000 train. data). Loss: 0.005939159542322159\n",
      "Training log: 14 epoch (29568 / 60000 train. data). Loss: 0.0015690624713897705\n",
      "Training log: 14 epoch (30848 / 60000 train. data). Loss: 0.00849016010761261\n",
      "Training log: 14 epoch (32128 / 60000 train. data). Loss: 0.008233025670051575\n",
      "Training log: 14 epoch (33408 / 60000 train. data). Loss: 0.011952564120292664\n",
      "Training log: 14 epoch (34688 / 60000 train. data). Loss: 0.03571821376681328\n",
      "Training log: 14 epoch (35968 / 60000 train. data). Loss: 0.07836838811635971\n",
      "Training log: 14 epoch (37248 / 60000 train. data). Loss: 0.04621436446905136\n",
      "Training log: 14 epoch (38528 / 60000 train. data). Loss: 0.0028843246400356293\n",
      "Training log: 14 epoch (39808 / 60000 train. data). Loss: 0.013926425948739052\n",
      "Training log: 14 epoch (41088 / 60000 train. data). Loss: 0.04641668498516083\n",
      "Training log: 14 epoch (42368 / 60000 train. data). Loss: 0.030686264857649803\n",
      "Training log: 14 epoch (43648 / 60000 train. data). Loss: 0.054181262850761414\n",
      "Training log: 14 epoch (44928 / 60000 train. data). Loss: 0.1208161786198616\n",
      "Training log: 14 epoch (46208 / 60000 train. data). Loss: 0.00747256726026535\n",
      "Training log: 14 epoch (47488 / 60000 train. data). Loss: 0.01787872426211834\n",
      "Training log: 14 epoch (48768 / 60000 train. data). Loss: 0.021898146718740463\n",
      "Training log: 14 epoch (50048 / 60000 train. data). Loss: 0.01819983869791031\n",
      "Training log: 14 epoch (51328 / 60000 train. data). Loss: 0.032634187489748\n",
      "Training log: 14 epoch (52608 / 60000 train. data). Loss: 0.013052038848400116\n",
      "Training log: 14 epoch (53888 / 60000 train. data). Loss: 0.02023850753903389\n",
      "Training log: 14 epoch (55168 / 60000 train. data). Loss: 0.016352728009223938\n",
      "Training log: 14 epoch (56448 / 60000 train. data). Loss: 0.013553008437156677\n",
      "Training log: 14 epoch (57728 / 60000 train. data). Loss: 0.016596995294094086\n",
      "Training log: 14 epoch (59008 / 60000 train. data). Loss: 0.04753004014492035\n",
      "Test loss (avg): 0.08832593717575073, Accuracy: 0.9776\n",
      "Training log: 15 epoch (128 / 60000 train. data). Loss: 0.025779301300644875\n",
      "Training log: 15 epoch (1408 / 60000 train. data). Loss: 0.004953272640705109\n",
      "Training log: 15 epoch (2688 / 60000 train. data). Loss: 0.03839410841464996\n",
      "Training log: 15 epoch (3968 / 60000 train. data). Loss: 0.007517348974943161\n",
      "Training log: 15 epoch (5248 / 60000 train. data). Loss: 0.0115657988935709\n",
      "Training log: 15 epoch (6528 / 60000 train. data). Loss: 0.005069449543952942\n",
      "Training log: 15 epoch (7808 / 60000 train. data). Loss: 0.079466812312603\n",
      "Training log: 15 epoch (9088 / 60000 train. data). Loss: 0.00660434365272522\n",
      "Training log: 15 epoch (10368 / 60000 train. data). Loss: 0.012138720601797104\n",
      "Training log: 15 epoch (11648 / 60000 train. data). Loss: 0.0020950064063072205\n",
      "Training log: 15 epoch (12928 / 60000 train. data). Loss: 0.009843561798334122\n",
      "Training log: 15 epoch (14208 / 60000 train. data). Loss: 0.03863714635372162\n",
      "Training log: 15 epoch (15488 / 60000 train. data). Loss: 0.03359882906079292\n",
      "Training log: 15 epoch (16768 / 60000 train. data). Loss: 0.029229987412691116\n",
      "Training log: 15 epoch (18048 / 60000 train. data). Loss: 0.04550165683031082\n",
      "Training log: 15 epoch (19328 / 60000 train. data). Loss: 0.05974693223834038\n",
      "Training log: 15 epoch (20608 / 60000 train. data). Loss: 0.009205643087625504\n",
      "Training log: 15 epoch (21888 / 60000 train. data). Loss: 0.03527343273162842\n",
      "Training log: 15 epoch (23168 / 60000 train. data). Loss: 0.006364148110151291\n",
      "Training log: 15 epoch (24448 / 60000 train. data). Loss: 0.013615868985652924\n",
      "Training log: 15 epoch (25728 / 60000 train. data). Loss: 0.017194099724292755\n",
      "Training log: 15 epoch (27008 / 60000 train. data). Loss: 0.07462413609027863\n",
      "Training log: 15 epoch (28288 / 60000 train. data). Loss: 0.002632729709148407\n",
      "Training log: 15 epoch (29568 / 60000 train. data). Loss: 0.01069018803536892\n",
      "Training log: 15 epoch (30848 / 60000 train. data). Loss: 0.02764280140399933\n",
      "Training log: 15 epoch (32128 / 60000 train. data). Loss: 0.01098359003663063\n",
      "Training log: 15 epoch (33408 / 60000 train. data). Loss: 0.010026656091213226\n",
      "Training log: 15 epoch (34688 / 60000 train. data). Loss: 0.011793702840805054\n",
      "Training log: 15 epoch (35968 / 60000 train. data). Loss: 0.0009751059114933014\n",
      "Training log: 15 epoch (37248 / 60000 train. data). Loss: 0.0385444201529026\n",
      "Training log: 15 epoch (38528 / 60000 train. data). Loss: 0.0057013146579265594\n",
      "Training log: 15 epoch (39808 / 60000 train. data). Loss: 0.025913923978805542\n",
      "Training log: 15 epoch (41088 / 60000 train. data). Loss: 0.005372755229473114\n",
      "Training log: 15 epoch (42368 / 60000 train. data). Loss: 0.0023948252201080322\n",
      "Training log: 15 epoch (43648 / 60000 train. data). Loss: 0.006069250404834747\n",
      "Training log: 15 epoch (44928 / 60000 train. data). Loss: 0.004837188869714737\n",
      "Training log: 15 epoch (46208 / 60000 train. data). Loss: 0.012071333825588226\n",
      "Training log: 15 epoch (47488 / 60000 train. data). Loss: 0.00405675545334816\n",
      "Training log: 15 epoch (48768 / 60000 train. data). Loss: 0.004136908799409866\n",
      "Training log: 15 epoch (50048 / 60000 train. data). Loss: 0.05295611172914505\n",
      "Training log: 15 epoch (51328 / 60000 train. data). Loss: 0.021038025617599487\n",
      "Training log: 15 epoch (52608 / 60000 train. data). Loss: 0.003070160746574402\n",
      "Training log: 15 epoch (53888 / 60000 train. data). Loss: 0.008090108633041382\n",
      "Training log: 15 epoch (55168 / 60000 train. data). Loss: 0.005725473165512085\n",
      "Training log: 15 epoch (56448 / 60000 train. data). Loss: 0.0021590441465377808\n",
      "Training log: 15 epoch (57728 / 60000 train. data). Loss: 0.024265676736831665\n",
      "Training log: 15 epoch (59008 / 60000 train. data). Loss: 0.006800495088100433\n",
      "Test loss (avg): 0.09870009329319, Accuracy: 0.977\n",
      "Training log: 16 epoch (128 / 60000 train. data). Loss: 0.03821643441915512\n",
      "Training log: 16 epoch (1408 / 60000 train. data). Loss: 0.0017580613493919373\n",
      "Training log: 16 epoch (2688 / 60000 train. data). Loss: 0.004532821476459503\n",
      "Training log: 16 epoch (3968 / 60000 train. data). Loss: 0.029337041079998016\n",
      "Training log: 16 epoch (5248 / 60000 train. data). Loss: 0.00045809894800186157\n",
      "Training log: 16 epoch (6528 / 60000 train. data). Loss: 0.03646812587976456\n",
      "Training log: 16 epoch (7808 / 60000 train. data). Loss: 0.022240128368139267\n",
      "Training log: 16 epoch (9088 / 60000 train. data). Loss: 0.01624811440706253\n",
      "Training log: 16 epoch (10368 / 60000 train. data). Loss: 0.002043534070253372\n",
      "Training log: 16 epoch (11648 / 60000 train. data). Loss: 0.01232641190290451\n",
      "Training log: 16 epoch (12928 / 60000 train. data). Loss: 0.11572998017072678\n",
      "Training log: 16 epoch (14208 / 60000 train. data). Loss: 0.025702711194753647\n",
      "Training log: 16 epoch (15488 / 60000 train. data). Loss: 0.008776668459177017\n",
      "Training log: 16 epoch (16768 / 60000 train. data). Loss: 0.017866719514131546\n",
      "Training log: 16 epoch (18048 / 60000 train. data). Loss: 0.012361850589513779\n",
      "Training log: 16 epoch (19328 / 60000 train. data). Loss: 0.0064187608659267426\n",
      "Training log: 16 epoch (20608 / 60000 train. data). Loss: 0.03232632204890251\n",
      "Training log: 16 epoch (21888 / 60000 train. data). Loss: 0.03465646505355835\n",
      "Training log: 16 epoch (23168 / 60000 train. data). Loss: 0.06663313508033752\n",
      "Training log: 16 epoch (24448 / 60000 train. data). Loss: 0.0066473521292209625\n",
      "Training log: 16 epoch (25728 / 60000 train. data). Loss: 0.04852307215332985\n",
      "Training log: 16 epoch (27008 / 60000 train. data). Loss: 0.033770352602005005\n",
      "Training log: 16 epoch (28288 / 60000 train. data). Loss: 0.0021417103707790375\n",
      "Training log: 16 epoch (29568 / 60000 train. data). Loss: 0.03297354653477669\n",
      "Training log: 16 epoch (30848 / 60000 train. data). Loss: 0.04309672862291336\n",
      "Training log: 16 epoch (32128 / 60000 train. data). Loss: 0.010033011436462402\n",
      "Training log: 16 epoch (33408 / 60000 train. data). Loss: 0.010409478098154068\n",
      "Training log: 16 epoch (34688 / 60000 train. data). Loss: 0.03491862118244171\n",
      "Training log: 16 epoch (35968 / 60000 train. data). Loss: 0.004292663186788559\n",
      "Training log: 16 epoch (37248 / 60000 train. data). Loss: 0.025551538914442062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 16 epoch (38528 / 60000 train. data). Loss: 0.018658149987459183\n",
      "Training log: 16 epoch (39808 / 60000 train. data). Loss: 0.0238876324146986\n",
      "Training log: 16 epoch (41088 / 60000 train. data). Loss: 0.0035015903413295746\n",
      "Training log: 16 epoch (42368 / 60000 train. data). Loss: 0.043658606708049774\n",
      "Training log: 16 epoch (43648 / 60000 train. data). Loss: 0.0032039061188697815\n",
      "Training log: 16 epoch (44928 / 60000 train. data). Loss: 0.011658705770969391\n",
      "Training log: 16 epoch (46208 / 60000 train. data). Loss: 0.037655141204595566\n",
      "Training log: 16 epoch (47488 / 60000 train. data). Loss: 0.01129651814699173\n",
      "Training log: 16 epoch (48768 / 60000 train. data). Loss: 0.033766429871320724\n",
      "Training log: 16 epoch (50048 / 60000 train. data). Loss: 0.005910497158765793\n",
      "Training log: 16 epoch (51328 / 60000 train. data). Loss: 0.05646355822682381\n",
      "Training log: 16 epoch (52608 / 60000 train. data). Loss: 0.004601515829563141\n",
      "Training log: 16 epoch (53888 / 60000 train. data). Loss: 0.00380779430270195\n",
      "Training log: 16 epoch (55168 / 60000 train. data). Loss: 0.02362919971346855\n",
      "Training log: 16 epoch (56448 / 60000 train. data). Loss: 0.04462290555238724\n",
      "Training log: 16 epoch (57728 / 60000 train. data). Loss: 0.007730498909950256\n",
      "Training log: 16 epoch (59008 / 60000 train. data). Loss: 0.0011087097227573395\n",
      "Test loss (avg): 0.09535251202583313, Accuracy: 0.9789\n",
      "Training log: 17 epoch (128 / 60000 train. data). Loss: 0.0020586512982845306\n",
      "Training log: 17 epoch (1408 / 60000 train. data). Loss: 0.0010006949305534363\n",
      "Training log: 17 epoch (2688 / 60000 train. data). Loss: 0.010280873626470566\n",
      "Training log: 17 epoch (3968 / 60000 train. data). Loss: 0.021786140277981758\n",
      "Training log: 17 epoch (5248 / 60000 train. data). Loss: 0.031000936403870583\n",
      "Training log: 17 epoch (6528 / 60000 train. data). Loss: 0.0020326748490333557\n",
      "Training log: 17 epoch (7808 / 60000 train. data). Loss: 0.0013004764914512634\n",
      "Training log: 17 epoch (9088 / 60000 train. data). Loss: 0.025985825806856155\n",
      "Training log: 17 epoch (10368 / 60000 train. data). Loss: 0.009980741888284683\n",
      "Training log: 17 epoch (11648 / 60000 train. data). Loss: 0.015275023877620697\n",
      "Training log: 17 epoch (12928 / 60000 train. data). Loss: 0.0052293092012405396\n",
      "Training log: 17 epoch (14208 / 60000 train. data). Loss: 0.0025509968400001526\n",
      "Training log: 17 epoch (15488 / 60000 train. data). Loss: 0.00982273742556572\n",
      "Training log: 17 epoch (16768 / 60000 train. data). Loss: 0.0014689043164253235\n",
      "Training log: 17 epoch (18048 / 60000 train. data). Loss: 0.0012982897460460663\n",
      "Training log: 17 epoch (19328 / 60000 train. data). Loss: 0.001118481159210205\n",
      "Training log: 17 epoch (20608 / 60000 train. data). Loss: 0.002485271543264389\n",
      "Training log: 17 epoch (21888 / 60000 train. data). Loss: 0.002358831465244293\n",
      "Training log: 17 epoch (23168 / 60000 train. data). Loss: 0.03230218589305878\n",
      "Training log: 17 epoch (24448 / 60000 train. data). Loss: 0.002486392855644226\n",
      "Training log: 17 epoch (25728 / 60000 train. data). Loss: 0.029480289667844772\n",
      "Training log: 17 epoch (27008 / 60000 train. data). Loss: 0.00573345273733139\n",
      "Training log: 17 epoch (28288 / 60000 train. data). Loss: 0.030759701505303383\n",
      "Training log: 17 epoch (29568 / 60000 train. data). Loss: 0.003527022898197174\n",
      "Training log: 17 epoch (30848 / 60000 train. data). Loss: 0.06581337749958038\n",
      "Training log: 17 epoch (32128 / 60000 train. data). Loss: 0.006398878991603851\n",
      "Training log: 17 epoch (33408 / 60000 train. data). Loss: 0.0068405792117118835\n",
      "Training log: 17 epoch (34688 / 60000 train. data). Loss: 0.014620840549468994\n",
      "Training log: 17 epoch (35968 / 60000 train. data). Loss: 0.017731379717588425\n",
      "Training log: 17 epoch (37248 / 60000 train. data). Loss: 8.144229650497437e-05\n",
      "Training log: 17 epoch (38528 / 60000 train. data). Loss: 0.004784207791090012\n",
      "Training log: 17 epoch (39808 / 60000 train. data). Loss: 0.008391343057155609\n",
      "Training log: 17 epoch (41088 / 60000 train. data). Loss: 0.0038837045431137085\n",
      "Training log: 17 epoch (42368 / 60000 train. data). Loss: 0.0017820186913013458\n",
      "Training log: 17 epoch (43648 / 60000 train. data). Loss: 0.0032270289957523346\n",
      "Training log: 17 epoch (44928 / 60000 train. data). Loss: 0.0010995566844940186\n",
      "Training log: 17 epoch (46208 / 60000 train. data). Loss: 0.058738790452480316\n",
      "Training log: 17 epoch (47488 / 60000 train. data). Loss: 0.035722535103559494\n",
      "Training log: 17 epoch (48768 / 60000 train. data). Loss: 0.06258539110422134\n",
      "Training log: 17 epoch (50048 / 60000 train. data). Loss: 0.008853048086166382\n",
      "Training log: 17 epoch (51328 / 60000 train. data). Loss: 0.01446571946144104\n",
      "Training log: 17 epoch (52608 / 60000 train. data). Loss: 0.0658123716711998\n",
      "Training log: 17 epoch (53888 / 60000 train. data). Loss: 0.0021428540349006653\n",
      "Training log: 17 epoch (55168 / 60000 train. data). Loss: 0.010846637189388275\n",
      "Training log: 17 epoch (56448 / 60000 train. data). Loss: 0.017513064667582512\n",
      "Training log: 17 epoch (57728 / 60000 train. data). Loss: 0.010994421318173409\n",
      "Training log: 17 epoch (59008 / 60000 train. data). Loss: 0.001300375908613205\n",
      "Test loss (avg): 0.09611838862895966, Accuracy: 0.9777\n",
      "Training log: 18 epoch (128 / 60000 train. data). Loss: 0.0031048916280269623\n",
      "Training log: 18 epoch (1408 / 60000 train. data). Loss: 0.036491937935352325\n",
      "Training log: 18 epoch (2688 / 60000 train. data). Loss: 0.005099207162857056\n",
      "Training log: 18 epoch (3968 / 60000 train. data). Loss: 0.0038466639816761017\n",
      "Training log: 18 epoch (5248 / 60000 train. data). Loss: 0.02106771618127823\n",
      "Training log: 18 epoch (6528 / 60000 train. data). Loss: 0.01369507610797882\n",
      "Training log: 18 epoch (7808 / 60000 train. data). Loss: 0.000684596598148346\n",
      "Training log: 18 epoch (9088 / 60000 train. data). Loss: 0.0016010701656341553\n",
      "Training log: 18 epoch (10368 / 60000 train. data). Loss: 0.0072065964341163635\n",
      "Training log: 18 epoch (11648 / 60000 train. data). Loss: 0.03474239632487297\n",
      "Training log: 18 epoch (12928 / 60000 train. data). Loss: 0.04349089786410332\n",
      "Training log: 18 epoch (14208 / 60000 train. data). Loss: 0.02535867691040039\n",
      "Training log: 18 epoch (15488 / 60000 train. data). Loss: 0.011715425178408623\n",
      "Training log: 18 epoch (16768 / 60000 train. data). Loss: 0.023152731359004974\n",
      "Training log: 18 epoch (18048 / 60000 train. data). Loss: 0.0057059116661548615\n",
      "Training log: 18 epoch (19328 / 60000 train. data). Loss: 0.00266261026263237\n",
      "Training log: 18 epoch (20608 / 60000 train. data). Loss: 0.004227925091981888\n",
      "Training log: 18 epoch (21888 / 60000 train. data). Loss: 0.0015697851777076721\n",
      "Training log: 18 epoch (23168 / 60000 train. data). Loss: 0.007874708622694016\n",
      "Training log: 18 epoch (24448 / 60000 train. data). Loss: 0.011224694550037384\n",
      "Training log: 18 epoch (25728 / 60000 train. data). Loss: 0.003351837396621704\n",
      "Training log: 18 epoch (27008 / 60000 train. data). Loss: 0.0011279843747615814\n",
      "Training log: 18 epoch (28288 / 60000 train. data). Loss: 0.002081148326396942\n",
      "Training log: 18 epoch (29568 / 60000 train. data). Loss: 0.002834722399711609\n",
      "Training log: 18 epoch (30848 / 60000 train. data). Loss: 0.0059120915830135345\n",
      "Training log: 18 epoch (32128 / 60000 train. data). Loss: 0.03644757717847824\n",
      "Training log: 18 epoch (33408 / 60000 train. data). Loss: 0.007650740444660187\n",
      "Training log: 18 epoch (34688 / 60000 train. data). Loss: 0.017626382410526276\n",
      "Training log: 18 epoch (35968 / 60000 train. data). Loss: 0.000977586954832077\n",
      "Training log: 18 epoch (37248 / 60000 train. data). Loss: 0.0016845613718032837\n",
      "Training log: 18 epoch (38528 / 60000 train. data). Loss: 0.0007490627467632294\n",
      "Training log: 18 epoch (39808 / 60000 train. data). Loss: 0.0013878121972084045\n",
      "Training log: 18 epoch (41088 / 60000 train. data). Loss: 0.03308266028761864\n",
      "Training log: 18 epoch (42368 / 60000 train. data). Loss: 0.046892303973436356\n",
      "Training log: 18 epoch (43648 / 60000 train. data). Loss: 0.02716059982776642\n",
      "Training log: 18 epoch (44928 / 60000 train. data). Loss: 0.03210567310452461\n",
      "Training log: 18 epoch (46208 / 60000 train. data). Loss: 0.005180351436138153\n",
      "Training log: 18 epoch (47488 / 60000 train. data). Loss: 0.02322009950876236\n",
      "Training log: 18 epoch (48768 / 60000 train. data). Loss: 8.364766836166382e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 18 epoch (50048 / 60000 train. data). Loss: 0.000978086143732071\n",
      "Training log: 18 epoch (51328 / 60000 train. data). Loss: 0.014210289344191551\n",
      "Training log: 18 epoch (52608 / 60000 train. data). Loss: 0.013902902603149414\n",
      "Training log: 18 epoch (53888 / 60000 train. data). Loss: 0.02308919094502926\n",
      "Training log: 18 epoch (55168 / 60000 train. data). Loss: 0.031432874500751495\n",
      "Training log: 18 epoch (56448 / 60000 train. data). Loss: 0.025022834539413452\n",
      "Training log: 18 epoch (57728 / 60000 train. data). Loss: 0.005622752010822296\n",
      "Training log: 18 epoch (59008 / 60000 train. data). Loss: 0.0007462985813617706\n",
      "Test loss (avg): 0.10167348017692567, Accuracy: 0.9774\n",
      "Training log: 19 epoch (128 / 60000 train. data). Loss: 0.0280899815261364\n",
      "Training log: 19 epoch (1408 / 60000 train. data). Loss: 0.0018364563584327698\n",
      "Training log: 19 epoch (2688 / 60000 train. data). Loss: 0.023870307952165604\n",
      "Training log: 19 epoch (3968 / 60000 train. data). Loss: 0.010125109925866127\n",
      "Training log: 19 epoch (5248 / 60000 train. data). Loss: 0.005240790545940399\n",
      "Training log: 19 epoch (6528 / 60000 train. data). Loss: 0.028292881324887276\n",
      "Training log: 19 epoch (7808 / 60000 train. data). Loss: 0.002690281718969345\n",
      "Training log: 19 epoch (9088 / 60000 train. data). Loss: 0.00844104029238224\n",
      "Training log: 19 epoch (10368 / 60000 train. data). Loss: 0.0037085488438606262\n",
      "Training log: 19 epoch (11648 / 60000 train. data). Loss: 0.0005268193781375885\n",
      "Training log: 19 epoch (12928 / 60000 train. data). Loss: 0.0005738697946071625\n",
      "Training log: 19 epoch (14208 / 60000 train. data). Loss: 0.002271253615617752\n",
      "Training log: 19 epoch (15488 / 60000 train. data). Loss: 0.0018886663019657135\n",
      "Training log: 19 epoch (16768 / 60000 train. data). Loss: 0.004533439874649048\n",
      "Training log: 19 epoch (18048 / 60000 train. data). Loss: 0.01577617973089218\n",
      "Training log: 19 epoch (19328 / 60000 train. data). Loss: 0.00652477890253067\n",
      "Training log: 19 epoch (20608 / 60000 train. data). Loss: 0.04355650767683983\n",
      "Training log: 19 epoch (21888 / 60000 train. data). Loss: 0.0028194747865200043\n",
      "Training log: 19 epoch (23168 / 60000 train. data). Loss: 0.03531690686941147\n",
      "Training log: 19 epoch (24448 / 60000 train. data). Loss: 0.0003596469759941101\n",
      "Training log: 19 epoch (25728 / 60000 train. data). Loss: 0.0018964298069477081\n",
      "Training log: 19 epoch (27008 / 60000 train. data). Loss: 0.0013274922966957092\n",
      "Training log: 19 epoch (28288 / 60000 train. data). Loss: 0.003072217106819153\n",
      "Training log: 19 epoch (29568 / 60000 train. data). Loss: 0.026658106595277786\n",
      "Training log: 19 epoch (30848 / 60000 train. data). Loss: 0.003941837698221207\n",
      "Training log: 19 epoch (32128 / 60000 train. data). Loss: 0.004225052893161774\n",
      "Training log: 19 epoch (33408 / 60000 train. data). Loss: 0.014488164335489273\n",
      "Training log: 19 epoch (34688 / 60000 train. data). Loss: 0.0016774684190750122\n",
      "Training log: 19 epoch (35968 / 60000 train. data). Loss: 0.000296764075756073\n",
      "Training log: 19 epoch (37248 / 60000 train. data). Loss: 0.0008220747113227844\n",
      "Training log: 19 epoch (38528 / 60000 train. data). Loss: 0.0010014250874519348\n",
      "Training log: 19 epoch (39808 / 60000 train. data). Loss: 0.0012958049774169922\n",
      "Training log: 19 epoch (41088 / 60000 train. data). Loss: 0.010236185044050217\n",
      "Training log: 19 epoch (42368 / 60000 train. data). Loss: 0.007803475484251976\n",
      "Training log: 19 epoch (43648 / 60000 train. data). Loss: 0.0003741458058357239\n",
      "Training log: 19 epoch (44928 / 60000 train. data). Loss: 0.03314916417002678\n",
      "Training log: 19 epoch (46208 / 60000 train. data). Loss: 0.036990515887737274\n",
      "Training log: 19 epoch (47488 / 60000 train. data). Loss: 0.002395372837781906\n",
      "Training log: 19 epoch (48768 / 60000 train. data). Loss: 0.002716001123189926\n",
      "Training log: 19 epoch (50048 / 60000 train. data). Loss: 0.01215941272675991\n",
      "Training log: 19 epoch (51328 / 60000 train. data). Loss: 0.027146607637405396\n",
      "Training log: 19 epoch (52608 / 60000 train. data). Loss: 0.003022588789463043\n",
      "Training log: 19 epoch (53888 / 60000 train. data). Loss: 0.018347540870308876\n",
      "Training log: 19 epoch (55168 / 60000 train. data). Loss: 0.001051776111125946\n",
      "Training log: 19 epoch (56448 / 60000 train. data). Loss: 0.0019112415611743927\n",
      "Training log: 19 epoch (57728 / 60000 train. data). Loss: 0.01664174348115921\n",
      "Training log: 19 epoch (59008 / 60000 train. data). Loss: 0.004454810172319412\n",
      "Test loss (avg): 0.09148743131160736, Accuracy: 0.9794\n",
      "Training log: 20 epoch (128 / 60000 train. data). Loss: 0.00234169140458107\n",
      "Training log: 20 epoch (1408 / 60000 train. data). Loss: 0.05335593968629837\n",
      "Training log: 20 epoch (2688 / 60000 train. data). Loss: 0.010263074189424515\n",
      "Training log: 20 epoch (3968 / 60000 train. data). Loss: 0.006295297294855118\n",
      "Training log: 20 epoch (5248 / 60000 train. data). Loss: 0.025500841438770294\n",
      "Training log: 20 epoch (6528 / 60000 train. data). Loss: 0.0015864372253417969\n",
      "Training log: 20 epoch (7808 / 60000 train. data). Loss: 0.07182324677705765\n",
      "Training log: 20 epoch (9088 / 60000 train. data). Loss: 0.00418255478143692\n",
      "Training log: 20 epoch (10368 / 60000 train. data). Loss: 0.0014373697340488434\n",
      "Training log: 20 epoch (11648 / 60000 train. data). Loss: 0.0002853609621524811\n",
      "Training log: 20 epoch (12928 / 60000 train. data). Loss: 0.00705442950129509\n",
      "Training log: 20 epoch (14208 / 60000 train. data). Loss: 0.0041051097214221954\n",
      "Training log: 20 epoch (15488 / 60000 train. data). Loss: 0.0011928267776966095\n",
      "Training log: 20 epoch (16768 / 60000 train. data). Loss: 0.00020237639546394348\n",
      "Training log: 20 epoch (18048 / 60000 train. data). Loss: 0.0005712173879146576\n",
      "Training log: 20 epoch (19328 / 60000 train. data). Loss: 0.03376971557736397\n",
      "Training log: 20 epoch (20608 / 60000 train. data). Loss: 0.002406291663646698\n",
      "Training log: 20 epoch (21888 / 60000 train. data). Loss: 0.0031867995858192444\n",
      "Training log: 20 epoch (23168 / 60000 train. data). Loss: 0.011296991258859634\n",
      "Training log: 20 epoch (24448 / 60000 train. data). Loss: 0.0011404715478420258\n",
      "Training log: 20 epoch (25728 / 60000 train. data). Loss: 0.0005444176495075226\n",
      "Training log: 20 epoch (27008 / 60000 train. data). Loss: 0.0014149993658065796\n",
      "Training log: 20 epoch (28288 / 60000 train. data). Loss: 0.00970308855175972\n",
      "Training log: 20 epoch (29568 / 60000 train. data). Loss: 0.00023696571588516235\n",
      "Training log: 20 epoch (30848 / 60000 train. data). Loss: 0.001852773129940033\n",
      "Training log: 20 epoch (32128 / 60000 train. data). Loss: 0.0011634454131126404\n",
      "Training log: 20 epoch (33408 / 60000 train. data). Loss: 0.0005301237106323242\n",
      "Training log: 20 epoch (34688 / 60000 train. data). Loss: 0.020691480487585068\n",
      "Training log: 20 epoch (35968 / 60000 train. data). Loss: 0.0039421916007995605\n",
      "Training log: 20 epoch (37248 / 60000 train. data). Loss: 0.00023024529218673706\n",
      "Training log: 20 epoch (38528 / 60000 train. data). Loss: 0.0019895844161510468\n",
      "Training log: 20 epoch (39808 / 60000 train. data). Loss: 0.0014228560030460358\n",
      "Training log: 20 epoch (41088 / 60000 train. data). Loss: 0.0077294595539569855\n",
      "Training log: 20 epoch (42368 / 60000 train. data). Loss: 0.015034642070531845\n",
      "Training log: 20 epoch (43648 / 60000 train. data). Loss: 0.0013890750706195831\n",
      "Training log: 20 epoch (44928 / 60000 train. data). Loss: 0.007688948884606361\n",
      "Training log: 20 epoch (46208 / 60000 train. data). Loss: 0.013714328408241272\n",
      "Training log: 20 epoch (47488 / 60000 train. data). Loss: 0.005738312378525734\n",
      "Training log: 20 epoch (48768 / 60000 train. data). Loss: 0.012512288987636566\n",
      "Training log: 20 epoch (50048 / 60000 train. data). Loss: 0.005613632500171661\n",
      "Training log: 20 epoch (51328 / 60000 train. data). Loss: 0.011084340512752533\n",
      "Training log: 20 epoch (52608 / 60000 train. data). Loss: 0.000899147242307663\n",
      "Training log: 20 epoch (53888 / 60000 train. data). Loss: 0.01026981696486473\n",
      "Training log: 20 epoch (55168 / 60000 train. data). Loss: 0.004780959337949753\n",
      "Training log: 20 epoch (56448 / 60000 train. data). Loss: 0.0009430311620235443\n",
      "Training log: 20 epoch (57728 / 60000 train. data). Loss: 0.00018919631838798523\n",
      "Training log: 20 epoch (59008 / 60000 train. data). Loss: 0.013156806118786335\n",
      "Test loss (avg): 0.08895174322128296, Accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "for i_epoch in range(num_epoch):\n",
    "    loss = None\n",
    "    \n",
    "    net.train(True)\n",
    "    \n",
    "    for i, (data, target) in enumerate(loaders['train']):\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = f.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(i_epoch+1, (i+1)*128, loss.item()))\n",
    "    \n",
    "    history['train_loss'].append(loss)\n",
    "    \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = net(data)\n",
    "            test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= 10000\n",
    "    print('Test loss (avg): {}, Accuracy: {}'.format(test_loss, correct / 10000))\n",
    "    \n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(correct / 10000)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)], 'test_loss': [0.29929737558364866, 0.2110747040271759, 0.14177264289855956, 0.1473788492202759, 0.11964009656906129, 0.11983619952201843, 0.09189538135528565, 0.09345129075050354, 0.10163369121551513, 0.08311127707958221, 0.08250986528396606, 0.08199368405342101, 0.08726600470542907, 0.08832593717575073, 0.09870009329319, 0.09535251202583313, 0.09611838862895966, 0.10167348017692567, 0.09148743131160736, 0.08895174322128296], 'test_acc': [0.9109, 0.9351, 0.9564, 0.9573, 0.9632, 0.9641, 0.972, 0.9723, 0.9711, 0.9764, 0.9769, 0.9768, 0.9785, 0.9776, 0.977, 0.9789, 0.9777, 0.9774, 0.9794, 0.9814]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEWCAYAAABollyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3xU9Z3/8dcnd0hCCCThFu6oiApIA1pvWG0r2i3W0ovautV213Zbd7v7W7erv+762+W3/my3dq+123W3bLW2tdatlba0avGyoEUTkYvILUEgIVwSEgIJ5P75/XFO6BASMpDLTGbez8djHpw55ztzPnOYvHPynTPfr7k7IiKSuFJiXYCIiAwuBb2ISIJT0IuIJDgFvYhIglPQi4gkOAW9iEiCU9CLiCQ4Bb3EFTPbbWbvH4DnudPM1g5ETSLDnYJeJEbMLDXWNUhyUNBL3DCz7wNTgJ+bWaOZfSVcf7mZvWZmR8xso5ldG/GYO81sl5kdM7N3zexTZnYh8B3gveHzHOllf3eZ2dbwsbvM7PPdtt9sZhvM7KiZVZjZknD9GDP7LzOrNrN6M/tZRC1ruz2Hm9mscPl7ZvZvZrbKzJqA95nZh8zsrXAflWb2N90ef1XEa68M97HQzA6aWVpEu2VmtuEcD70kOnfXTbe4uQG7gfdH3J8EHAZuIjgx+UB4vxDIBo4CF4RtJwAXhct3Amv72NeHgJmAAYuB48CCcNsioCHcX0pYx+xw2y+BHwP5QDqwuLd9Ag7MCpe/Fz7nleFzZgHXApeE9+cCB4GPhO2nAMeA28L9jAXmh9veAW6M2M8zwJ/H+v9Pt/i86Yxe4t2ngVXuvsrdO939BaCMIPgBOoGLzWyEu+939y3RPrG7/9LdKzzwCvA8cHW4+XPACnd/IdzvPnffZmYTgBuBL7h7vbu3hY+N1rPu/mr4nM3u/rK7bw7vbwJ+RPBLB+BTwG/c/Ufhfg67e9dZ+2PhscHMxgA3AD88izokiSjoJd5NBT4edl0cCbthrgImuHsT8EngC8B+M/ulmc2O9onN7EYzW2dmdeHz3gQUhJsnAxU9PGwyUOfu9ef4eiq71XCZmb1kZjVm1kDwWvqqAeAJ4MNmlgN8Aljj7vvPsSZJcAp6iTfdh1OtBL7v7qMjbtnu/jUAd3/O3T9A0G2zDfiPXp7nFGaWCfw38DAwzt1HA6sIunG69juzh4dWAmPMbHQP25qAkRH7GB/F6/shsBKY7O55BJ8t9FUD7r4P+C1wC3AH8P2e2omAgl7iz0FgRsT9rjPXG8ws1cyyzOxaMys2s3FmttTMsoEWoBHoiHieYjPL6GU/GUAmUAO0m9mNwAcjtn8XuMvMrjezFDObZGazw7PmXwHfNrN8M0s3s2vCx2wELjKz+WaWBfxNFK83l+AvhGYzWwTcHrHtB8D7zewTZpZmZmPNbH7E9seBrxD08T8Txb4kSSnoJd48BPxV2E1zr7tXAjcD/5sglCuBvyB476YAfw5UA3UEfdtfDJ/nRWALcMDMarvvxN2PAX8CPAXUEwTsyojtbwB3Af9I8AHqKwTdSBCcQbcR/AVxCPjT8DE7gOXAb4CdQDTX8X8RWG5mx4AHwnq6athL0J305+Hr2wDMi3jsM2FNz4TdWCI9MndNPCIyXJlZBfB5d/9NrGuR+KUzepFhysyWEfT5vxjrWiS+pfXdRETijZm9DMwB7nD3zhiXI3FOXTciIglOXTciIgku7rpuCgoKfNq0abEuQ0RkWHnzzTdr3b2wp21xF/TTpk2jrKws1mWIiAwrZrant23quhERSXAKehGRBKegFxFJcAp6EZEEp6AXEUlwCnoRkQSnoBcRSXBxdx29iEiyOdbcxvNbDtLS3sntl00Z8OdX0IuIxEBzWwcvb69h5cZ9rN56iJb2Ti6dMjp2QW9mS4B/BlKB/+yaxi1i+1RgBVBIMEHCp929Ktz298CHCLqJXgC+7BpJTUSSUEen89uKwzy7YR+/fvsAx1raKcjJ4LZFU/jwvIksmNLTDJX912fQm1kq8AjwAaAKKDWzle7+TkSzh4HH3f0xM7uOYJagO8zsCuBKYG7Ybi3BLEAvD9xLEBGJX+7OhsojPLuhml9s2k9tYws5mWksuXg8S+dN5IqZY0lLHdyPS6M5o18ElLv7LgAze5JgarfIoJ8D/Fm4/BLws3DZgSyC+TkNSCeYy1NEJKHtOHiMlRuqWbmxmr11x8lIS+H62UUsnTeR980uIis9dchqiSboJxHM09mlCrisW5uNwDKC7p1bgFwzG+vuvzWzl4D9BEH/LXff2n0HZnY3cDfAlCkD3z8lIjIUquqP8/ON+3l2wz62HThGisGVswr44+tmccPF4xmVlR6TuqIJeuthXfc+9nuBb5nZncD/APuAdjObBVwIFIftXjCza9z9f055MvdHgUcBSkpK1H8vIsPG4cYWVm3ez7MbqinbUw/Agimj+dulF3HTJRMozM2McYXRBX0VMDnifjFQHdnA3auBjwKYWQ6wzN0bwjP1de7eGG77FXA5wS8DEREA6ptaeWHrQdo6OplXPJoLxueSPsj91v3R2NLO81sO8OyGataW19LR6VwwLpe/uOECls6byOQxI2Nd4imiCfpS4Dwzm05wpn4rcHtkAzMrAOrCuSvvJ7gCB2Av8Idm9hDBXwaLgX8aoNpFJNTc1sG6XYfpdGdmYQ7F+SNJTenpj/H4UdfUyvNbDvDLzft5reIwHZ2/+2M+My2FiyflMa94NPMmB/9OHTsSs9i9pq7LIX++sZrfbA2ueZ80egSfv2YGS+dPZPb4UTGrrS99Br27t5vZPcBzBJdXrnD3LWa2HChz95XAtcBDZuYEZ+tfCh/+NHAdsJmgu+fX7v7zgX8ZIsnnQEMzL247xIvbDrK2vJbmtt/NEZ6RlsKMgmxmFuYwsyiHmYXZzCrKYUZBDiMyhu5DwO7qmlp5bssBVkWE+9SxI7n7mhl86JIJjMpKZ2PVETZWHmFj1RF++MYeVrwavK68EenMmzyaecVB8M+dnEdRbtag1nvK5ZBbDnCsuZ2x2RncunAyS+dPYsGU0TH95ROtuJscvKSkxDXDlMjpOjudzfsaWL3tEKu3HmRL9VEAivNH8P4Lx/G+2UXkZKZScaiJ8ppGKg41UlHTyN6640ScLDNp9AhmFeWEvwSymRX+MhibnTEooXW4sYXnthxk1eb9/HZXEO7Txo7kpksmcNMlE7ho4qhe99ve0cnOQ40ng39DZQM7Dh47efY/MS8rCP/Jo5lXPJppBSPJyUwjOyONlHP8iybycshfbt5PzbHgcsgbLhrPzfOH5nLIc2Fmb7p7SY/bFPQi8auppZ01O2t5cdtBXtxWQ21jCykG75maz/UXjuP62UXMKso5Y0A3t3Ww5/BxKmoaKQ/Dv6KmkYpDTZxo6zjZbvTIdCbnj2TcqCzG52UyflQWRaOyGD8qi/F5WYwblcWorLSofhkcbmzh1+GZ+7pddaeE+4fmTmDOhN7DvS8nWjvYUt3AhsojbKxqYGPlEfbWHT+t3ciMVHIy04JbVhD+OVlpJ9dlZ6aRm5VGdkYqOVnp5GSm8va+o6dcDnndBUXcPH/oL4c8Fwp6kWGksu44L247xOpth1hXcZjWjk5ys9K49oIirp9dxOLzC8nPzuj3fjo7nf1Hm0+e+ZcfamTfkRMcaGjm4NFm6o+3nfaYEempjM/Loig3k/F5wS+BcSd/EWSy7cCx4My94jCdDtMLsrnpkvHcdEn/wr0vdU2tbKw6woGGZhqb22lsCW5NLacuH2tup6m1ncbmdppaOmjt6Dzlebouh1w6b2JML4c8Fwp6kX7o7HSOnGjjcGMLtY2t1Da2nFw+3BS5rpX2jk7SUlNISzHSUo3UlBTSU43UFCM9JYXUcH2wPeWUf1PM2LzvCDsONgIwozCb62cXcd3scZRMyx/yq1Ca2zo4dLSFA0ebOXC0mYMNzactHzraclpYTi/I5kNht8yFE3Ljug+7pb2DppaOk78EikZlUpAT+8shz8WZgl6DmomEVm3ez/o99RxuCoK7K8DrmlpPuSKkS4rBmOxMCnIyGJuTweTJo8lIS6G9o5P2Tqe9w4N/Ozvp6HTaOjpp7+ykuT1iW0e4rbOTjg5nWkE2n1w4hetmFzG9IDsGR+F3stJTmTJ2JFPG9n6poLtTf7zt5F8B4/OymD0+vsM9UmZaKplpqYwZgL+Q4pmCXpKeu/PN53fwrZfKyUpPoSAnOKubNDqLuZPyKMjNYGx2JgW5mRRkZzA2Jwj30SMz4v4SxsFmZozJzmBMdgZzJsbv5YXJTkEvSa2j03ng2bf5wet7uW3RFP7uIxcnfXhL4lHQS9Jqbe/kfz21gV9s2s8Xr53JX9xwwbDpchA5Gwp6SUrHW9v5oyfW88qOGv73TbO5+5qZsS5JZNAo6CXpNBxv47OPlfLW3nq+vuwSPrlQI6ZKYlPQS1I5dLSZ31/xBrtqmvj2pxaw5OIJsS5JZNAp6CVp7D18nE9/93VqG1tYcedCrjqvINYliQwJBb0khe0HjnHHd1+npb2TH/zBZVw6JT/WJYkMGQW9JLz1e+u5679KyUpP4SdfeC/nj8uNdUkiQ0pBLwltzc4a7n78TYpGZfLE5y6LuwkhRIaCgl4S1qrN+/nyk28xszCHxz+3aNDHLheJVwp6GTANJ9qorDt+yoiBvxtBsCMcMbCnbcHteEsH4/OymFucx/xwjPGLJ+ad00QZP3pjL199ZjMLpuTz3TsXkjdi+IxCKDLQFPQyIPYePs6Hv7WWhhOnD20LYEYwHnhmGtmZvxv/uyBnJNnh+OAjMlKprDvOW3uP8ItN+wFITTHOH5cbzCoUTi5x/ricM0788G8vV/D1X2/j2gsK+bdPvSemMyqJxAMFvfRbW0cnf/LkW3S6863bLyV/ZMapEztkpjEyPfWsZvw5dKyZTZUN4axCR/jV2wd4srQSgKz0FC6ZlMfc4uCsf37xaCaPGQHA1369jX9/ZRcfnjeRb358Hhlp8TcTkMhQ03j00m/feG4bj7xUwbduv5TfmztxUPbh7uw5fPxk8G+sPMKW6qO0tAdjoeePTGfi6BFsqT7Kpy+fwt8u1eBkklz6PR69mS0B/plgcvD/dPevdds+FVgBFAJ1wKfdvcrM3gf8Y0TT2cCt7v6zs38ZEo9eq6jl2y9X8ImS4kELeQiGw51WkM20gmxunj8JCP6S2H7g2MnJpLfuP8a9HzyfL71vlgYnE4nQ5xm9maUCO4APAFVAKXCbu78T0eYnwC/c/TEzuw64y93v6PY8Y4ByoNjdT5/gMaQz+uGjvqmVG/95DSMzU/nFH1/FyAz1BIrEypnO6KPpwFwElLv7LndvBZ4Ebu7WZg6wOlx+qYftAB8DfnWmkJfhw935yn9voq6plX+59VKFvEgciyboJwGVEferwnWRNgLLwuVbgFwzG9utza3Aj3ragZndbWZlZlZWU1MTRUkSa0+s28ML7xzkK0su4OJJebEuR0TOIJqg76mzs3t/z73AYjN7C1gM7APaTz6B2QTgEuC5nnbg7o+6e4m7lxQWFkZVuMTO9gPH+LtfbmXx+YV89srpsS5HRPoQzd/bVcDkiPvFQHVkA3evBj4KYGY5wDJ3b4ho8gngGXfv+SJrGTaa2zr44x+tJzcrnYc/Pu+sLpkUkdiI5oy+FDjPzKabWQZBF8zKyAZmVmBmXc91P8EVOJFuo5duGxleHvzlVnYcbOSbn5hHYW5mrMsRkSj0GfTu3g7cQ9DtshV4yt23mNlyM1saNrsW2G5mO4BxwINdjzezaQR/EbwyoJXLkHt+ywG+v24Pf3DVdBafry42keFCX5iSqBxoaGbJP/8Pxfkj+O8/uoLMNA0rIBJP+nt5pSS5jk7nz368gdb2Tv7l1ksV8iLDjC5+lj5955UKfrvrMH//sbnMKMyJdTkicpZ0Ri9ntH5vPf/wwg5+b+4EPv6e4liXIyLnQEEvvTra3MaXn3yL8aOyePCWSzR+jMgwpa4b6ZG789c/e5vqI8089fnLNXGHyDCmM3rp0U/X7+PZDdV8+frzeM/UMbEuR0T6QUEvp9ld28QDz77Noulj+NL7ZsW6HBHpJwW9nKK1PZgtKi01hX/65HxN3iGSANRHL6f45gvb2VTVwHc+/R4mjh4R63JEZADojF5OWrOzhn9/ZRe3XzaFJRePj3U5IjJAdEafhJrbOthV00RFTSMVNY2UH2qkoqaJikONnFeUw19/aE6sSxSRAaSgT2CHG1uCAD8Z5sGtqv4EXUMcmcHk/JHMLMzmyplj+cwV0xiRoSEORBKJgj5BuDs/Xb+PN96tOxno9cd/N/x/VnoKMwpymD85n2ULiplVlMPMwhymF2STla5gF0lkCvoE4O5847ntfPvlCsZkZzCrMIclF09gZmE2M4tymFWYw6TRIzRJiEiSUtAngH99sZxvv1zBbYum8P9uuVhDFYjIKXTVzTD3nVcq+IcXdrBsQTEPfkQhLyKnU9APY//16rt87Vfb+PC8ifz9x+aqa0ZEeqSgH6Z++Ppe/vbn73DDReP4h0/M0zdYRaRXCvph6Ok3q/jqzzZz3ewi/vW2BaSn6r9RRHoXVUKY2RIz225m5WZ2Xw/bp5rZajPbZGYvm1lxxLYpZva8mW01s3fCycLlHK3cWM1Xnt7IlTML+PanFpCRppAXkTPrMyXMLBV4BLgRmAPcZmbdvzr5MPC4u88FlgMPRWx7HPiGu18ILAIODUThyejXbx/gz368gZJpY/iP3y/R9e8iEpVoTgcXAeXuvsvdW4EngZu7tZkDrA6XX+raHv5CSHP3FwDcvdHdjw9I5UnmxW0H+eMfrWducR4r7lyob6+KSNSiCfpJQGXE/apwXaSNwLJw+RYg18zGAucDR8zsp2b2lpl9I/wL4RRmdreZlZlZWU1Nzdm/igS3ZmcNX3hiPbPHj+J7dy0iJ1NffxCR6EUT9D1dzuHd7t8LLDazt4DFwD6gneALWVeH2xcCM4A7T3sy90fdvcTdSwoLC6OvPgms23WYP3y8jBkF2Tz+2UWa0k9Ezlo0QV8FTI64XwxURzZw92p3/6i7Xwp8NVzXED72rbDbpx34GbBgQCpPAm/uqeOz3yulOH8kT/zBZeRnZ8S6JBEZhqIJ+lLgPDObbmYZwK3AysgGZlZgZl3PdT+wIuKx+WbWdZp+HfBO/8tOfJuqjnDnilKKcjP54R9cRkFOZqxLEpFhqs+gD8/E7wGeA7YCT7n7FjNbbmZLw2bXAtvNbAcwDngwfGwHQbfNajPbTNAN9B8D/ioSzDvVR7nju2+QNzKdH/7h5RSNyop1SSIyjJl79+722CopKfGysrJYlxEzOw8e45OPriMzLYWnPv9eJo8ZGeuSRGQYMLM33b2kp236tk0c2VXTyO3/+TqpKcYP//ByhbyIDAhdpxcDnZ3OviMnwglCmk7O/vRO9VEy0lL48d2XM70gO9ZlikiCUNAPoua2Dt6tDedmPdREeU0jFYca2VXbSHNb58l2eSPSmVWUw4cumcDnrp7OeeNyY1i1iCQaBf0A+vXbB3hzT93JybYr64+fMjfrpNEjmFWUw3tnjmVmYQ4zC7OZVZTDmOwMjSMvIoNGQT9A9h05wReeeJPMtBRmFOYwtziPWy6ddHIqv+kF2Rq2QERiQkE/QNbuDIZuWHnPVVwwXl0vIhI/dNXNAFmzs5ai3EzOH5cT61JERE6hoB8AnZ3OaxWHuWpWgfraRSTuKOgHwDv7j1LX1MqVswpiXYqIyGkU9ANgbXktAFedp6AXkfijoB8Ar5bXcv64HMZpTBoRiUMK+n5qbuvgjXfr1G0jInFLQd9Pb+6pp6W9k6vVbSMicUpB309rdtaSlmJcNn1srEsREemRgr6f1pbXsGBKPtmax1VE4pSCvh/qmlrZUn1UV9uISFxT0PfDaxW1uOuyShGJbwr6fli7s5bcrDTmTsqLdSkiIr1S0J8jd2fNzlreO2Msaak6jCISv6JKKDNbYmbbzazczO7rYftUM1ttZpvM7GUzK47Y1mFmG8LbyoEsPpb2HD7OviMndFmliMS9Pi8VMbNU4BHgA0AVUGpmK939nYhmDwOPu/tjZnYd8BBwR7jthLvPH+C6Y25NOOyBviglIvEumjP6RUC5u+9y91bgSeDmbm3mAKvD5Zd62J5wXt1Zy6TRIzS3q4jEvWiCfhJQGXG/KlwXaSOwLFy+Bcg1s65vEGWZWZmZrTOzj/S0AzO7O2xTVlNTcxblx0ZHp/NaRa2GJRaRYSGaoO8pybzb/XuBxWb2FrAY2Ae0h9umuHsJcDvwT2Y287Qnc3/U3UvcvaSwsDD66mNkU9URjja3c6X650VkGIjm65xVwOSI+8VAdWQDd68GPgpgZjnAMndviNiGu+8ys5eBS4GKflceQ6929c/P1LAHIhL/ojmjLwXOM7PpZpYB3AqccvWMmRWYWddz3Q+sCNfnm1lmVxvgSiDyQ9xhac3OWi6aOIqxOZmxLkVEpE99Br27twP3AM8BW4Gn3H2LmS03s6Vhs2uB7Wa2AxgHPBiuvxAoM7ONBB/Sfq3b1TrDTlNLO+v31nOVrrYRkWEiqpG43H0VsKrbugcilp8Gnu7hca8Bl/Szxrjyxu462jpcwx6IyLChr3SepbU7a8lIS2HhtDGxLkVEJCoK+rO0dmctC6flk5WeGutSRESioqA/C4eONbP94DGumhX/l4CKiHRR0J+FrssqNb6NiAwnCvqzsGZnLfkj05kzYVSsSxERiZqCPkruzqvltVwxq4CUFA17ICLDh4I+SuWHGjl4tIWrdf28iAwzCvoordmpYYlFZHhS0Efp1fJapo0dyeQxI2NdiojIWVHQR6Gto5N1uw7r27AiMiwp6KOwofIITa0dGt9GRIYlBX0U1uysJcXgvTMV9CIy/Cjoo7B2Zw1zi0eTNyI91qWIiJw1BX0fjja3sbGqQd02IjJsKej7sK7iMB2dGpZYRIYvBX0f1pbXMiI9lQVT8mNdiojIOVHQ92FteS2XzRhDRpoOlYgMT0qvM6g+coJdNU3qnxeRYU1BfwZrd3YNS6zx50Vk+Ioq6M1siZltN7NyM7uvh+1TzWy1mW0ys5fNrLjb9lFmts/MvjVQhQ+FteW1FOZmcv64nFiXIiJyzvoMejNLBR4BbgTmALeZ2ZxuzR4GHnf3ucBy4KFu2/8v8Er/yx06nZ3BsMRXzSrATMMSi8jwFc0Z/SKg3N13uXsr8CRwc7c2c4DV4fJLkdvN7D3AOOD5/pc7dLYeOMrhplb1z4vIsBdN0E8CKiPuV4XrIm0EloXLtwC5ZjbWzFKAbwJ/caYdmNndZlZmZmU1NTXRVT7IuqYN1LDEIjLcRRP0PfVbeLf79wKLzewtYDGwD2gHvgiscvdKzsDdH3X3EncvKSyMjw8+1+ys5byiHMbnZcW6FBGRfkmLok0VMDnifjFQHdnA3auBjwKYWQ6wzN0bzOy9wNVm9kUgB8gws0Z3P+0D3XjS3NbBG+/WcftlU2JdiohIv0UT9KXAeWY2neBM/Vbg9sgGZlYA1Ll7J3A/sALA3T8V0eZOoCTeQx5g/Z56Wto71T8vIgmhz64bd28H7gGeA7YCT7n7FjNbbmZLw2bXAtvNbAfBB68PDlK9Q2JNeS1pKcZlM8bGuhQRkX6L5owed18FrOq27oGI5aeBp/t4ju8B3zvrCmNg7c5aFkzJJyczqsMjIhLX9M3YbuqbWnm7ukFX24hIwlDQd/NaxWHc0bDEIpIwFPTdrC2vITczjXnFebEuRURkQCjou1lbXsvlM8eSlqpDIyKJQWkWYc/hJirrTnC1um1EJIEo6COsCYcl1vXzIpJIFPQRXi2vZWJeFtMLsmNdiojIgFHQhzo6ndcqDnPVeRqWWEQSi4I+tO3AURpOtHHFTHXbiEhiUdCHSt+tA2Dh9DExrkREZGAp6EOle+qZmJfFpNEjYl2KiMiAUtAD7k7pu3U6mxeRhKSgByrrTnDoWAsl0xT0IpJ4FPRA6e6gf36Rgl5EEpCCniDo80akc15RTqxLEREZcAp6gqAvmZpPSoqunxeRxJP0QX+4sYWKmib1z4tIwkr6oC/bUw/Aoun5Ma5ERGRwJH3Ql75bR0ZaChdP0vjzIpKYogp6M1tiZtvNrNzM7uth+1QzW21mm8zsZTMrjlj/ppltMLMtZvaFgX4B/VW6p575xaPJTEuNdSkiIoOiz6A3s1TgEeBGYA5wm5nN6dbsYeBxd58LLAceCtfvB65w9/nAZcB9ZjZxoIrvr+Ot7WzZ18BCdduISAKL5ox+EVDu7rvcvRV4Eri5W5s5wOpw+aWu7e7e6u4t4frMKPc3ZDbsPUJ7p+uDWBFJaNEE7ySgMuJ+Vbgu0kZgWbh8C5BrZmMBzGyymW0Kn+Pr7l7dfQdmdreZlZlZWU1Nzdm+hnNWurseM3jPVJ3Ri0jiiiboe7q43LvdvxdYbGZvAYuBfUA7gLtXhl06s4DPmNm4057M/VF3L3H3ksLCwrN6Af1RuruO2eNHMSorfcj2KSIy1KIJ+ipgcsT9YuCUs3J3r3b3j7r7pcBXw3UN3dsAW4Cr+1XxAGnv6GT93noWTdPZvIgktmiCvhQ4z8ymm1kGcCuwMrKBmRWYWddz3Q+sCNcXm9mIcDkfuBLYPlDF98c7+49yvLVD/fMikvD6DHp3bwfuAZ4DtgJPufsWM1tuZkvDZtcC281sBzAOeDBcfyHwupltBF4BHnb3zQP8Gs5J6e7gi1ILFfQikuDSomnk7quAVd3WPRCx/DTwdA+PewGY288aB0Xpu3VMHjOC8XlZsS5FRGRQxdXljkPF3SnbU8fCqTqbF5HEl5RB/25tE7WNrZpRSkSSQlIGfdnJ/nldcSMiiS8pg750dx35I9OZWaiJRkQk8SVt0JdMG4OZJhoRkcSXdEF/6Fgzuw8fV7eNiCSNpAv6Ml0/LyJJJumCvnR3HVnpKVw0URONiEhySMqgv3RyPhlpSffSRSRJJVXaNba08071UfXPi0hSSaqgX7+nnk5HX5QSkaSSVEFftruOFINLp+iMXkSSR1IFfenueuZMHEVOZjjLuD8AAAuqSURBVFRjuYmIJISkCfrW9k7eqqzXZZUiknSSJui3VDfQ3NapoBeRpJM0QV+6uw6AEl1xIyJJJomCvp5pY0dSlKuJRkQkuSRF0Hd2OmW769RtIyJJKSmCfldtI/XH2xT0IpKUogp6M1tiZtvNrNzM7uth+1QzW21mm8zsZTMrDtfPN7PfmtmWcNsnB/oFROONd4OBzNQ/LyLJqM+gN7NU4BHgRmAOcJuZzenW7GHgcXefCywHHgrXHwd+390vApYA/2Rmoweq+GiV7a6jICeD6QXZQ71rEZGYi+aMfhFQ7u673L0VeBK4uVubOcDqcPmlru3uvsPdd4bL1cAhoHAgCj8bpXvqKJmqiUZEJDlFE/STgMqI+1XhukgbgWXh8i1ArpmNjWxgZouADKCi+w7M7G4zKzOzspqammhrj8qBhmYq605ofBsRSVrRBH1Pp8He7f69wGIzewtYDOwD2k8+gdkE4PvAXe7eedqTuT/q7iXuXlJYOLAn/F3Xz2vEShFJVtEM+lIFTI64XwxURzYIu2U+CmBmOcAyd28I748Cfgn8lbuvG4iiz0bp7jpGZqQyZ8Kood61iEhciOaMvhQ4z8ymm1kGcCuwMrKBmRWYWddz3Q+sCNdnAM8QfFD7k4ErO3qlu+tZMCWftNSkuJJUROQ0faafu7cD9wDPAVuBp9x9i5ktN7OlYbNrge1mtgMYBzwYrv8EcA1wp5ltCG/zB/pF9KbhRBvbDhzVZZUiktSiGq/X3VcBq7qteyBi+Wng6R4e9wTwRD9rPGfr99bjDov0RSkRSWIJ3Z9R+m4daSnG/ClDfum+iEjcSOigL9tdz0WT8hiZoYlGRCR5JWzQt7R3sKHqCAunqn9eRJJbwgb95qoGWts79UUpEUl6CRv0pbvDgcx0Ri8iSS6Bg76OGYXZjM3JjHUpIiIxlZBB3zXRiC6rFBFJ0KDfcegYR5vbKVHQi4gkZtB39c/rjF5EJFGD/t06inIzmTxmRKxLERGJuYQM+rLddSycrolGREQgAYN+35ETVDc064tSIiKhhAv60neDiUb0QayISCDxgn53HTmZaVyoiUZERIAEDfoFU/NJTVH/vIgIJFjQHzneyo6DjSzSRCMiIiclVNCXdY1vo/55EZGTEiroS/fUkZ5qzJ+siUZERLokVNCX7a7nkkl5ZKWnxroUEZG4EVXQm9kSM9tuZuVmdl8P26ea2Woz22RmL5tZccS2X5vZETP7xUAW3l1zWwebqo6wUN02IiKn6DPozSwVeAS4EZgD3GZmc7o1exh43N3nAsuBhyK2fQO4Y2DK7d3R5jZuvHgC15xfONi7EhEZVqI5o18ElLv7LndvBZ4Ebu7WZg6wOlx+KXK7u68Gjg1ArWdUlJvFv9x2KVfOKhjsXYmIDCvRBP0koDLiflW4LtJGYFm4fAuQa2Zjoy3CzO42szIzK6upqYn2YSIiEoVogr6nbx55t/v3AovN7C1gMbAPaI+2CHd/1N1L3L2ksFBdLyIiAyktijZVwOSI+8VAdWQDd68GPgpgZjnAMndvGKgiRUTk3EVzRl8KnGdm080sA7gVWBnZwMwKzKzrue4HVgxsmSIicq76DHp3bwfuAZ4DtgJPufsWM1tuZkvDZtcC281sBzAOeLDr8Wa2BvgJcL2ZVZnZDQP8GkRE5AzMvXt3e2yVlJR4WVlZrMsQERlWzOxNdy/paVtCfTNWREROp6AXEUlwcdd1Y2Y1wJ5+PEUBUDtA5QwG1dc/qq9/VF//xHN9U929x+vT4y7o+8vMynrrp4oHqq9/VF//qL7+iff6eqOuGxGRBKegFxFJcIkY9I/GuoA+qL7+UX39o/r6J97r61HC9dGLiMipEvGMXkREIijoRUQS3LAM+iimNsw0sx+H2183s2lDWNtkM3vJzLaa2RYz+3IPba41swYz2xDeHhiq+iJq2G1mm8P9nzbmhAX+JTyGm8xswRDWdkHEsdlgZkfN7E+7tRnSY2hmK8zskJm9HbFujJm9YGY7w3/ze3nsZ8I2O83sM0NY3zfMbFv4//eMmY3u5bFnfC8MYn1/Y2b7Iv4Pb+rlsWf8eR/E+n4cUdtuM9vQy2MH/fj1m7sPqxuQClQAM4AMgklP5nRr80XgO+HyrcCPh7C+CcCCcDkX2NFDfdcCv4jxcdwNFJxh+03ArwjmI7gceD2G/98HCL4MErNjCFwDLADejlj398B94fJ9wNd7eNwYYFf4b364nD9E9X0QSAuXv95TfdG8Fwaxvr8B7o3i//+MP++DVV+37d8EHojV8evvbTie0UczteHNwGPh8tMEI2f2NIHKgHP3/e6+Plw+RjDiZ/cZuYaDmwnmAXZ3XweMNrMJMajjeqDC3fvzbel+c/f/Aeq6rY58nz0GfKSHh94AvODude5eD7wALBmK+tz9eQ9GnwVYRzCXREz0cvyiEc3Pe7+dqb4wOz4B/Gig9ztUhmPQRzO14ck24Ru9AYh6asOBEnYZXQq83sPm95rZRjP7lZldNKSFBRx43szeNLO7e9gezXEeCrfS+w9YrI/hOHffD8EveKCohzbxchw/S/AXWk/6ei8MpnvCrqUVvXR9xcPxuxo46O47e9key+MXleEY9NFMbRhNm0FlwUxb/w38qbsf7bZ5PUFXxDzgX4GfDWVtoSvdfQFwI/AlM7um2/Z4OIYZwFKC+Qy6i4djGI14OI5fJZja8we9NOnrvTBY/g2YCcwH9hN0j3QX8+MH3MaZz+ZjdfyiNhyDvs+pDSPbmFkakMe5/dl4TswsnSDkf+DuP+2+3d2PuntjuLwKSDezgqGqL9xvdfjvIeAZgj+RI0VznAfbjcB6dz/YfUM8HEPgYFd3VvjvoR7axPQ4hh/+/h7wKQ87lLuL4r0wKNz9oLt3uHsn8B+97DfWxy+NYJrUH/fWJlbH72wMx6Dvc2rD8H7X1Q0fA17s7U0+0ML+vO8CW939H3ppM77rMwMzW0Tw/3B4KOoL95ltZrldywQf2r3drdlK4PfDq28uBxq6uimGUK9nUrE+hqHI99lngGd7aPMc8EEzyw+7Jj4Yrht0ZrYE+Etgqbsf76VNNO+Fwaov8jOfW3rZbzQ/74Pp/cA2d6/qaWMsj99ZifWnwedyI7giZAfBp/FfDdctJ3hDA2QR/LlfDrwBzBjC2q4i+NNyE7AhvN0EfAH4QtjmHmALwRUE64Arhvj4zQj3vTGso+sYRtZowCPhMd4MlAxxjSMJgjsvYl3MjiHBL5z9QBvBWebnCD73WQ3sDP8dE7YtAf4z4rGfDd+L5cBdQ1hfOUH/dtf7sOtKtInAqjO9F4aovu+H761NBOE9oXt94f3Tft6Hor5w/fe63nMRbYf8+PX3piEQREQS3HDsuhERkbOgoBcRSXAKehGRBKegFxFJcAp6EZEEp6AXGUDhqJq/iHUdIpEU9CIiCU5BL0nJzD5tZm+EY4j/u5mlmlmjmX3TzNab2WozKwzbzjezdRHjuueH62eZ2W/CgdXWm9nM8OlzzOzpcCz4HwzVyKkivVHQS9IxswuBTxIMRjUf6AA+BWQTjK2zAHgF+D/hQx4H/tLd5xJ8k7Nr/Q+ARzwYWO0Kgm9WQjBi6Z8Ccwi+OXnloL8okTNIi3UBIjFwPfAeoDQ82R5BMCBZJ78bvOoJ4KdmlgeMdvdXwvWPAT8JxzeZ5O7PALh7M0D4fG94ODZKOCvRNGDt4L8skZ4p6CUZGfCYu99/ykqzv+7W7kzjg5ypO6YlYrkD/ZxJjKnrRpLRauBjZlYEJ+d+nUrw8/CxsM3twFp3bwDqzezqcP0dwCsezDFQZWYfCZ8j08xGDumrEImSzjQk6bj7O2b2VwSzAqUQjFj4JaAJuMjM3iSYleyT4UM+A3wnDPJdwF3h+juAfzez5eFzfHwIX4ZI1DR6pUjIzBrdPSfWdYgMNHXdiIgkOJ3Ri4gkOJ3Ri4gkOAW9iEiCU9CLiCQ4Bb2ISIJT0IuIJLj/D2Kom+MWlhx4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epoch\n",
    "print(history)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['train_loss'])), history['train_loss'], label='train_loss')\n",
    "plt.plot(range(len(history['test_loss'])), history['test_loss'], label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss2.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['test_acc'])), history['test_acc'])\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('test_acc2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
