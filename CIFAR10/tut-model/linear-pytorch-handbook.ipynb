{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CIFAR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # Normalize(平均, 偏差)\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3*32*32, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return f.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = LinearNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_loader, test_loader = data_loader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (64 / 60000 train. data). Loss: 2.343085527420044\n",
      "Training log: 1 epoch (704 / 60000 train. data). Loss: 2.2664434909820557\n",
      "Training log: 1 epoch (1344 / 60000 train. data). Loss: 2.155883550643921\n",
      "Training log: 1 epoch (1984 / 60000 train. data). Loss: 2.2007012367248535\n",
      "Training log: 1 epoch (2624 / 60000 train. data). Loss: 2.120431423187256\n",
      "Training log: 1 epoch (3264 / 60000 train. data). Loss: 2.134082317352295\n",
      "Training log: 1 epoch (3904 / 60000 train. data). Loss: 2.1673357486724854\n",
      "Training log: 1 epoch (4544 / 60000 train. data). Loss: 2.120565414428711\n",
      "Training log: 1 epoch (5184 / 60000 train. data). Loss: 2.2198359966278076\n",
      "Training log: 1 epoch (5824 / 60000 train. data). Loss: 2.0086278915405273\n",
      "Training log: 1 epoch (6464 / 60000 train. data). Loss: 1.984784722328186\n",
      "Training log: 1 epoch (7104 / 60000 train. data). Loss: 1.8617666959762573\n",
      "Training log: 1 epoch (7744 / 60000 train. data). Loss: 1.879723072052002\n",
      "Training log: 1 epoch (8384 / 60000 train. data). Loss: 2.0487828254699707\n",
      "Training log: 1 epoch (9024 / 60000 train. data). Loss: 1.9303748607635498\n",
      "Training log: 1 epoch (9664 / 60000 train. data). Loss: 2.104482889175415\n",
      "Training log: 1 epoch (10304 / 60000 train. data). Loss: 1.957425594329834\n",
      "Training log: 1 epoch (10944 / 60000 train. data). Loss: 1.9875218868255615\n",
      "Training log: 1 epoch (11584 / 60000 train. data). Loss: 2.059727191925049\n",
      "Training log: 1 epoch (12224 / 60000 train. data). Loss: 1.840815544128418\n",
      "Training log: 1 epoch (12864 / 60000 train. data). Loss: 1.9250050783157349\n",
      "Training log: 1 epoch (13504 / 60000 train. data). Loss: 2.0401933193206787\n",
      "Training log: 1 epoch (14144 / 60000 train. data). Loss: 1.7666583061218262\n",
      "Training log: 1 epoch (14784 / 60000 train. data). Loss: 1.862508773803711\n",
      "Training log: 1 epoch (15424 / 60000 train. data). Loss: 2.281940221786499\n",
      "Training log: 1 epoch (16064 / 60000 train. data). Loss: 2.1090784072875977\n",
      "Training log: 1 epoch (16704 / 60000 train. data). Loss: 1.7578494548797607\n",
      "Training log: 1 epoch (17344 / 60000 train. data). Loss: 1.966643214225769\n",
      "Training log: 1 epoch (17984 / 60000 train. data). Loss: 1.819367527961731\n",
      "Training log: 1 epoch (18624 / 60000 train. data). Loss: 1.8585854768753052\n",
      "Training log: 1 epoch (19264 / 60000 train. data). Loss: 1.7857307195663452\n",
      "Training log: 1 epoch (19904 / 60000 train. data). Loss: 2.0638210773468018\n",
      "Training log: 1 epoch (20544 / 60000 train. data). Loss: 1.7224674224853516\n",
      "Training log: 1 epoch (21184 / 60000 train. data). Loss: 1.7769578695297241\n",
      "Training log: 1 epoch (21824 / 60000 train. data). Loss: 1.8734416961669922\n",
      "Training log: 1 epoch (22464 / 60000 train. data). Loss: 1.7898330688476562\n",
      "Training log: 1 epoch (23104 / 60000 train. data). Loss: 1.7718614339828491\n",
      "Training log: 1 epoch (23744 / 60000 train. data). Loss: 2.0874314308166504\n",
      "Training log: 1 epoch (24384 / 60000 train. data). Loss: 2.066932201385498\n",
      "Training log: 1 epoch (25024 / 60000 train. data). Loss: 1.9144022464752197\n",
      "Training log: 1 epoch (25664 / 60000 train. data). Loss: 1.9933414459228516\n",
      "Training log: 1 epoch (26304 / 60000 train. data). Loss: 1.9828524589538574\n",
      "Training log: 1 epoch (26944 / 60000 train. data). Loss: 1.7902171611785889\n",
      "Training log: 1 epoch (27584 / 60000 train. data). Loss: 1.9857100248336792\n",
      "Training log: 1 epoch (28224 / 60000 train. data). Loss: 1.8322203159332275\n",
      "Training log: 1 epoch (28864 / 60000 train. data). Loss: 1.9308991432189941\n",
      "Training log: 1 epoch (29504 / 60000 train. data). Loss: 1.8456335067749023\n",
      "Training log: 1 epoch (30144 / 60000 train. data). Loss: 1.8424592018127441\n",
      "Training log: 1 epoch (30784 / 60000 train. data). Loss: 2.109652042388916\n",
      "Training log: 1 epoch (31424 / 60000 train. data). Loss: 1.6585227251052856\n",
      "Training log: 1 epoch (32064 / 60000 train. data). Loss: 2.0211844444274902\n",
      "Training log: 1 epoch (32704 / 60000 train. data). Loss: 1.8993040323257446\n",
      "Training log: 1 epoch (33344 / 60000 train. data). Loss: 1.8941285610198975\n",
      "Training log: 1 epoch (33984 / 60000 train. data). Loss: 1.881539225578308\n",
      "Training log: 1 epoch (34624 / 60000 train. data). Loss: 2.0463356971740723\n",
      "Training log: 1 epoch (35264 / 60000 train. data). Loss: 1.7968425750732422\n",
      "Training log: 1 epoch (35904 / 60000 train. data). Loss: 2.208329916000366\n",
      "Training log: 1 epoch (36544 / 60000 train. data). Loss: 1.7430673837661743\n",
      "Training log: 1 epoch (37184 / 60000 train. data). Loss: 1.8457216024398804\n",
      "Training log: 1 epoch (37824 / 60000 train. data). Loss: 1.6936171054840088\n",
      "Training log: 1 epoch (38464 / 60000 train. data). Loss: 1.8421450853347778\n",
      "Training log: 1 epoch (39104 / 60000 train. data). Loss: 1.806479811668396\n",
      "Training log: 1 epoch (39744 / 60000 train. data). Loss: 1.729499340057373\n",
      "Training log: 1 epoch (40384 / 60000 train. data). Loss: 1.7589685916900635\n",
      "Training log: 1 epoch (41024 / 60000 train. data). Loss: 1.7349629402160645\n",
      "Training log: 1 epoch (41664 / 60000 train. data). Loss: 1.6603100299835205\n",
      "Training log: 1 epoch (42304 / 60000 train. data). Loss: 1.6806381940841675\n",
      "Training log: 1 epoch (42944 / 60000 train. data). Loss: 1.8126046657562256\n",
      "Training log: 1 epoch (43584 / 60000 train. data). Loss: 1.8834683895111084\n",
      "Training log: 1 epoch (44224 / 60000 train. data). Loss: 2.0604982376098633\n",
      "Training log: 1 epoch (44864 / 60000 train. data). Loss: 1.9271838665008545\n",
      "Training log: 1 epoch (45504 / 60000 train. data). Loss: 1.9885711669921875\n",
      "Training log: 1 epoch (46144 / 60000 train. data). Loss: 1.8753124475479126\n",
      "Training log: 1 epoch (46784 / 60000 train. data). Loss: 1.6341423988342285\n",
      "Training log: 1 epoch (47424 / 60000 train. data). Loss: 1.8241170644760132\n",
      "Training log: 1 epoch (48064 / 60000 train. data). Loss: 1.7817469835281372\n",
      "Training log: 1 epoch (48704 / 60000 train. data). Loss: 1.7790839672088623\n",
      "Training log: 1 epoch (49344 / 60000 train. data). Loss: 1.9665539264678955\n",
      "Training log: 1 epoch (49984 / 60000 train. data). Loss: 1.7310357093811035\n",
      "Test loss (avg): 1.8065056337672434, Accuracy: 0.3644\n",
      "Training log: 2 epoch (64 / 60000 train. data). Loss: 1.9227052927017212\n",
      "Training log: 2 epoch (704 / 60000 train. data). Loss: 2.0770246982574463\n",
      "Training log: 2 epoch (1344 / 60000 train. data). Loss: 1.835593819618225\n",
      "Training log: 2 epoch (1984 / 60000 train. data). Loss: 1.7375279664993286\n",
      "Training log: 2 epoch (2624 / 60000 train. data). Loss: 1.5266859531402588\n",
      "Training log: 2 epoch (3264 / 60000 train. data). Loss: 1.8115252256393433\n",
      "Training log: 2 epoch (3904 / 60000 train. data). Loss: 1.7839823961257935\n",
      "Training log: 2 epoch (4544 / 60000 train. data). Loss: 1.8257726430892944\n",
      "Training log: 2 epoch (5184 / 60000 train. data). Loss: 1.7517057657241821\n",
      "Training log: 2 epoch (5824 / 60000 train. data). Loss: 1.9166334867477417\n",
      "Training log: 2 epoch (6464 / 60000 train. data). Loss: 1.8773458003997803\n",
      "Training log: 2 epoch (7104 / 60000 train. data). Loss: 1.827022910118103\n",
      "Training log: 2 epoch (7744 / 60000 train. data). Loss: 1.8659461736679077\n",
      "Training log: 2 epoch (8384 / 60000 train. data). Loss: 1.7765138149261475\n",
      "Training log: 2 epoch (9024 / 60000 train. data). Loss: 1.7568703889846802\n",
      "Training log: 2 epoch (9664 / 60000 train. data). Loss: 2.0199472904205322\n",
      "Training log: 2 epoch (10304 / 60000 train. data). Loss: 2.020695209503174\n",
      "Training log: 2 epoch (10944 / 60000 train. data). Loss: 1.639101266860962\n",
      "Training log: 2 epoch (11584 / 60000 train. data). Loss: 1.8799196481704712\n",
      "Training log: 2 epoch (12224 / 60000 train. data). Loss: 1.7261359691619873\n",
      "Training log: 2 epoch (12864 / 60000 train. data). Loss: 1.5258021354675293\n",
      "Training log: 2 epoch (13504 / 60000 train. data). Loss: 1.7031275033950806\n",
      "Training log: 2 epoch (14144 / 60000 train. data). Loss: 1.7288267612457275\n",
      "Training log: 2 epoch (14784 / 60000 train. data). Loss: 1.7138500213623047\n",
      "Training log: 2 epoch (15424 / 60000 train. data). Loss: 1.727548599243164\n",
      "Training log: 2 epoch (16064 / 60000 train. data). Loss: 1.8398611545562744\n",
      "Training log: 2 epoch (16704 / 60000 train. data). Loss: 1.8126171827316284\n",
      "Training log: 2 epoch (17344 / 60000 train. data). Loss: 1.7072759866714478\n",
      "Training log: 2 epoch (17984 / 60000 train. data). Loss: 1.8743047714233398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 2 epoch (18624 / 60000 train. data). Loss: 1.8465875387191772\n",
      "Training log: 2 epoch (19264 / 60000 train. data). Loss: 1.9145270586013794\n",
      "Training log: 2 epoch (19904 / 60000 train. data). Loss: 1.7582823038101196\n",
      "Training log: 2 epoch (20544 / 60000 train. data). Loss: 1.837105393409729\n",
      "Training log: 2 epoch (21184 / 60000 train. data). Loss: 1.9613507986068726\n",
      "Training log: 2 epoch (21824 / 60000 train. data). Loss: 1.7205018997192383\n",
      "Training log: 2 epoch (22464 / 60000 train. data). Loss: 1.746718406677246\n",
      "Training log: 2 epoch (23104 / 60000 train. data). Loss: 1.8876994848251343\n",
      "Training log: 2 epoch (23744 / 60000 train. data). Loss: 1.7648628950119019\n",
      "Training log: 2 epoch (24384 / 60000 train. data). Loss: 1.8232636451721191\n",
      "Training log: 2 epoch (25024 / 60000 train. data). Loss: 1.7491614818572998\n",
      "Training log: 2 epoch (25664 / 60000 train. data). Loss: 1.8725497722625732\n",
      "Training log: 2 epoch (26304 / 60000 train. data). Loss: 1.8275766372680664\n",
      "Training log: 2 epoch (26944 / 60000 train. data). Loss: 1.9699333906173706\n",
      "Training log: 2 epoch (27584 / 60000 train. data). Loss: 1.8405094146728516\n",
      "Training log: 2 epoch (28224 / 60000 train. data). Loss: 1.707026720046997\n",
      "Training log: 2 epoch (28864 / 60000 train. data). Loss: 1.9073596000671387\n",
      "Training log: 2 epoch (29504 / 60000 train. data). Loss: 1.5449199676513672\n",
      "Training log: 2 epoch (30144 / 60000 train. data). Loss: 1.9277663230895996\n",
      "Training log: 2 epoch (30784 / 60000 train. data). Loss: 1.9245402812957764\n",
      "Training log: 2 epoch (31424 / 60000 train. data). Loss: 2.1380696296691895\n",
      "Training log: 2 epoch (32064 / 60000 train. data). Loss: 1.815849781036377\n",
      "Training log: 2 epoch (32704 / 60000 train. data). Loss: 1.7974807024002075\n",
      "Training log: 2 epoch (33344 / 60000 train. data). Loss: 2.0250566005706787\n",
      "Training log: 2 epoch (33984 / 60000 train. data). Loss: 1.9667783975601196\n",
      "Training log: 2 epoch (34624 / 60000 train. data). Loss: 2.1122069358825684\n",
      "Training log: 2 epoch (35264 / 60000 train. data). Loss: 1.7226260900497437\n",
      "Training log: 2 epoch (35904 / 60000 train. data). Loss: 1.9095704555511475\n",
      "Training log: 2 epoch (36544 / 60000 train. data). Loss: 1.8237907886505127\n",
      "Training log: 2 epoch (37184 / 60000 train. data). Loss: 1.957180142402649\n",
      "Training log: 2 epoch (37824 / 60000 train. data). Loss: 1.5784448385238647\n",
      "Training log: 2 epoch (38464 / 60000 train. data). Loss: 1.6222269535064697\n",
      "Training log: 2 epoch (39104 / 60000 train. data). Loss: 1.5986976623535156\n",
      "Training log: 2 epoch (39744 / 60000 train. data). Loss: 1.9858686923980713\n",
      "Training log: 2 epoch (40384 / 60000 train. data). Loss: 2.03836727142334\n",
      "Training log: 2 epoch (41024 / 60000 train. data). Loss: 1.5673518180847168\n",
      "Training log: 2 epoch (41664 / 60000 train. data). Loss: 1.9773153066635132\n",
      "Training log: 2 epoch (42304 / 60000 train. data). Loss: 1.6578516960144043\n",
      "Training log: 2 epoch (42944 / 60000 train. data). Loss: 1.6781367063522339\n",
      "Training log: 2 epoch (43584 / 60000 train. data). Loss: 1.8573143482208252\n",
      "Training log: 2 epoch (44224 / 60000 train. data). Loss: 2.006850242614746\n",
      "Training log: 2 epoch (44864 / 60000 train. data). Loss: 1.9327269792556763\n",
      "Training log: 2 epoch (45504 / 60000 train. data). Loss: 1.6827871799468994\n",
      "Training log: 2 epoch (46144 / 60000 train. data). Loss: 1.928162693977356\n",
      "Training log: 2 epoch (46784 / 60000 train. data). Loss: 1.9154001474380493\n",
      "Training log: 2 epoch (47424 / 60000 train. data). Loss: 2.1184580326080322\n",
      "Training log: 2 epoch (48064 / 60000 train. data). Loss: 1.8080816268920898\n",
      "Training log: 2 epoch (48704 / 60000 train. data). Loss: 1.7764137983322144\n",
      "Training log: 2 epoch (49344 / 60000 train. data). Loss: 1.760055661201477\n",
      "Training log: 2 epoch (49984 / 60000 train. data). Loss: 1.6764202117919922\n",
      "Test loss (avg): 1.7981510792568232, Accuracy: 0.3653\n",
      "Training log: 3 epoch (64 / 60000 train. data). Loss: 1.7005797624588013\n",
      "Training log: 3 epoch (704 / 60000 train. data). Loss: 1.6721876859664917\n",
      "Training log: 3 epoch (1344 / 60000 train. data). Loss: 1.8106566667556763\n",
      "Training log: 3 epoch (1984 / 60000 train. data). Loss: 1.618306040763855\n",
      "Training log: 3 epoch (2624 / 60000 train. data). Loss: 1.848374605178833\n",
      "Training log: 3 epoch (3264 / 60000 train. data). Loss: 1.795624017715454\n",
      "Training log: 3 epoch (3904 / 60000 train. data). Loss: 1.664150595664978\n",
      "Training log: 3 epoch (4544 / 60000 train. data). Loss: 1.5759302377700806\n",
      "Training log: 3 epoch (5184 / 60000 train. data). Loss: 1.8331828117370605\n",
      "Training log: 3 epoch (5824 / 60000 train. data). Loss: 1.7387025356292725\n",
      "Training log: 3 epoch (6464 / 60000 train. data). Loss: 1.6348105669021606\n",
      "Training log: 3 epoch (7104 / 60000 train. data). Loss: 1.6925214529037476\n",
      "Training log: 3 epoch (7744 / 60000 train. data). Loss: 1.844558835029602\n",
      "Training log: 3 epoch (8384 / 60000 train. data). Loss: 1.855410099029541\n",
      "Training log: 3 epoch (9024 / 60000 train. data). Loss: 1.7971471548080444\n",
      "Training log: 3 epoch (9664 / 60000 train. data). Loss: 1.9500808715820312\n",
      "Training log: 3 epoch (10304 / 60000 train. data). Loss: 1.6945805549621582\n",
      "Training log: 3 epoch (10944 / 60000 train. data). Loss: 1.8213320970535278\n",
      "Training log: 3 epoch (11584 / 60000 train. data). Loss: 1.72687566280365\n",
      "Training log: 3 epoch (12224 / 60000 train. data). Loss: 1.811404824256897\n",
      "Training log: 3 epoch (12864 / 60000 train. data). Loss: 1.7447682619094849\n",
      "Training log: 3 epoch (13504 / 60000 train. data). Loss: 1.6651294231414795\n",
      "Training log: 3 epoch (14144 / 60000 train. data). Loss: 1.950063943862915\n",
      "Training log: 3 epoch (14784 / 60000 train. data). Loss: 1.6140292882919312\n",
      "Training log: 3 epoch (15424 / 60000 train. data). Loss: 1.8457622528076172\n",
      "Training log: 3 epoch (16064 / 60000 train. data). Loss: 1.6945065259933472\n",
      "Training log: 3 epoch (16704 / 60000 train. data). Loss: 1.7180145978927612\n",
      "Training log: 3 epoch (17344 / 60000 train. data). Loss: 2.143028736114502\n",
      "Training log: 3 epoch (17984 / 60000 train. data). Loss: 1.7327874898910522\n",
      "Training log: 3 epoch (18624 / 60000 train. data). Loss: 1.8827123641967773\n",
      "Training log: 3 epoch (19264 / 60000 train. data). Loss: 1.653441309928894\n",
      "Training log: 3 epoch (19904 / 60000 train. data). Loss: 1.8663356304168701\n",
      "Training log: 3 epoch (20544 / 60000 train. data). Loss: 1.9086556434631348\n",
      "Training log: 3 epoch (21184 / 60000 train. data). Loss: 1.6953098773956299\n",
      "Training log: 3 epoch (21824 / 60000 train. data). Loss: 1.9447225332260132\n",
      "Training log: 3 epoch (22464 / 60000 train. data). Loss: 1.7912113666534424\n",
      "Training log: 3 epoch (23104 / 60000 train. data). Loss: 1.6388885974884033\n",
      "Training log: 3 epoch (23744 / 60000 train. data). Loss: 1.7702399492263794\n",
      "Training log: 3 epoch (24384 / 60000 train. data). Loss: 1.7538336515426636\n",
      "Training log: 3 epoch (25024 / 60000 train. data). Loss: 1.6092265844345093\n",
      "Training log: 3 epoch (25664 / 60000 train. data). Loss: 1.6328997611999512\n",
      "Training log: 3 epoch (26304 / 60000 train. data). Loss: 1.6061304807662964\n",
      "Training log: 3 epoch (26944 / 60000 train. data). Loss: 1.7800116539001465\n",
      "Training log: 3 epoch (27584 / 60000 train. data). Loss: 1.798385739326477\n",
      "Training log: 3 epoch (28224 / 60000 train. data). Loss: 1.8058135509490967\n",
      "Training log: 3 epoch (28864 / 60000 train. data). Loss: 1.8105288743972778\n",
      "Training log: 3 epoch (29504 / 60000 train. data). Loss: 1.828315258026123\n",
      "Training log: 3 epoch (30144 / 60000 train. data). Loss: 1.882381796836853\n",
      "Training log: 3 epoch (30784 / 60000 train. data). Loss: 2.093151330947876\n",
      "Training log: 3 epoch (31424 / 60000 train. data). Loss: 1.638488531112671\n",
      "Training log: 3 epoch (32064 / 60000 train. data). Loss: 1.6459004878997803\n",
      "Training log: 3 epoch (32704 / 60000 train. data). Loss: 1.6740556955337524\n",
      "Training log: 3 epoch (33344 / 60000 train. data). Loss: 1.3974807262420654\n",
      "Training log: 3 epoch (33984 / 60000 train. data). Loss: 1.6825945377349854\n",
      "Training log: 3 epoch (34624 / 60000 train. data). Loss: 1.8469302654266357\n",
      "Training log: 3 epoch (35264 / 60000 train. data). Loss: 1.8181735277175903\n",
      "Training log: 3 epoch (35904 / 60000 train. data). Loss: 1.8656259775161743\n",
      "Training log: 3 epoch (36544 / 60000 train. data). Loss: 1.915684461593628\n",
      "Training log: 3 epoch (37184 / 60000 train. data). Loss: 1.7819807529449463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 3 epoch (37824 / 60000 train. data). Loss: 1.8196851015090942\n",
      "Training log: 3 epoch (38464 / 60000 train. data). Loss: 1.6093618869781494\n",
      "Training log: 3 epoch (39104 / 60000 train. data). Loss: 1.567617416381836\n",
      "Training log: 3 epoch (39744 / 60000 train. data). Loss: 1.8496512174606323\n",
      "Training log: 3 epoch (40384 / 60000 train. data). Loss: 1.5464954376220703\n",
      "Training log: 3 epoch (41024 / 60000 train. data). Loss: 1.6954432725906372\n",
      "Training log: 3 epoch (41664 / 60000 train. data). Loss: 1.7764616012573242\n",
      "Training log: 3 epoch (42304 / 60000 train. data). Loss: 1.7405341863632202\n",
      "Training log: 3 epoch (42944 / 60000 train. data). Loss: 1.7049912214279175\n",
      "Training log: 3 epoch (43584 / 60000 train. data). Loss: 1.7052879333496094\n",
      "Training log: 3 epoch (44224 / 60000 train. data). Loss: 1.6022112369537354\n",
      "Training log: 3 epoch (44864 / 60000 train. data). Loss: 1.8659143447875977\n",
      "Training log: 3 epoch (45504 / 60000 train. data). Loss: 1.6728127002716064\n",
      "Training log: 3 epoch (46144 / 60000 train. data). Loss: 1.8652645349502563\n",
      "Training log: 3 epoch (46784 / 60000 train. data). Loss: 1.7257813215255737\n",
      "Training log: 3 epoch (47424 / 60000 train. data). Loss: 1.7321383953094482\n",
      "Training log: 3 epoch (48064 / 60000 train. data). Loss: 1.790038824081421\n",
      "Training log: 3 epoch (48704 / 60000 train. data). Loss: 1.5109680891036987\n",
      "Training log: 3 epoch (49344 / 60000 train. data). Loss: 1.8016161918640137\n",
      "Training log: 3 epoch (49984 / 60000 train. data). Loss: 1.7739615440368652\n",
      "Test loss (avg): 1.8153648558695605, Accuracy: 0.3828\n",
      "Training log: 4 epoch (64 / 60000 train. data). Loss: 1.5912615060806274\n",
      "Training log: 4 epoch (704 / 60000 train. data). Loss: 2.2437751293182373\n",
      "Training log: 4 epoch (1344 / 60000 train. data). Loss: 1.5832455158233643\n",
      "Training log: 4 epoch (1984 / 60000 train. data). Loss: 2.0010268688201904\n",
      "Training log: 4 epoch (2624 / 60000 train. data). Loss: 1.7376986742019653\n",
      "Training log: 4 epoch (3264 / 60000 train. data). Loss: 1.6616239547729492\n",
      "Training log: 4 epoch (3904 / 60000 train. data). Loss: 2.037606716156006\n",
      "Training log: 4 epoch (4544 / 60000 train. data). Loss: 1.608518123626709\n",
      "Training log: 4 epoch (5184 / 60000 train. data). Loss: 1.6922086477279663\n",
      "Training log: 4 epoch (5824 / 60000 train. data). Loss: 1.5622072219848633\n",
      "Training log: 4 epoch (6464 / 60000 train. data). Loss: 1.8321243524551392\n",
      "Training log: 4 epoch (7104 / 60000 train. data). Loss: 1.5992077589035034\n",
      "Training log: 4 epoch (7744 / 60000 train. data). Loss: 1.9091535806655884\n",
      "Training log: 4 epoch (8384 / 60000 train. data). Loss: 1.5347042083740234\n",
      "Training log: 4 epoch (9024 / 60000 train. data). Loss: 1.7557756900787354\n",
      "Training log: 4 epoch (9664 / 60000 train. data). Loss: 1.7521717548370361\n",
      "Training log: 4 epoch (10304 / 60000 train. data). Loss: 1.809759259223938\n",
      "Training log: 4 epoch (10944 / 60000 train. data). Loss: 1.4924122095108032\n",
      "Training log: 4 epoch (11584 / 60000 train. data). Loss: 1.6368597745895386\n",
      "Training log: 4 epoch (12224 / 60000 train. data). Loss: 1.7119038105010986\n",
      "Training log: 4 epoch (12864 / 60000 train. data). Loss: 1.5591096878051758\n",
      "Training log: 4 epoch (13504 / 60000 train. data). Loss: 1.7728372812271118\n",
      "Training log: 4 epoch (14144 / 60000 train. data). Loss: 1.8298028707504272\n",
      "Training log: 4 epoch (14784 / 60000 train. data). Loss: 1.7228341102600098\n",
      "Training log: 4 epoch (15424 / 60000 train. data). Loss: 1.8047689199447632\n",
      "Training log: 4 epoch (16064 / 60000 train. data). Loss: 1.6060020923614502\n",
      "Training log: 4 epoch (16704 / 60000 train. data). Loss: 1.6714295148849487\n",
      "Training log: 4 epoch (17344 / 60000 train. data). Loss: 1.9002559185028076\n",
      "Training log: 4 epoch (17984 / 60000 train. data). Loss: 1.8680732250213623\n",
      "Training log: 4 epoch (18624 / 60000 train. data). Loss: 1.5758370161056519\n",
      "Training log: 4 epoch (19264 / 60000 train. data). Loss: 1.793406367301941\n",
      "Training log: 4 epoch (19904 / 60000 train. data). Loss: 1.9156779050827026\n",
      "Training log: 4 epoch (20544 / 60000 train. data). Loss: 1.6154810190200806\n",
      "Training log: 4 epoch (21184 / 60000 train. data). Loss: 1.6904810667037964\n",
      "Training log: 4 epoch (21824 / 60000 train. data). Loss: 1.5397573709487915\n",
      "Training log: 4 epoch (22464 / 60000 train. data). Loss: 1.6512807607650757\n",
      "Training log: 4 epoch (23104 / 60000 train. data). Loss: 1.7066744565963745\n",
      "Training log: 4 epoch (23744 / 60000 train. data). Loss: 1.6941438913345337\n",
      "Training log: 4 epoch (24384 / 60000 train. data). Loss: 1.567112684249878\n",
      "Training log: 4 epoch (25024 / 60000 train. data). Loss: 1.616466760635376\n",
      "Training log: 4 epoch (25664 / 60000 train. data). Loss: 1.7835232019424438\n",
      "Training log: 4 epoch (26304 / 60000 train. data). Loss: 1.5812053680419922\n",
      "Training log: 4 epoch (26944 / 60000 train. data). Loss: 1.6729629039764404\n",
      "Training log: 4 epoch (27584 / 60000 train. data). Loss: 1.7761783599853516\n",
      "Training log: 4 epoch (28224 / 60000 train. data). Loss: 1.7123042345046997\n",
      "Training log: 4 epoch (28864 / 60000 train. data). Loss: 1.8221125602722168\n",
      "Training log: 4 epoch (29504 / 60000 train. data). Loss: 1.7548078298568726\n",
      "Training log: 4 epoch (30144 / 60000 train. data). Loss: 1.9516921043395996\n",
      "Training log: 4 epoch (30784 / 60000 train. data). Loss: 1.600663185119629\n",
      "Training log: 4 epoch (31424 / 60000 train. data). Loss: 1.4901034832000732\n",
      "Training log: 4 epoch (32064 / 60000 train. data). Loss: 1.7478737831115723\n",
      "Training log: 4 epoch (32704 / 60000 train. data). Loss: 1.4114307165145874\n",
      "Training log: 4 epoch (33344 / 60000 train. data). Loss: 1.6582345962524414\n",
      "Training log: 4 epoch (33984 / 60000 train. data). Loss: 1.6252679824829102\n",
      "Training log: 4 epoch (34624 / 60000 train. data). Loss: 1.3978872299194336\n",
      "Training log: 4 epoch (35264 / 60000 train. data). Loss: 1.6926110982894897\n",
      "Training log: 4 epoch (35904 / 60000 train. data). Loss: 1.658186435699463\n",
      "Training log: 4 epoch (36544 / 60000 train. data). Loss: 1.9205251932144165\n",
      "Training log: 4 epoch (37184 / 60000 train. data). Loss: 1.4705170392990112\n",
      "Training log: 4 epoch (37824 / 60000 train. data). Loss: 1.5540311336517334\n",
      "Training log: 4 epoch (38464 / 60000 train. data). Loss: 1.4620730876922607\n",
      "Training log: 4 epoch (39104 / 60000 train. data). Loss: 1.770238995552063\n",
      "Training log: 4 epoch (39744 / 60000 train. data). Loss: 1.7897781133651733\n",
      "Training log: 4 epoch (40384 / 60000 train. data). Loss: 1.7139273881912231\n",
      "Training log: 4 epoch (41024 / 60000 train. data). Loss: 1.9640967845916748\n",
      "Training log: 4 epoch (41664 / 60000 train. data). Loss: 1.6491539478302002\n",
      "Training log: 4 epoch (42304 / 60000 train. data). Loss: 1.57882821559906\n",
      "Training log: 4 epoch (42944 / 60000 train. data). Loss: 1.5901625156402588\n",
      "Training log: 4 epoch (43584 / 60000 train. data). Loss: 1.8710750341415405\n",
      "Training log: 4 epoch (44224 / 60000 train. data). Loss: 1.6934078931808472\n",
      "Training log: 4 epoch (44864 / 60000 train. data). Loss: 1.6679999828338623\n",
      "Training log: 4 epoch (45504 / 60000 train. data). Loss: 1.8121395111083984\n",
      "Training log: 4 epoch (46144 / 60000 train. data). Loss: 1.7219390869140625\n",
      "Training log: 4 epoch (46784 / 60000 train. data). Loss: 1.7212356328964233\n",
      "Training log: 4 epoch (47424 / 60000 train. data). Loss: 1.9061981439590454\n",
      "Training log: 4 epoch (48064 / 60000 train. data). Loss: 1.7839007377624512\n",
      "Training log: 4 epoch (48704 / 60000 train. data). Loss: 1.785071611404419\n",
      "Training log: 4 epoch (49344 / 60000 train. data). Loss: 1.5959895849227905\n",
      "Training log: 4 epoch (49984 / 60000 train. data). Loss: 1.9744113683700562\n",
      "Test loss (avg): 1.7161058741769972, Accuracy: 0.4001\n",
      "Training log: 5 epoch (64 / 60000 train. data). Loss: 1.656491994857788\n",
      "Training log: 5 epoch (704 / 60000 train. data). Loss: 1.567454218864441\n",
      "Training log: 5 epoch (1344 / 60000 train. data). Loss: 1.67583167552948\n",
      "Training log: 5 epoch (1984 / 60000 train. data). Loss: 1.549483299255371\n",
      "Training log: 5 epoch (2624 / 60000 train. data). Loss: 1.9776543378829956\n",
      "Training log: 5 epoch (3264 / 60000 train. data). Loss: 1.742357611656189\n",
      "Training log: 5 epoch (3904 / 60000 train. data). Loss: 1.988044261932373\n",
      "Training log: 5 epoch (4544 / 60000 train. data). Loss: 1.668950080871582\n",
      "Training log: 5 epoch (5184 / 60000 train. data). Loss: 1.5679751634597778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 5 epoch (5824 / 60000 train. data). Loss: 1.7982057332992554\n",
      "Training log: 5 epoch (6464 / 60000 train. data). Loss: 1.6817119121551514\n",
      "Training log: 5 epoch (7104 / 60000 train. data). Loss: 1.7458961009979248\n",
      "Training log: 5 epoch (7744 / 60000 train. data). Loss: 1.860746145248413\n",
      "Training log: 5 epoch (8384 / 60000 train. data). Loss: 1.7147928476333618\n",
      "Training log: 5 epoch (9024 / 60000 train. data). Loss: 1.4697632789611816\n",
      "Training log: 5 epoch (9664 / 60000 train. data). Loss: 1.7661733627319336\n",
      "Training log: 5 epoch (10304 / 60000 train. data). Loss: 1.7400414943695068\n",
      "Training log: 5 epoch (10944 / 60000 train. data). Loss: 1.8545955419540405\n",
      "Training log: 5 epoch (11584 / 60000 train. data). Loss: 1.5662832260131836\n",
      "Training log: 5 epoch (12224 / 60000 train. data). Loss: 1.833535075187683\n",
      "Training log: 5 epoch (12864 / 60000 train. data). Loss: 1.39066743850708\n",
      "Training log: 5 epoch (13504 / 60000 train. data). Loss: 1.599547266960144\n",
      "Training log: 5 epoch (14144 / 60000 train. data). Loss: 1.9036579132080078\n",
      "Training log: 5 epoch (14784 / 60000 train. data). Loss: 1.6837646961212158\n",
      "Training log: 5 epoch (15424 / 60000 train. data). Loss: 1.8869683742523193\n",
      "Training log: 5 epoch (16064 / 60000 train. data). Loss: 1.7977759838104248\n",
      "Training log: 5 epoch (16704 / 60000 train. data). Loss: 1.6271978616714478\n",
      "Training log: 5 epoch (17344 / 60000 train. data). Loss: 1.5732288360595703\n",
      "Training log: 5 epoch (17984 / 60000 train. data). Loss: 1.482401728630066\n",
      "Training log: 5 epoch (18624 / 60000 train. data). Loss: 1.742476224899292\n",
      "Training log: 5 epoch (19264 / 60000 train. data). Loss: 1.5740419626235962\n",
      "Training log: 5 epoch (19904 / 60000 train. data). Loss: 1.563006043434143\n",
      "Training log: 5 epoch (20544 / 60000 train. data). Loss: 1.793921947479248\n",
      "Training log: 5 epoch (21184 / 60000 train. data). Loss: 1.6187869310379028\n",
      "Training log: 5 epoch (21824 / 60000 train. data). Loss: 1.5788614749908447\n",
      "Training log: 5 epoch (22464 / 60000 train. data). Loss: 1.3967900276184082\n",
      "Training log: 5 epoch (23104 / 60000 train. data). Loss: 1.7443090677261353\n",
      "Training log: 5 epoch (23744 / 60000 train. data). Loss: 1.711739420890808\n",
      "Training log: 5 epoch (24384 / 60000 train. data). Loss: 1.56639564037323\n",
      "Training log: 5 epoch (25024 / 60000 train. data). Loss: 1.7742170095443726\n",
      "Training log: 5 epoch (25664 / 60000 train. data). Loss: 1.4265153408050537\n",
      "Training log: 5 epoch (26304 / 60000 train. data). Loss: 1.5378494262695312\n",
      "Training log: 5 epoch (26944 / 60000 train. data). Loss: 1.7818835973739624\n",
      "Training log: 5 epoch (27584 / 60000 train. data). Loss: 1.6746022701263428\n",
      "Training log: 5 epoch (28224 / 60000 train. data). Loss: 1.8203741312026978\n",
      "Training log: 5 epoch (28864 / 60000 train. data). Loss: 1.6034117937088013\n",
      "Training log: 5 epoch (29504 / 60000 train. data). Loss: 1.494626760482788\n",
      "Training log: 5 epoch (30144 / 60000 train. data). Loss: 1.7368361949920654\n",
      "Training log: 5 epoch (30784 / 60000 train. data). Loss: 1.7840120792388916\n",
      "Training log: 5 epoch (31424 / 60000 train. data). Loss: 1.6487101316452026\n",
      "Training log: 5 epoch (32064 / 60000 train. data). Loss: 1.7655189037322998\n",
      "Training log: 5 epoch (32704 / 60000 train. data). Loss: 1.7680188417434692\n",
      "Training log: 5 epoch (33344 / 60000 train. data). Loss: 1.4875092506408691\n",
      "Training log: 5 epoch (33984 / 60000 train. data). Loss: 1.6258546113967896\n",
      "Training log: 5 epoch (34624 / 60000 train. data). Loss: 1.4636636972427368\n",
      "Training log: 5 epoch (35264 / 60000 train. data). Loss: 1.8276236057281494\n",
      "Training log: 5 epoch (35904 / 60000 train. data). Loss: 1.5522840023040771\n",
      "Training log: 5 epoch (36544 / 60000 train. data). Loss: 1.7186695337295532\n",
      "Training log: 5 epoch (37184 / 60000 train. data). Loss: 1.812943458557129\n",
      "Training log: 5 epoch (37824 / 60000 train. data). Loss: 1.848499059677124\n",
      "Training log: 5 epoch (38464 / 60000 train. data). Loss: 1.666690707206726\n",
      "Training log: 5 epoch (39104 / 60000 train. data). Loss: 1.9135936498641968\n",
      "Training log: 5 epoch (39744 / 60000 train. data). Loss: 1.6051961183547974\n",
      "Training log: 5 epoch (40384 / 60000 train. data). Loss: 1.785830020904541\n",
      "Training log: 5 epoch (41024 / 60000 train. data). Loss: 1.6566351652145386\n",
      "Training log: 5 epoch (41664 / 60000 train. data). Loss: 1.566283106803894\n",
      "Training log: 5 epoch (42304 / 60000 train. data). Loss: 1.7230440378189087\n",
      "Training log: 5 epoch (42944 / 60000 train. data). Loss: 1.939417839050293\n",
      "Training log: 5 epoch (43584 / 60000 train. data). Loss: 1.9015873670578003\n",
      "Training log: 5 epoch (44224 / 60000 train. data). Loss: 1.5131837129592896\n",
      "Training log: 5 epoch (44864 / 60000 train. data). Loss: 1.6405783891677856\n",
      "Training log: 5 epoch (45504 / 60000 train. data). Loss: 1.7795661687850952\n",
      "Training log: 5 epoch (46144 / 60000 train. data). Loss: 1.5911064147949219\n",
      "Training log: 5 epoch (46784 / 60000 train. data). Loss: 1.5752880573272705\n",
      "Training log: 5 epoch (47424 / 60000 train. data). Loss: 1.4535502195358276\n",
      "Training log: 5 epoch (48064 / 60000 train. data). Loss: 1.5774016380310059\n",
      "Training log: 5 epoch (48704 / 60000 train. data). Loss: 1.6931331157684326\n",
      "Training log: 5 epoch (49344 / 60000 train. data). Loss: 1.6006089448928833\n",
      "Training log: 5 epoch (49984 / 60000 train. data). Loss: 1.7368628978729248\n",
      "Test loss (avg): 1.7270270548049051, Accuracy: 0.3911\n",
      "Training log: 6 epoch (64 / 60000 train. data). Loss: 1.8222514390945435\n",
      "Training log: 6 epoch (704 / 60000 train. data). Loss: 1.5058008432388306\n",
      "Training log: 6 epoch (1344 / 60000 train. data). Loss: 1.6749210357666016\n",
      "Training log: 6 epoch (1984 / 60000 train. data). Loss: 1.6668347120285034\n",
      "Training log: 6 epoch (2624 / 60000 train. data). Loss: 1.4997459650039673\n",
      "Training log: 6 epoch (3264 / 60000 train. data). Loss: 1.7318415641784668\n",
      "Training log: 6 epoch (3904 / 60000 train. data). Loss: 1.5448671579360962\n",
      "Training log: 6 epoch (4544 / 60000 train. data). Loss: 1.821869134902954\n",
      "Training log: 6 epoch (5184 / 60000 train. data). Loss: 1.7430521249771118\n",
      "Training log: 6 epoch (5824 / 60000 train. data). Loss: 1.5562467575073242\n",
      "Training log: 6 epoch (6464 / 60000 train. data). Loss: 1.500264286994934\n",
      "Training log: 6 epoch (7104 / 60000 train. data). Loss: 1.6077204942703247\n",
      "Training log: 6 epoch (7744 / 60000 train. data). Loss: 1.6089682579040527\n",
      "Training log: 6 epoch (8384 / 60000 train. data). Loss: 1.5538634061813354\n",
      "Training log: 6 epoch (9024 / 60000 train. data). Loss: 1.784438967704773\n",
      "Training log: 6 epoch (9664 / 60000 train. data). Loss: 1.9303548336029053\n",
      "Training log: 6 epoch (10304 / 60000 train. data). Loss: 1.6786469221115112\n",
      "Training log: 6 epoch (10944 / 60000 train. data). Loss: 1.675432801246643\n",
      "Training log: 6 epoch (11584 / 60000 train. data). Loss: 1.4738118648529053\n",
      "Training log: 6 epoch (12224 / 60000 train. data). Loss: 1.7894352674484253\n",
      "Training log: 6 epoch (12864 / 60000 train. data). Loss: 1.4322654008865356\n",
      "Training log: 6 epoch (13504 / 60000 train. data). Loss: 1.5885612964630127\n",
      "Training log: 6 epoch (14144 / 60000 train. data). Loss: 1.5725317001342773\n",
      "Training log: 6 epoch (14784 / 60000 train. data). Loss: 1.5627217292785645\n",
      "Training log: 6 epoch (15424 / 60000 train. data). Loss: 1.5387765169143677\n",
      "Training log: 6 epoch (16064 / 60000 train. data). Loss: 1.866425633430481\n",
      "Training log: 6 epoch (16704 / 60000 train. data). Loss: 1.6959155797958374\n",
      "Training log: 6 epoch (17344 / 60000 train. data). Loss: 1.5164824724197388\n",
      "Training log: 6 epoch (17984 / 60000 train. data). Loss: 1.6762968301773071\n",
      "Training log: 6 epoch (18624 / 60000 train. data). Loss: 1.45345938205719\n",
      "Training log: 6 epoch (19264 / 60000 train. data). Loss: 1.6315115690231323\n",
      "Training log: 6 epoch (19904 / 60000 train. data). Loss: 1.595531702041626\n",
      "Training log: 6 epoch (20544 / 60000 train. data). Loss: 1.7829633951187134\n",
      "Training log: 6 epoch (21184 / 60000 train. data). Loss: 1.5263670682907104\n",
      "Training log: 6 epoch (21824 / 60000 train. data). Loss: 1.6189470291137695\n",
      "Training log: 6 epoch (22464 / 60000 train. data). Loss: 1.692962408065796\n",
      "Training log: 6 epoch (23104 / 60000 train. data). Loss: 1.8375763893127441\n",
      "Training log: 6 epoch (23744 / 60000 train. data). Loss: 1.6226972341537476\n",
      "Training log: 6 epoch (24384 / 60000 train. data). Loss: 1.4917857646942139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 6 epoch (25024 / 60000 train. data). Loss: 1.5561188459396362\n",
      "Training log: 6 epoch (25664 / 60000 train. data). Loss: 1.4517592191696167\n",
      "Training log: 6 epoch (26304 / 60000 train. data). Loss: 1.707542061805725\n",
      "Training log: 6 epoch (26944 / 60000 train. data). Loss: 1.6477528810501099\n",
      "Training log: 6 epoch (27584 / 60000 train. data). Loss: 1.6823458671569824\n",
      "Training log: 6 epoch (28224 / 60000 train. data). Loss: 1.5097357034683228\n",
      "Training log: 6 epoch (28864 / 60000 train. data). Loss: 1.7205119132995605\n",
      "Training log: 6 epoch (29504 / 60000 train. data). Loss: 1.5401688814163208\n",
      "Training log: 6 epoch (30144 / 60000 train. data). Loss: 1.9168202877044678\n",
      "Training log: 6 epoch (30784 / 60000 train. data). Loss: 1.952653169631958\n",
      "Training log: 6 epoch (31424 / 60000 train. data). Loss: 1.5729756355285645\n",
      "Training log: 6 epoch (32064 / 60000 train. data). Loss: 1.6491329669952393\n",
      "Training log: 6 epoch (32704 / 60000 train. data). Loss: 1.5413432121276855\n",
      "Training log: 6 epoch (33344 / 60000 train. data). Loss: 1.5400639772415161\n",
      "Training log: 6 epoch (33984 / 60000 train. data). Loss: 1.852806568145752\n",
      "Training log: 6 epoch (34624 / 60000 train. data). Loss: 1.8215734958648682\n",
      "Training log: 6 epoch (35264 / 60000 train. data). Loss: 1.5908417701721191\n",
      "Training log: 6 epoch (35904 / 60000 train. data). Loss: 1.3706005811691284\n",
      "Training log: 6 epoch (36544 / 60000 train. data). Loss: 1.8041239976882935\n",
      "Training log: 6 epoch (37184 / 60000 train. data). Loss: 1.5245757102966309\n",
      "Training log: 6 epoch (37824 / 60000 train. data). Loss: 1.581520915031433\n",
      "Training log: 6 epoch (38464 / 60000 train. data). Loss: 1.7713285684585571\n",
      "Training log: 6 epoch (39104 / 60000 train. data). Loss: 1.5806984901428223\n",
      "Training log: 6 epoch (39744 / 60000 train. data). Loss: 1.5520436763763428\n",
      "Training log: 6 epoch (40384 / 60000 train. data). Loss: 1.9263356924057007\n",
      "Training log: 6 epoch (41024 / 60000 train. data). Loss: 1.608351230621338\n",
      "Training log: 6 epoch (41664 / 60000 train. data). Loss: 1.3958361148834229\n",
      "Training log: 6 epoch (42304 / 60000 train. data). Loss: 1.6783913373947144\n",
      "Training log: 6 epoch (42944 / 60000 train. data). Loss: 1.800602912902832\n",
      "Training log: 6 epoch (43584 / 60000 train. data). Loss: 1.8708196878433228\n",
      "Training log: 6 epoch (44224 / 60000 train. data). Loss: 1.769609808921814\n",
      "Training log: 6 epoch (44864 / 60000 train. data). Loss: 1.4783084392547607\n",
      "Training log: 6 epoch (45504 / 60000 train. data). Loss: 1.6259915828704834\n",
      "Training log: 6 epoch (46144 / 60000 train. data). Loss: 1.4388786554336548\n",
      "Training log: 6 epoch (46784 / 60000 train. data). Loss: 1.5610548257827759\n",
      "Training log: 6 epoch (47424 / 60000 train. data). Loss: 1.5801855325698853\n",
      "Training log: 6 epoch (48064 / 60000 train. data). Loss: 1.6049500703811646\n",
      "Training log: 6 epoch (48704 / 60000 train. data). Loss: 1.7826288938522339\n",
      "Training log: 6 epoch (49344 / 60000 train. data). Loss: 1.8041108846664429\n",
      "Training log: 6 epoch (49984 / 60000 train. data). Loss: 1.7076146602630615\n",
      "Test loss (avg): 1.6955419247317467, Accuracy: 0.4134\n",
      "Training log: 7 epoch (64 / 60000 train. data). Loss: 1.5148042440414429\n",
      "Training log: 7 epoch (704 / 60000 train. data). Loss: 1.7745072841644287\n",
      "Training log: 7 epoch (1344 / 60000 train. data). Loss: 1.4259573221206665\n",
      "Training log: 7 epoch (1984 / 60000 train. data). Loss: 1.5210925340652466\n",
      "Training log: 7 epoch (2624 / 60000 train. data). Loss: 1.5070186853408813\n",
      "Training log: 7 epoch (3264 / 60000 train. data). Loss: 1.6140162944793701\n",
      "Training log: 7 epoch (3904 / 60000 train. data). Loss: 1.428674340248108\n",
      "Training log: 7 epoch (4544 / 60000 train. data). Loss: 1.7057775259017944\n",
      "Training log: 7 epoch (5184 / 60000 train. data). Loss: 1.7842726707458496\n",
      "Training log: 7 epoch (5824 / 60000 train. data). Loss: 1.408934473991394\n",
      "Training log: 7 epoch (6464 / 60000 train. data). Loss: 1.4703012704849243\n",
      "Training log: 7 epoch (7104 / 60000 train. data). Loss: 1.4962950944900513\n",
      "Training log: 7 epoch (7744 / 60000 train. data). Loss: 1.8860570192337036\n",
      "Training log: 7 epoch (8384 / 60000 train. data). Loss: 1.4720501899719238\n",
      "Training log: 7 epoch (9024 / 60000 train. data). Loss: 1.8224505186080933\n",
      "Training log: 7 epoch (9664 / 60000 train. data). Loss: 1.5320311784744263\n",
      "Training log: 7 epoch (10304 / 60000 train. data). Loss: 1.8324421644210815\n",
      "Training log: 7 epoch (10944 / 60000 train. data). Loss: 1.6306382417678833\n",
      "Training log: 7 epoch (11584 / 60000 train. data). Loss: 1.4601153135299683\n",
      "Training log: 7 epoch (12224 / 60000 train. data). Loss: 1.5904616117477417\n",
      "Training log: 7 epoch (12864 / 60000 train. data). Loss: 1.4666588306427002\n",
      "Training log: 7 epoch (13504 / 60000 train. data). Loss: 1.3188889026641846\n",
      "Training log: 7 epoch (14144 / 60000 train. data). Loss: 1.4992451667785645\n",
      "Training log: 7 epoch (14784 / 60000 train. data). Loss: 1.454077124595642\n",
      "Training log: 7 epoch (15424 / 60000 train. data). Loss: 1.752474069595337\n",
      "Training log: 7 epoch (16064 / 60000 train. data). Loss: 1.5963616371154785\n",
      "Training log: 7 epoch (16704 / 60000 train. data). Loss: 1.6734294891357422\n",
      "Training log: 7 epoch (17344 / 60000 train. data). Loss: 1.4715960025787354\n",
      "Training log: 7 epoch (17984 / 60000 train. data). Loss: 1.4765355587005615\n",
      "Training log: 7 epoch (18624 / 60000 train. data). Loss: 1.5331348180770874\n",
      "Training log: 7 epoch (19264 / 60000 train. data). Loss: 1.5939942598342896\n",
      "Training log: 7 epoch (19904 / 60000 train. data). Loss: 1.6782830953598022\n",
      "Training log: 7 epoch (20544 / 60000 train. data). Loss: 1.4530611038208008\n",
      "Training log: 7 epoch (21184 / 60000 train. data). Loss: 1.38410222530365\n",
      "Training log: 7 epoch (21824 / 60000 train. data). Loss: 1.4232218265533447\n",
      "Training log: 7 epoch (22464 / 60000 train. data). Loss: 1.5286073684692383\n",
      "Training log: 7 epoch (23104 / 60000 train. data). Loss: 1.7282962799072266\n",
      "Training log: 7 epoch (23744 / 60000 train. data). Loss: 1.557773470878601\n",
      "Training log: 7 epoch (24384 / 60000 train. data). Loss: 1.6149463653564453\n",
      "Training log: 7 epoch (25024 / 60000 train. data). Loss: 1.6824368238449097\n",
      "Training log: 7 epoch (25664 / 60000 train. data). Loss: 1.4558674097061157\n",
      "Training log: 7 epoch (26304 / 60000 train. data). Loss: 1.6972813606262207\n",
      "Training log: 7 epoch (26944 / 60000 train. data). Loss: 1.4971036911010742\n",
      "Training log: 7 epoch (27584 / 60000 train. data). Loss: 1.406942367553711\n",
      "Training log: 7 epoch (28224 / 60000 train. data). Loss: 1.6855275630950928\n",
      "Training log: 7 epoch (28864 / 60000 train. data). Loss: 1.498856544494629\n",
      "Training log: 7 epoch (29504 / 60000 train. data). Loss: 1.67169189453125\n",
      "Training log: 7 epoch (30144 / 60000 train. data). Loss: 1.4843567609786987\n",
      "Training log: 7 epoch (30784 / 60000 train. data). Loss: 1.656785249710083\n",
      "Training log: 7 epoch (31424 / 60000 train. data). Loss: 1.7032760381698608\n",
      "Training log: 7 epoch (32064 / 60000 train. data). Loss: 1.5977888107299805\n",
      "Training log: 7 epoch (32704 / 60000 train. data). Loss: 1.674897313117981\n",
      "Training log: 7 epoch (33344 / 60000 train. data). Loss: 1.5161491632461548\n",
      "Training log: 7 epoch (33984 / 60000 train. data). Loss: 1.557491660118103\n",
      "Training log: 7 epoch (34624 / 60000 train. data). Loss: 1.5537950992584229\n",
      "Training log: 7 epoch (35264 / 60000 train. data). Loss: 1.5310696363449097\n",
      "Training log: 7 epoch (35904 / 60000 train. data). Loss: 1.3414409160614014\n",
      "Training log: 7 epoch (36544 / 60000 train. data). Loss: 1.3453571796417236\n",
      "Training log: 7 epoch (37184 / 60000 train. data). Loss: 1.2779630422592163\n",
      "Training log: 7 epoch (37824 / 60000 train. data). Loss: 1.7181001901626587\n",
      "Training log: 7 epoch (38464 / 60000 train. data). Loss: 1.3752374649047852\n",
      "Training log: 7 epoch (39104 / 60000 train. data). Loss: 1.611611008644104\n",
      "Training log: 7 epoch (39744 / 60000 train. data). Loss: 1.6971246004104614\n",
      "Training log: 7 epoch (40384 / 60000 train. data). Loss: 1.6385955810546875\n",
      "Training log: 7 epoch (41024 / 60000 train. data). Loss: 1.7596144676208496\n",
      "Training log: 7 epoch (41664 / 60000 train. data). Loss: 1.7238317728042603\n",
      "Training log: 7 epoch (42304 / 60000 train. data). Loss: 1.9705184698104858\n",
      "Training log: 7 epoch (42944 / 60000 train. data). Loss: 1.6456209421157837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 7 epoch (43584 / 60000 train. data). Loss: 1.647953987121582\n",
      "Training log: 7 epoch (44224 / 60000 train. data). Loss: 1.7242974042892456\n",
      "Training log: 7 epoch (44864 / 60000 train. data). Loss: 1.6877551078796387\n",
      "Training log: 7 epoch (45504 / 60000 train. data). Loss: 1.7250055074691772\n",
      "Training log: 7 epoch (46144 / 60000 train. data). Loss: 1.7385778427124023\n",
      "Training log: 7 epoch (46784 / 60000 train. data). Loss: 1.575919270515442\n",
      "Training log: 7 epoch (47424 / 60000 train. data). Loss: 1.746803641319275\n",
      "Training log: 7 epoch (48064 / 60000 train. data). Loss: 1.7173906564712524\n",
      "Training log: 7 epoch (48704 / 60000 train. data). Loss: 1.4109716415405273\n",
      "Training log: 7 epoch (49344 / 60000 train. data). Loss: 1.4582867622375488\n",
      "Training log: 7 epoch (49984 / 60000 train. data). Loss: 1.5446367263793945\n",
      "Test loss (avg): 1.6210983482895382, Accuracy: 0.4403\n",
      "Training log: 8 epoch (64 / 60000 train. data). Loss: 1.48987877368927\n",
      "Training log: 8 epoch (704 / 60000 train. data). Loss: 1.6086174249649048\n",
      "Training log: 8 epoch (1344 / 60000 train. data). Loss: 1.5671327114105225\n",
      "Training log: 8 epoch (1984 / 60000 train. data). Loss: 1.508675456047058\n",
      "Training log: 8 epoch (2624 / 60000 train. data). Loss: 1.8479194641113281\n",
      "Training log: 8 epoch (3264 / 60000 train. data). Loss: 1.6724185943603516\n",
      "Training log: 8 epoch (3904 / 60000 train. data). Loss: 1.8112447261810303\n",
      "Training log: 8 epoch (4544 / 60000 train. data). Loss: 1.6782927513122559\n",
      "Training log: 8 epoch (5184 / 60000 train. data). Loss: 1.6185575723648071\n",
      "Training log: 8 epoch (5824 / 60000 train. data). Loss: 1.381288766860962\n",
      "Training log: 8 epoch (6464 / 60000 train. data). Loss: 1.8249913454055786\n",
      "Training log: 8 epoch (7104 / 60000 train. data). Loss: 1.3375413417816162\n",
      "Training log: 8 epoch (7744 / 60000 train. data). Loss: 1.6683955192565918\n",
      "Training log: 8 epoch (8384 / 60000 train. data). Loss: 1.5595269203186035\n",
      "Training log: 8 epoch (9024 / 60000 train. data). Loss: 1.365079641342163\n",
      "Training log: 8 epoch (9664 / 60000 train. data). Loss: 1.5222452878952026\n",
      "Training log: 8 epoch (10304 / 60000 train. data). Loss: 1.6579819917678833\n",
      "Training log: 8 epoch (10944 / 60000 train. data). Loss: 1.5005110502243042\n",
      "Training log: 8 epoch (11584 / 60000 train. data). Loss: 1.8022613525390625\n",
      "Training log: 8 epoch (12224 / 60000 train. data). Loss: 1.7679171562194824\n",
      "Training log: 8 epoch (12864 / 60000 train. data). Loss: 1.562048316001892\n",
      "Training log: 8 epoch (13504 / 60000 train. data). Loss: 1.4858347177505493\n",
      "Training log: 8 epoch (14144 / 60000 train. data). Loss: 1.8291347026824951\n",
      "Training log: 8 epoch (14784 / 60000 train. data). Loss: 1.5138622522354126\n",
      "Training log: 8 epoch (15424 / 60000 train. data). Loss: 1.7512587308883667\n",
      "Training log: 8 epoch (16064 / 60000 train. data). Loss: 1.5875474214553833\n",
      "Training log: 8 epoch (16704 / 60000 train. data). Loss: 1.5404932498931885\n",
      "Training log: 8 epoch (17344 / 60000 train. data). Loss: 1.5855748653411865\n",
      "Training log: 8 epoch (17984 / 60000 train. data). Loss: 1.7919647693634033\n",
      "Training log: 8 epoch (18624 / 60000 train. data). Loss: 1.7065134048461914\n",
      "Training log: 8 epoch (19264 / 60000 train. data). Loss: 1.5909017324447632\n",
      "Training log: 8 epoch (19904 / 60000 train. data). Loss: 1.5210155248641968\n",
      "Training log: 8 epoch (20544 / 60000 train. data). Loss: 1.3013365268707275\n",
      "Training log: 8 epoch (21184 / 60000 train. data). Loss: 1.817211389541626\n",
      "Training log: 8 epoch (21824 / 60000 train. data). Loss: 1.6341931819915771\n",
      "Training log: 8 epoch (22464 / 60000 train. data). Loss: 1.4703435897827148\n",
      "Training log: 8 epoch (23104 / 60000 train. data). Loss: 1.4032071828842163\n",
      "Training log: 8 epoch (23744 / 60000 train. data). Loss: 1.6407049894332886\n",
      "Training log: 8 epoch (24384 / 60000 train. data). Loss: 1.630853533744812\n",
      "Training log: 8 epoch (25024 / 60000 train. data). Loss: 1.3275984525680542\n",
      "Training log: 8 epoch (25664 / 60000 train. data). Loss: 1.7011709213256836\n",
      "Training log: 8 epoch (26304 / 60000 train. data). Loss: 1.6060665845870972\n",
      "Training log: 8 epoch (26944 / 60000 train. data). Loss: 1.7499513626098633\n",
      "Training log: 8 epoch (27584 / 60000 train. data). Loss: 1.6730347871780396\n",
      "Training log: 8 epoch (28224 / 60000 train. data). Loss: 1.5457239151000977\n",
      "Training log: 8 epoch (28864 / 60000 train. data). Loss: 1.5990804433822632\n",
      "Training log: 8 epoch (29504 / 60000 train. data). Loss: 1.5854451656341553\n",
      "Training log: 8 epoch (30144 / 60000 train. data). Loss: 1.3798326253890991\n",
      "Training log: 8 epoch (30784 / 60000 train. data). Loss: 1.5122261047363281\n",
      "Training log: 8 epoch (31424 / 60000 train. data). Loss: 1.5486079454421997\n",
      "Training log: 8 epoch (32064 / 60000 train. data). Loss: 1.5121761560440063\n",
      "Training log: 8 epoch (32704 / 60000 train. data). Loss: 1.717272162437439\n",
      "Training log: 8 epoch (33344 / 60000 train. data). Loss: 1.525614619255066\n",
      "Training log: 8 epoch (33984 / 60000 train. data). Loss: 1.6508394479751587\n",
      "Training log: 8 epoch (34624 / 60000 train. data). Loss: 1.3191437721252441\n",
      "Training log: 8 epoch (35264 / 60000 train. data). Loss: 1.8095850944519043\n",
      "Training log: 8 epoch (35904 / 60000 train. data). Loss: 1.5939500331878662\n",
      "Training log: 8 epoch (36544 / 60000 train. data). Loss: 1.6873685121536255\n",
      "Training log: 8 epoch (37184 / 60000 train. data). Loss: 1.621991515159607\n",
      "Training log: 8 epoch (37824 / 60000 train. data). Loss: 1.5529875755310059\n",
      "Training log: 8 epoch (38464 / 60000 train. data). Loss: 1.4403066635131836\n",
      "Training log: 8 epoch (39104 / 60000 train. data). Loss: 1.5013896226882935\n",
      "Training log: 8 epoch (39744 / 60000 train. data). Loss: 1.488031029701233\n",
      "Training log: 8 epoch (40384 / 60000 train. data). Loss: 1.7415621280670166\n",
      "Training log: 8 epoch (41024 / 60000 train. data). Loss: 1.5320286750793457\n",
      "Training log: 8 epoch (41664 / 60000 train. data). Loss: 1.513895034790039\n",
      "Training log: 8 epoch (42304 / 60000 train. data). Loss: 1.3950388431549072\n",
      "Training log: 8 epoch (42944 / 60000 train. data). Loss: 1.846326231956482\n",
      "Training log: 8 epoch (43584 / 60000 train. data). Loss: 1.4278273582458496\n",
      "Training log: 8 epoch (44224 / 60000 train. data). Loss: 1.4781739711761475\n",
      "Training log: 8 epoch (44864 / 60000 train. data). Loss: 1.3920570611953735\n",
      "Training log: 8 epoch (45504 / 60000 train. data). Loss: 1.7017956972122192\n",
      "Training log: 8 epoch (46144 / 60000 train. data). Loss: 1.5792324542999268\n",
      "Training log: 8 epoch (46784 / 60000 train. data). Loss: 1.7813172340393066\n",
      "Training log: 8 epoch (47424 / 60000 train. data). Loss: 1.5745989084243774\n",
      "Training log: 8 epoch (48064 / 60000 train. data). Loss: 1.5911887884140015\n",
      "Training log: 8 epoch (48704 / 60000 train. data). Loss: 1.7013226747512817\n",
      "Training log: 8 epoch (49344 / 60000 train. data). Loss: 1.73403799533844\n",
      "Training log: 8 epoch (49984 / 60000 train. data). Loss: 1.553171157836914\n",
      "Test loss (avg): 1.642423692022919, Accuracy: 0.4282\n",
      "Training log: 9 epoch (64 / 60000 train. data). Loss: 1.478966474533081\n",
      "Training log: 9 epoch (704 / 60000 train. data). Loss: 1.6369853019714355\n",
      "Training log: 9 epoch (1344 / 60000 train. data). Loss: 1.5497727394104004\n",
      "Training log: 9 epoch (1984 / 60000 train. data). Loss: 1.4072612524032593\n",
      "Training log: 9 epoch (2624 / 60000 train. data). Loss: 1.7400243282318115\n",
      "Training log: 9 epoch (3264 / 60000 train. data). Loss: 1.4106472730636597\n",
      "Training log: 9 epoch (3904 / 60000 train. data). Loss: 1.3697562217712402\n",
      "Training log: 9 epoch (4544 / 60000 train. data). Loss: 1.4814032316207886\n",
      "Training log: 9 epoch (5184 / 60000 train. data). Loss: 1.5341154336929321\n",
      "Training log: 9 epoch (5824 / 60000 train. data). Loss: 1.442947268486023\n",
      "Training log: 9 epoch (6464 / 60000 train. data). Loss: 1.5549204349517822\n",
      "Training log: 9 epoch (7104 / 60000 train. data). Loss: 1.6042370796203613\n",
      "Training log: 9 epoch (7744 / 60000 train. data). Loss: 1.5454394817352295\n",
      "Training log: 9 epoch (8384 / 60000 train. data). Loss: 1.6453038454055786\n",
      "Training log: 9 epoch (9024 / 60000 train. data). Loss: 1.4183827638626099\n",
      "Training log: 9 epoch (9664 / 60000 train. data). Loss: 1.3424625396728516\n",
      "Training log: 9 epoch (10304 / 60000 train. data). Loss: 1.6743848323822021\n",
      "Training log: 9 epoch (10944 / 60000 train. data). Loss: 1.487656593322754\n",
      "Training log: 9 epoch (11584 / 60000 train. data). Loss: 1.372734785079956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 9 epoch (12224 / 60000 train. data). Loss: 1.4191097021102905\n",
      "Training log: 9 epoch (12864 / 60000 train. data). Loss: 1.348122477531433\n",
      "Training log: 9 epoch (13504 / 60000 train. data). Loss: 1.70465886592865\n",
      "Training log: 9 epoch (14144 / 60000 train. data). Loss: 1.5627206563949585\n",
      "Training log: 9 epoch (14784 / 60000 train. data). Loss: 1.4546242952346802\n",
      "Training log: 9 epoch (15424 / 60000 train. data). Loss: 1.4105706214904785\n",
      "Training log: 9 epoch (16064 / 60000 train. data). Loss: 1.6496328115463257\n",
      "Training log: 9 epoch (16704 / 60000 train. data). Loss: 1.5371664762496948\n",
      "Training log: 9 epoch (17344 / 60000 train. data). Loss: 1.4413923025131226\n",
      "Training log: 9 epoch (17984 / 60000 train. data). Loss: 1.3958059549331665\n",
      "Training log: 9 epoch (18624 / 60000 train. data). Loss: 1.3907883167266846\n",
      "Training log: 9 epoch (19264 / 60000 train. data). Loss: 1.4350438117980957\n",
      "Training log: 9 epoch (19904 / 60000 train. data). Loss: 1.3854392766952515\n",
      "Training log: 9 epoch (20544 / 60000 train. data). Loss: 1.3558553457260132\n",
      "Training log: 9 epoch (21184 / 60000 train. data). Loss: 1.5323522090911865\n",
      "Training log: 9 epoch (21824 / 60000 train. data). Loss: 1.6541366577148438\n",
      "Training log: 9 epoch (22464 / 60000 train. data). Loss: 1.4809657335281372\n",
      "Training log: 9 epoch (23104 / 60000 train. data). Loss: 1.5328209400177002\n",
      "Training log: 9 epoch (23744 / 60000 train. data). Loss: 1.412521481513977\n",
      "Training log: 9 epoch (24384 / 60000 train. data). Loss: 1.5137102603912354\n",
      "Training log: 9 epoch (25024 / 60000 train. data). Loss: 1.4391424655914307\n",
      "Training log: 9 epoch (25664 / 60000 train. data). Loss: 1.761834740638733\n",
      "Training log: 9 epoch (26304 / 60000 train. data). Loss: 1.7455435991287231\n",
      "Training log: 9 epoch (26944 / 60000 train. data). Loss: 1.381515383720398\n",
      "Training log: 9 epoch (27584 / 60000 train. data). Loss: 1.3602617979049683\n",
      "Training log: 9 epoch (28224 / 60000 train. data). Loss: 1.4157902002334595\n",
      "Training log: 9 epoch (28864 / 60000 train. data). Loss: 1.4737666845321655\n",
      "Training log: 9 epoch (29504 / 60000 train. data). Loss: 1.4252843856811523\n",
      "Training log: 9 epoch (30144 / 60000 train. data). Loss: 1.4390830993652344\n",
      "Training log: 9 epoch (30784 / 60000 train. data). Loss: 1.7329403162002563\n",
      "Training log: 9 epoch (31424 / 60000 train. data). Loss: 1.9457995891571045\n",
      "Training log: 9 epoch (32064 / 60000 train. data). Loss: 1.6461751461029053\n",
      "Training log: 9 epoch (32704 / 60000 train. data). Loss: 1.4197821617126465\n",
      "Training log: 9 epoch (33344 / 60000 train. data). Loss: 1.614186406135559\n",
      "Training log: 9 epoch (33984 / 60000 train. data). Loss: 1.5371912717819214\n",
      "Training log: 9 epoch (34624 / 60000 train. data). Loss: 1.6049212217330933\n",
      "Training log: 9 epoch (35264 / 60000 train. data). Loss: 1.4381728172302246\n",
      "Training log: 9 epoch (35904 / 60000 train. data). Loss: 1.5698940753936768\n",
      "Training log: 9 epoch (36544 / 60000 train. data). Loss: 1.5406248569488525\n",
      "Training log: 9 epoch (37184 / 60000 train. data). Loss: 1.7428876161575317\n",
      "Training log: 9 epoch (37824 / 60000 train. data). Loss: 1.550549864768982\n",
      "Training log: 9 epoch (38464 / 60000 train. data). Loss: 1.1203502416610718\n",
      "Training log: 9 epoch (39104 / 60000 train. data). Loss: 1.6849746704101562\n",
      "Training log: 9 epoch (39744 / 60000 train. data). Loss: 1.5233044624328613\n",
      "Training log: 9 epoch (40384 / 60000 train. data). Loss: 1.5995087623596191\n",
      "Training log: 9 epoch (41024 / 60000 train. data). Loss: 1.7509534358978271\n",
      "Training log: 9 epoch (41664 / 60000 train. data). Loss: 1.7168314456939697\n",
      "Training log: 9 epoch (42304 / 60000 train. data). Loss: 1.8590238094329834\n",
      "Training log: 9 epoch (42944 / 60000 train. data). Loss: 1.3896912336349487\n",
      "Training log: 9 epoch (43584 / 60000 train. data). Loss: 1.4297868013381958\n",
      "Training log: 9 epoch (44224 / 60000 train. data). Loss: 1.5764243602752686\n",
      "Training log: 9 epoch (44864 / 60000 train. data). Loss: 1.4909114837646484\n",
      "Training log: 9 epoch (45504 / 60000 train. data). Loss: 1.5905640125274658\n",
      "Training log: 9 epoch (46144 / 60000 train. data). Loss: 1.5531773567199707\n",
      "Training log: 9 epoch (46784 / 60000 train. data). Loss: 1.3407185077667236\n",
      "Training log: 9 epoch (47424 / 60000 train. data). Loss: 1.3685534000396729\n",
      "Training log: 9 epoch (48064 / 60000 train. data). Loss: 1.5126774311065674\n",
      "Training log: 9 epoch (48704 / 60000 train. data). Loss: 1.3120195865631104\n",
      "Training log: 9 epoch (49344 / 60000 train. data). Loss: 1.5348056554794312\n",
      "Training log: 9 epoch (49984 / 60000 train. data). Loss: 1.6545556783676147\n",
      "Test loss (avg): 1.5768233856577782, Accuracy: 0.4546\n",
      "Training log: 10 epoch (64 / 60000 train. data). Loss: 1.6267999410629272\n",
      "Training log: 10 epoch (704 / 60000 train. data). Loss: 1.4621009826660156\n",
      "Training log: 10 epoch (1344 / 60000 train. data). Loss: 1.611784815788269\n",
      "Training log: 10 epoch (1984 / 60000 train. data). Loss: 1.8327500820159912\n",
      "Training log: 10 epoch (2624 / 60000 train. data). Loss: 1.9256190061569214\n",
      "Training log: 10 epoch (3264 / 60000 train. data). Loss: 1.4062449932098389\n",
      "Training log: 10 epoch (3904 / 60000 train. data). Loss: 1.4455417394638062\n",
      "Training log: 10 epoch (4544 / 60000 train. data). Loss: 1.2763996124267578\n",
      "Training log: 10 epoch (5184 / 60000 train. data). Loss: 1.5334702730178833\n",
      "Training log: 10 epoch (5824 / 60000 train. data). Loss: 1.7065670490264893\n",
      "Training log: 10 epoch (6464 / 60000 train. data). Loss: 1.5050369501113892\n",
      "Training log: 10 epoch (7104 / 60000 train. data). Loss: 1.6167763471603394\n",
      "Training log: 10 epoch (7744 / 60000 train. data). Loss: 1.600953459739685\n",
      "Training log: 10 epoch (8384 / 60000 train. data). Loss: 1.2565006017684937\n",
      "Training log: 10 epoch (9024 / 60000 train. data). Loss: 1.387239933013916\n",
      "Training log: 10 epoch (9664 / 60000 train. data). Loss: 1.7746975421905518\n",
      "Training log: 10 epoch (10304 / 60000 train. data). Loss: 1.3504973649978638\n",
      "Training log: 10 epoch (10944 / 60000 train. data). Loss: 1.4833018779754639\n",
      "Training log: 10 epoch (11584 / 60000 train. data). Loss: 1.4616496562957764\n",
      "Training log: 10 epoch (12224 / 60000 train. data). Loss: 1.4267369508743286\n",
      "Training log: 10 epoch (12864 / 60000 train. data). Loss: 1.6274553537368774\n",
      "Training log: 10 epoch (13504 / 60000 train. data). Loss: 1.3022913932800293\n",
      "Training log: 10 epoch (14144 / 60000 train. data). Loss: 1.4942694902420044\n",
      "Training log: 10 epoch (14784 / 60000 train. data). Loss: 1.6456053256988525\n",
      "Training log: 10 epoch (15424 / 60000 train. data). Loss: 1.5373729467391968\n",
      "Training log: 10 epoch (16064 / 60000 train. data). Loss: 1.8057496547698975\n",
      "Training log: 10 epoch (16704 / 60000 train. data). Loss: 1.4458792209625244\n",
      "Training log: 10 epoch (17344 / 60000 train. data). Loss: 1.3824785947799683\n",
      "Training log: 10 epoch (17984 / 60000 train. data). Loss: 1.4474622011184692\n",
      "Training log: 10 epoch (18624 / 60000 train. data). Loss: 1.5437101125717163\n",
      "Training log: 10 epoch (19264 / 60000 train. data). Loss: 1.5100634098052979\n",
      "Training log: 10 epoch (19904 / 60000 train. data). Loss: 1.2379186153411865\n",
      "Training log: 10 epoch (20544 / 60000 train. data). Loss: 1.4916954040527344\n",
      "Training log: 10 epoch (21184 / 60000 train. data). Loss: 1.45893394947052\n",
      "Training log: 10 epoch (21824 / 60000 train. data). Loss: 1.6428406238555908\n",
      "Training log: 10 epoch (22464 / 60000 train. data). Loss: 1.4602797031402588\n",
      "Training log: 10 epoch (23104 / 60000 train. data). Loss: 1.5396569967269897\n",
      "Training log: 10 epoch (23744 / 60000 train. data). Loss: 1.4484113454818726\n",
      "Training log: 10 epoch (24384 / 60000 train. data). Loss: 1.4820491075515747\n",
      "Training log: 10 epoch (25024 / 60000 train. data). Loss: 1.3173781633377075\n",
      "Training log: 10 epoch (25664 / 60000 train. data). Loss: 1.4581806659698486\n",
      "Training log: 10 epoch (26304 / 60000 train. data). Loss: 1.6004869937896729\n",
      "Training log: 10 epoch (26944 / 60000 train. data). Loss: 1.406754732131958\n",
      "Training log: 10 epoch (27584 / 60000 train. data). Loss: 1.5254722833633423\n",
      "Training log: 10 epoch (28224 / 60000 train. data). Loss: 1.4936732053756714\n",
      "Training log: 10 epoch (28864 / 60000 train. data). Loss: 1.6767688989639282\n",
      "Training log: 10 epoch (29504 / 60000 train. data). Loss: 1.4964474439620972\n",
      "Training log: 10 epoch (30144 / 60000 train. data). Loss: 1.4929999113082886\n",
      "Training log: 10 epoch (30784 / 60000 train. data). Loss: 1.6464645862579346\n",
      "Training log: 10 epoch (31424 / 60000 train. data). Loss: 1.3909046649932861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 10 epoch (32064 / 60000 train. data). Loss: 1.5816302299499512\n",
      "Training log: 10 epoch (32704 / 60000 train. data). Loss: 1.4248048067092896\n",
      "Training log: 10 epoch (33344 / 60000 train. data). Loss: 1.4809116125106812\n",
      "Training log: 10 epoch (33984 / 60000 train. data). Loss: 1.630377173423767\n",
      "Training log: 10 epoch (34624 / 60000 train. data). Loss: 1.6003516912460327\n",
      "Training log: 10 epoch (35264 / 60000 train. data). Loss: 1.5445939302444458\n",
      "Training log: 10 epoch (35904 / 60000 train. data). Loss: 1.5442038774490356\n",
      "Training log: 10 epoch (36544 / 60000 train. data). Loss: 1.4276968240737915\n",
      "Training log: 10 epoch (37184 / 60000 train. data). Loss: 1.5097804069519043\n",
      "Training log: 10 epoch (37824 / 60000 train. data). Loss: 1.6219992637634277\n",
      "Training log: 10 epoch (38464 / 60000 train. data). Loss: 1.6089457273483276\n",
      "Training log: 10 epoch (39104 / 60000 train. data). Loss: 1.4481908082962036\n",
      "Training log: 10 epoch (39744 / 60000 train. data). Loss: 1.3050100803375244\n",
      "Training log: 10 epoch (40384 / 60000 train. data). Loss: 1.6807048320770264\n",
      "Training log: 10 epoch (41024 / 60000 train. data). Loss: 1.6280032396316528\n",
      "Training log: 10 epoch (41664 / 60000 train. data). Loss: 1.2996087074279785\n",
      "Training log: 10 epoch (42304 / 60000 train. data). Loss: 1.513694405555725\n",
      "Training log: 10 epoch (42944 / 60000 train. data). Loss: 1.5889708995819092\n",
      "Training log: 10 epoch (43584 / 60000 train. data). Loss: 1.3393269777297974\n",
      "Training log: 10 epoch (44224 / 60000 train. data). Loss: 1.2324734926223755\n",
      "Training log: 10 epoch (44864 / 60000 train. data). Loss: 1.6772456169128418\n",
      "Training log: 10 epoch (45504 / 60000 train. data). Loss: 1.3038865327835083\n",
      "Training log: 10 epoch (46144 / 60000 train. data). Loss: 1.5973422527313232\n",
      "Training log: 10 epoch (46784 / 60000 train. data). Loss: 1.5361754894256592\n",
      "Training log: 10 epoch (47424 / 60000 train. data). Loss: 1.7536019086837769\n",
      "Training log: 10 epoch (48064 / 60000 train. data). Loss: 1.3203692436218262\n",
      "Training log: 10 epoch (48704 / 60000 train. data). Loss: 1.528734803199768\n",
      "Training log: 10 epoch (49344 / 60000 train. data). Loss: 1.669663906097412\n",
      "Training log: 10 epoch (49984 / 60000 train. data). Loss: 1.7320377826690674\n",
      "Test loss (avg): 1.568482499213735, Accuracy: 0.4619\n",
      "Training log: 11 epoch (64 / 60000 train. data). Loss: 1.7195096015930176\n",
      "Training log: 11 epoch (704 / 60000 train. data). Loss: 1.8441362380981445\n",
      "Training log: 11 epoch (1344 / 60000 train. data). Loss: 1.271396279335022\n",
      "Training log: 11 epoch (1984 / 60000 train. data). Loss: 1.540055513381958\n",
      "Training log: 11 epoch (2624 / 60000 train. data). Loss: 1.6205101013183594\n",
      "Training log: 11 epoch (3264 / 60000 train. data). Loss: 1.296582818031311\n",
      "Training log: 11 epoch (3904 / 60000 train. data). Loss: 1.3905525207519531\n",
      "Training log: 11 epoch (4544 / 60000 train. data). Loss: 1.489216685295105\n",
      "Training log: 11 epoch (5184 / 60000 train. data). Loss: 1.4402146339416504\n",
      "Training log: 11 epoch (5824 / 60000 train. data). Loss: 1.355103850364685\n",
      "Training log: 11 epoch (6464 / 60000 train. data). Loss: 1.5585219860076904\n",
      "Training log: 11 epoch (7104 / 60000 train. data). Loss: 1.4778918027877808\n",
      "Training log: 11 epoch (7744 / 60000 train. data). Loss: 1.6082584857940674\n",
      "Training log: 11 epoch (8384 / 60000 train. data). Loss: 1.3620364665985107\n",
      "Training log: 11 epoch (9024 / 60000 train. data). Loss: 1.0712354183197021\n",
      "Training log: 11 epoch (9664 / 60000 train. data). Loss: 1.2739441394805908\n",
      "Training log: 11 epoch (10304 / 60000 train. data). Loss: 1.7432897090911865\n",
      "Training log: 11 epoch (10944 / 60000 train. data). Loss: 1.498547911643982\n",
      "Training log: 11 epoch (11584 / 60000 train. data). Loss: 1.595341444015503\n",
      "Training log: 11 epoch (12224 / 60000 train. data). Loss: 1.7330626249313354\n",
      "Training log: 11 epoch (12864 / 60000 train. data). Loss: 1.529503345489502\n",
      "Training log: 11 epoch (13504 / 60000 train. data). Loss: 1.5962060689926147\n",
      "Training log: 11 epoch (14144 / 60000 train. data). Loss: 1.2907180786132812\n",
      "Training log: 11 epoch (14784 / 60000 train. data). Loss: 1.3408620357513428\n",
      "Training log: 11 epoch (15424 / 60000 train. data). Loss: 1.271835446357727\n",
      "Training log: 11 epoch (16064 / 60000 train. data). Loss: 1.344452977180481\n",
      "Training log: 11 epoch (16704 / 60000 train. data). Loss: 1.3992434740066528\n",
      "Training log: 11 epoch (17344 / 60000 train. data). Loss: 1.3893206119537354\n",
      "Training log: 11 epoch (17984 / 60000 train. data). Loss: 1.4572662115097046\n",
      "Training log: 11 epoch (18624 / 60000 train. data). Loss: 1.523868441581726\n",
      "Training log: 11 epoch (19264 / 60000 train. data). Loss: 1.4304554462432861\n",
      "Training log: 11 epoch (19904 / 60000 train. data). Loss: 1.5712167024612427\n",
      "Training log: 11 epoch (20544 / 60000 train. data). Loss: 1.3320955038070679\n",
      "Training log: 11 epoch (21184 / 60000 train. data). Loss: 1.3724464178085327\n",
      "Training log: 11 epoch (21824 / 60000 train. data). Loss: 1.6185200214385986\n",
      "Training log: 11 epoch (22464 / 60000 train. data). Loss: 1.6638292074203491\n",
      "Training log: 11 epoch (23104 / 60000 train. data). Loss: 1.3877161741256714\n",
      "Training log: 11 epoch (23744 / 60000 train. data). Loss: 1.8398677110671997\n",
      "Training log: 11 epoch (24384 / 60000 train. data). Loss: 1.6693508625030518\n",
      "Training log: 11 epoch (25024 / 60000 train. data). Loss: 1.4748839139938354\n",
      "Training log: 11 epoch (25664 / 60000 train. data). Loss: 1.5556548833847046\n",
      "Training log: 11 epoch (26304 / 60000 train. data). Loss: 1.500516414642334\n",
      "Training log: 11 epoch (26944 / 60000 train. data). Loss: 1.31406831741333\n",
      "Training log: 11 epoch (27584 / 60000 train. data). Loss: 1.4552286863327026\n",
      "Training log: 11 epoch (28224 / 60000 train. data). Loss: 1.619404911994934\n",
      "Training log: 11 epoch (28864 / 60000 train. data). Loss: 1.3104488849639893\n",
      "Training log: 11 epoch (29504 / 60000 train. data). Loss: 1.3010361194610596\n",
      "Training log: 11 epoch (30144 / 60000 train. data). Loss: 1.3959342241287231\n",
      "Training log: 11 epoch (30784 / 60000 train. data). Loss: 1.6480607986450195\n",
      "Training log: 11 epoch (31424 / 60000 train. data). Loss: 1.5751124620437622\n",
      "Training log: 11 epoch (32064 / 60000 train. data). Loss: 1.2177602052688599\n",
      "Training log: 11 epoch (32704 / 60000 train. data). Loss: 1.452376365661621\n",
      "Training log: 11 epoch (33344 / 60000 train. data). Loss: 1.2838128805160522\n",
      "Training log: 11 epoch (33984 / 60000 train. data). Loss: 1.5240905284881592\n",
      "Training log: 11 epoch (34624 / 60000 train. data). Loss: 1.6081784963607788\n",
      "Training log: 11 epoch (35264 / 60000 train. data). Loss: 1.6736934185028076\n",
      "Training log: 11 epoch (35904 / 60000 train. data). Loss: 1.5579454898834229\n",
      "Training log: 11 epoch (36544 / 60000 train. data). Loss: 1.4594703912734985\n",
      "Training log: 11 epoch (37184 / 60000 train. data). Loss: 1.6731761693954468\n",
      "Training log: 11 epoch (37824 / 60000 train. data). Loss: 1.5145659446716309\n",
      "Training log: 11 epoch (38464 / 60000 train. data). Loss: 1.540266990661621\n",
      "Training log: 11 epoch (39104 / 60000 train. data). Loss: 1.3001564741134644\n",
      "Training log: 11 epoch (39744 / 60000 train. data). Loss: 1.6085450649261475\n",
      "Training log: 11 epoch (40384 / 60000 train. data). Loss: 1.44227933883667\n",
      "Training log: 11 epoch (41024 / 60000 train. data). Loss: 1.4719407558441162\n",
      "Training log: 11 epoch (41664 / 60000 train. data). Loss: 1.712546706199646\n",
      "Training log: 11 epoch (42304 / 60000 train. data). Loss: 1.67851984500885\n",
      "Training log: 11 epoch (42944 / 60000 train. data). Loss: 1.3850270509719849\n",
      "Training log: 11 epoch (43584 / 60000 train. data). Loss: 1.4809271097183228\n",
      "Training log: 11 epoch (44224 / 60000 train. data). Loss: 1.6378190517425537\n",
      "Training log: 11 epoch (44864 / 60000 train. data). Loss: 1.6401143074035645\n",
      "Training log: 11 epoch (45504 / 60000 train. data). Loss: 1.5060464143753052\n",
      "Training log: 11 epoch (46144 / 60000 train. data). Loss: 1.6434880495071411\n",
      "Training log: 11 epoch (46784 / 60000 train. data). Loss: 1.4460573196411133\n",
      "Training log: 11 epoch (47424 / 60000 train. data). Loss: 1.3535348176956177\n",
      "Training log: 11 epoch (48064 / 60000 train. data). Loss: 1.2739299535751343\n",
      "Training log: 11 epoch (48704 / 60000 train. data). Loss: 1.5510953664779663\n",
      "Training log: 11 epoch (49344 / 60000 train. data). Loss: 1.5926399230957031\n",
      "Training log: 11 epoch (49984 / 60000 train. data). Loss: 1.4591777324676514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (avg): 1.5727734011449632, Accuracy: 0.4593\n",
      "Training log: 12 epoch (64 / 60000 train. data). Loss: 1.9524587392807007\n",
      "Training log: 12 epoch (704 / 60000 train. data). Loss: 1.4161707162857056\n",
      "Training log: 12 epoch (1344 / 60000 train. data). Loss: 1.460782766342163\n",
      "Training log: 12 epoch (1984 / 60000 train. data). Loss: 1.4335306882858276\n",
      "Training log: 12 epoch (2624 / 60000 train. data). Loss: 1.3335578441619873\n",
      "Training log: 12 epoch (3264 / 60000 train. data). Loss: 1.4093763828277588\n",
      "Training log: 12 epoch (3904 / 60000 train. data). Loss: 1.6243489980697632\n",
      "Training log: 12 epoch (4544 / 60000 train. data). Loss: 1.4155032634735107\n",
      "Training log: 12 epoch (5184 / 60000 train. data). Loss: 1.3634988069534302\n",
      "Training log: 12 epoch (5824 / 60000 train. data). Loss: 1.3188453912734985\n",
      "Training log: 12 epoch (6464 / 60000 train. data). Loss: 1.6646666526794434\n",
      "Training log: 12 epoch (7104 / 60000 train. data). Loss: 1.4809291362762451\n",
      "Training log: 12 epoch (7744 / 60000 train. data). Loss: 1.4888595342636108\n",
      "Training log: 12 epoch (8384 / 60000 train. data). Loss: 1.588265061378479\n",
      "Training log: 12 epoch (9024 / 60000 train. data). Loss: 1.5307976007461548\n",
      "Training log: 12 epoch (9664 / 60000 train. data). Loss: 1.2292072772979736\n",
      "Training log: 12 epoch (10304 / 60000 train. data). Loss: 1.3775221109390259\n",
      "Training log: 12 epoch (10944 / 60000 train. data). Loss: 1.4099094867706299\n",
      "Training log: 12 epoch (11584 / 60000 train. data). Loss: 1.2112529277801514\n",
      "Training log: 12 epoch (12224 / 60000 train. data). Loss: 1.4133198261260986\n",
      "Training log: 12 epoch (12864 / 60000 train. data). Loss: 1.2632520198822021\n",
      "Training log: 12 epoch (13504 / 60000 train. data). Loss: 1.3425999879837036\n",
      "Training log: 12 epoch (14144 / 60000 train. data). Loss: 1.4427977800369263\n",
      "Training log: 12 epoch (14784 / 60000 train. data). Loss: 1.5516070127487183\n",
      "Training log: 12 epoch (15424 / 60000 train. data). Loss: 1.479561686515808\n",
      "Training log: 12 epoch (16064 / 60000 train. data). Loss: 1.6409722566604614\n",
      "Training log: 12 epoch (16704 / 60000 train. data). Loss: 1.5031229257583618\n",
      "Training log: 12 epoch (17344 / 60000 train. data). Loss: 1.362471580505371\n",
      "Training log: 12 epoch (17984 / 60000 train. data). Loss: 1.4151049852371216\n",
      "Training log: 12 epoch (18624 / 60000 train. data). Loss: 1.2867330312728882\n",
      "Training log: 12 epoch (19264 / 60000 train. data). Loss: 1.5243377685546875\n",
      "Training log: 12 epoch (19904 / 60000 train. data). Loss: 1.5050419569015503\n",
      "Training log: 12 epoch (20544 / 60000 train. data). Loss: 1.1123307943344116\n",
      "Training log: 12 epoch (21184 / 60000 train. data). Loss: 1.2669095993041992\n",
      "Training log: 12 epoch (21824 / 60000 train. data). Loss: 1.5890579223632812\n",
      "Training log: 12 epoch (22464 / 60000 train. data). Loss: 1.4601653814315796\n",
      "Training log: 12 epoch (23104 / 60000 train. data). Loss: 1.5023242235183716\n",
      "Training log: 12 epoch (23744 / 60000 train. data). Loss: 1.499269723892212\n",
      "Training log: 12 epoch (24384 / 60000 train. data). Loss: 1.6021513938903809\n",
      "Training log: 12 epoch (25024 / 60000 train. data). Loss: 1.4332267045974731\n",
      "Training log: 12 epoch (25664 / 60000 train. data). Loss: 1.6037737131118774\n",
      "Training log: 12 epoch (26304 / 60000 train. data). Loss: 1.262972354888916\n",
      "Training log: 12 epoch (26944 / 60000 train. data). Loss: 1.3893663883209229\n",
      "Training log: 12 epoch (27584 / 60000 train. data). Loss: 1.132647156715393\n",
      "Training log: 12 epoch (28224 / 60000 train. data). Loss: 1.4365575313568115\n",
      "Training log: 12 epoch (28864 / 60000 train. data). Loss: 1.5846811532974243\n",
      "Training log: 12 epoch (29504 / 60000 train. data). Loss: 1.343244194984436\n",
      "Training log: 12 epoch (30144 / 60000 train. data). Loss: 1.420042634010315\n",
      "Training log: 12 epoch (30784 / 60000 train. data). Loss: 1.4007004499435425\n",
      "Training log: 12 epoch (31424 / 60000 train. data). Loss: 1.4481472969055176\n",
      "Training log: 12 epoch (32064 / 60000 train. data). Loss: 1.4016755819320679\n",
      "Training log: 12 epoch (32704 / 60000 train. data). Loss: 1.3603990077972412\n",
      "Training log: 12 epoch (33344 / 60000 train. data). Loss: 1.5410693883895874\n",
      "Training log: 12 epoch (33984 / 60000 train. data). Loss: 1.2952734231948853\n",
      "Training log: 12 epoch (34624 / 60000 train. data). Loss: 1.3813529014587402\n",
      "Training log: 12 epoch (35264 / 60000 train. data). Loss: 1.5963026285171509\n",
      "Training log: 12 epoch (35904 / 60000 train. data). Loss: 1.4426077604293823\n",
      "Training log: 12 epoch (36544 / 60000 train. data). Loss: 1.3362947702407837\n",
      "Training log: 12 epoch (37184 / 60000 train. data). Loss: 1.3385428190231323\n",
      "Training log: 12 epoch (37824 / 60000 train. data). Loss: 1.2275444269180298\n",
      "Training log: 12 epoch (38464 / 60000 train. data). Loss: 1.5331063270568848\n",
      "Training log: 12 epoch (39104 / 60000 train. data). Loss: 1.3796076774597168\n",
      "Training log: 12 epoch (39744 / 60000 train. data). Loss: 1.4741897583007812\n",
      "Training log: 12 epoch (40384 / 60000 train. data). Loss: 1.3802850246429443\n",
      "Training log: 12 epoch (41024 / 60000 train. data). Loss: 1.631371259689331\n",
      "Training log: 12 epoch (41664 / 60000 train. data). Loss: 1.5037176609039307\n",
      "Training log: 12 epoch (42304 / 60000 train. data). Loss: 1.4750890731811523\n",
      "Training log: 12 epoch (42944 / 60000 train. data). Loss: 1.3634850978851318\n",
      "Training log: 12 epoch (43584 / 60000 train. data). Loss: 1.3438349962234497\n",
      "Training log: 12 epoch (44224 / 60000 train. data). Loss: 1.4944143295288086\n",
      "Training log: 12 epoch (44864 / 60000 train. data). Loss: 1.4329842329025269\n",
      "Training log: 12 epoch (45504 / 60000 train. data). Loss: 1.2876664400100708\n",
      "Training log: 12 epoch (46144 / 60000 train. data). Loss: 1.4052597284317017\n",
      "Training log: 12 epoch (46784 / 60000 train. data). Loss: 1.5398364067077637\n",
      "Training log: 12 epoch (47424 / 60000 train. data). Loss: 1.4777709245681763\n",
      "Training log: 12 epoch (48064 / 60000 train. data). Loss: 1.4917082786560059\n",
      "Training log: 12 epoch (48704 / 60000 train. data). Loss: 1.6607950925827026\n",
      "Training log: 12 epoch (49344 / 60000 train. data). Loss: 1.5214204788208008\n",
      "Training log: 12 epoch (49984 / 60000 train. data). Loss: 1.4888174533843994\n",
      "Test loss (avg): 1.572849355685483, Accuracy: 0.46\n",
      "Training log: 13 epoch (64 / 60000 train. data). Loss: 1.533098816871643\n",
      "Training log: 13 epoch (704 / 60000 train. data). Loss: 1.3059507608413696\n",
      "Training log: 13 epoch (1344 / 60000 train. data). Loss: 1.6177364587783813\n",
      "Training log: 13 epoch (1984 / 60000 train. data). Loss: 1.2199991941452026\n",
      "Training log: 13 epoch (2624 / 60000 train. data). Loss: 1.4351569414138794\n",
      "Training log: 13 epoch (3264 / 60000 train. data). Loss: 1.5046494007110596\n",
      "Training log: 13 epoch (3904 / 60000 train. data). Loss: 1.4438366889953613\n",
      "Training log: 13 epoch (4544 / 60000 train. data). Loss: 1.4592180252075195\n",
      "Training log: 13 epoch (5184 / 60000 train. data). Loss: 1.9000047445297241\n",
      "Training log: 13 epoch (5824 / 60000 train. data). Loss: 1.1639485359191895\n",
      "Training log: 13 epoch (6464 / 60000 train. data). Loss: 1.2899976968765259\n",
      "Training log: 13 epoch (7104 / 60000 train. data). Loss: 1.3090764284133911\n",
      "Training log: 13 epoch (7744 / 60000 train. data). Loss: 1.3515039682388306\n",
      "Training log: 13 epoch (8384 / 60000 train. data). Loss: 1.3394474983215332\n",
      "Training log: 13 epoch (9024 / 60000 train. data). Loss: 1.445250153541565\n",
      "Training log: 13 epoch (9664 / 60000 train. data). Loss: 1.428539752960205\n",
      "Training log: 13 epoch (10304 / 60000 train. data). Loss: 1.7002826929092407\n",
      "Training log: 13 epoch (10944 / 60000 train. data). Loss: 1.600669026374817\n",
      "Training log: 13 epoch (11584 / 60000 train. data). Loss: 1.6132910251617432\n",
      "Training log: 13 epoch (12224 / 60000 train. data). Loss: 1.4946039915084839\n",
      "Training log: 13 epoch (12864 / 60000 train. data). Loss: 1.320567011833191\n",
      "Training log: 13 epoch (13504 / 60000 train. data). Loss: 1.4828097820281982\n",
      "Training log: 13 epoch (14144 / 60000 train. data). Loss: 1.2615947723388672\n",
      "Training log: 13 epoch (14784 / 60000 train. data). Loss: 1.8398276567459106\n",
      "Training log: 13 epoch (15424 / 60000 train. data). Loss: 1.3475710153579712\n",
      "Training log: 13 epoch (16064 / 60000 train. data). Loss: 1.3364131450653076\n",
      "Training log: 13 epoch (16704 / 60000 train. data). Loss: 1.2611793279647827\n",
      "Training log: 13 epoch (17344 / 60000 train. data). Loss: 1.4869333505630493\n",
      "Training log: 13 epoch (17984 / 60000 train. data). Loss: 1.6623488664627075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 13 epoch (18624 / 60000 train. data). Loss: 1.4635801315307617\n",
      "Training log: 13 epoch (19264 / 60000 train. data). Loss: 1.3528951406478882\n",
      "Training log: 13 epoch (19904 / 60000 train. data). Loss: 1.407533049583435\n",
      "Training log: 13 epoch (20544 / 60000 train. data). Loss: 1.0759882926940918\n",
      "Training log: 13 epoch (21184 / 60000 train. data). Loss: 1.3077590465545654\n",
      "Training log: 13 epoch (21824 / 60000 train. data). Loss: 1.408801794052124\n",
      "Training log: 13 epoch (22464 / 60000 train. data). Loss: 1.116182565689087\n",
      "Training log: 13 epoch (23104 / 60000 train. data). Loss: 1.5863953828811646\n",
      "Training log: 13 epoch (23744 / 60000 train. data). Loss: 1.250091791152954\n",
      "Training log: 13 epoch (24384 / 60000 train. data). Loss: 1.5031683444976807\n",
      "Training log: 13 epoch (25024 / 60000 train. data). Loss: 1.150505781173706\n",
      "Training log: 13 epoch (25664 / 60000 train. data). Loss: 1.447898268699646\n",
      "Training log: 13 epoch (26304 / 60000 train. data). Loss: 1.4472928047180176\n",
      "Training log: 13 epoch (26944 / 60000 train. data). Loss: 1.4140909910202026\n",
      "Training log: 13 epoch (27584 / 60000 train. data). Loss: 1.4056384563446045\n",
      "Training log: 13 epoch (28224 / 60000 train. data). Loss: 1.3376256227493286\n",
      "Training log: 13 epoch (28864 / 60000 train. data). Loss: 1.512136697769165\n",
      "Training log: 13 epoch (29504 / 60000 train. data). Loss: 1.386830449104309\n",
      "Training log: 13 epoch (30144 / 60000 train. data). Loss: 1.283748984336853\n",
      "Training log: 13 epoch (30784 / 60000 train. data). Loss: 1.558014988899231\n",
      "Training log: 13 epoch (31424 / 60000 train. data). Loss: 1.422805905342102\n",
      "Training log: 13 epoch (32064 / 60000 train. data). Loss: 1.3905049562454224\n",
      "Training log: 13 epoch (32704 / 60000 train. data). Loss: 1.3354082107543945\n",
      "Training log: 13 epoch (33344 / 60000 train. data). Loss: 1.5036165714263916\n",
      "Training log: 13 epoch (33984 / 60000 train. data). Loss: 1.3617115020751953\n",
      "Training log: 13 epoch (34624 / 60000 train. data). Loss: 1.2724518775939941\n",
      "Training log: 13 epoch (35264 / 60000 train. data). Loss: 1.4784296751022339\n",
      "Training log: 13 epoch (35904 / 60000 train. data). Loss: 1.2511612176895142\n",
      "Training log: 13 epoch (36544 / 60000 train. data). Loss: 1.3950904607772827\n",
      "Training log: 13 epoch (37184 / 60000 train. data). Loss: 1.2970709800720215\n",
      "Training log: 13 epoch (37824 / 60000 train. data). Loss: 1.56240713596344\n",
      "Training log: 13 epoch (38464 / 60000 train. data). Loss: 1.5881245136260986\n",
      "Training log: 13 epoch (39104 / 60000 train. data). Loss: 1.331931471824646\n",
      "Training log: 13 epoch (39744 / 60000 train. data). Loss: 1.3863787651062012\n",
      "Training log: 13 epoch (40384 / 60000 train. data). Loss: 1.494233250617981\n",
      "Training log: 13 epoch (41024 / 60000 train. data). Loss: 1.1442142724990845\n",
      "Training log: 13 epoch (41664 / 60000 train. data). Loss: 1.5247433185577393\n",
      "Training log: 13 epoch (42304 / 60000 train. data). Loss: 1.5870592594146729\n",
      "Training log: 13 epoch (42944 / 60000 train. data). Loss: 1.45311439037323\n",
      "Training log: 13 epoch (43584 / 60000 train. data). Loss: 1.1829696893692017\n",
      "Training log: 13 epoch (44224 / 60000 train. data). Loss: 1.6917359828948975\n",
      "Training log: 13 epoch (44864 / 60000 train. data). Loss: 1.2736740112304688\n",
      "Training log: 13 epoch (45504 / 60000 train. data). Loss: 1.4801677465438843\n",
      "Training log: 13 epoch (46144 / 60000 train. data). Loss: 1.5201151371002197\n",
      "Training log: 13 epoch (46784 / 60000 train. data). Loss: 1.2577450275421143\n",
      "Training log: 13 epoch (47424 / 60000 train. data). Loss: 1.4651360511779785\n",
      "Training log: 13 epoch (48064 / 60000 train. data). Loss: 1.4013901948928833\n",
      "Training log: 13 epoch (48704 / 60000 train. data). Loss: 1.5083645582199097\n",
      "Training log: 13 epoch (49344 / 60000 train. data). Loss: 1.5635758638381958\n",
      "Training log: 13 epoch (49984 / 60000 train. data). Loss: 1.5638149976730347\n",
      "Test loss (avg): 1.5405113871689815, Accuracy: 0.4595\n",
      "Training log: 14 epoch (64 / 60000 train. data). Loss: 1.5279536247253418\n",
      "Training log: 14 epoch (704 / 60000 train. data). Loss: 1.294969916343689\n",
      "Training log: 14 epoch (1344 / 60000 train. data). Loss: 1.4324617385864258\n",
      "Training log: 14 epoch (1984 / 60000 train. data). Loss: 1.289319634437561\n",
      "Training log: 14 epoch (2624 / 60000 train. data). Loss: 1.3700076341629028\n",
      "Training log: 14 epoch (3264 / 60000 train. data). Loss: 1.3022358417510986\n",
      "Training log: 14 epoch (3904 / 60000 train. data). Loss: 1.4031250476837158\n",
      "Training log: 14 epoch (4544 / 60000 train. data). Loss: 1.355987548828125\n",
      "Training log: 14 epoch (5184 / 60000 train. data). Loss: 1.2620973587036133\n",
      "Training log: 14 epoch (5824 / 60000 train. data). Loss: 1.5662739276885986\n",
      "Training log: 14 epoch (6464 / 60000 train. data). Loss: 1.4277821779251099\n",
      "Training log: 14 epoch (7104 / 60000 train. data). Loss: 1.4461114406585693\n",
      "Training log: 14 epoch (7744 / 60000 train. data). Loss: 1.267946481704712\n",
      "Training log: 14 epoch (8384 / 60000 train. data). Loss: 1.4180169105529785\n",
      "Training log: 14 epoch (9024 / 60000 train. data). Loss: 1.4330495595932007\n",
      "Training log: 14 epoch (9664 / 60000 train. data). Loss: 1.389717936515808\n",
      "Training log: 14 epoch (10304 / 60000 train. data). Loss: 1.6955528259277344\n",
      "Training log: 14 epoch (10944 / 60000 train. data). Loss: 1.3554503917694092\n",
      "Training log: 14 epoch (11584 / 60000 train. data). Loss: 1.18435537815094\n",
      "Training log: 14 epoch (12224 / 60000 train. data). Loss: 1.2758172750473022\n",
      "Training log: 14 epoch (12864 / 60000 train. data). Loss: 1.4891282320022583\n",
      "Training log: 14 epoch (13504 / 60000 train. data). Loss: 1.4371520280838013\n",
      "Training log: 14 epoch (14144 / 60000 train. data). Loss: 1.4996408224105835\n",
      "Training log: 14 epoch (14784 / 60000 train. data). Loss: 1.233262538909912\n",
      "Training log: 14 epoch (15424 / 60000 train. data). Loss: 1.372578740119934\n",
      "Training log: 14 epoch (16064 / 60000 train. data). Loss: 1.3707009553909302\n",
      "Training log: 14 epoch (16704 / 60000 train. data). Loss: 1.343430519104004\n",
      "Training log: 14 epoch (17344 / 60000 train. data). Loss: 1.2206754684448242\n",
      "Training log: 14 epoch (17984 / 60000 train. data). Loss: 1.3185992240905762\n",
      "Training log: 14 epoch (18624 / 60000 train. data). Loss: 1.621139645576477\n",
      "Training log: 14 epoch (19264 / 60000 train. data). Loss: 1.3212734460830688\n",
      "Training log: 14 epoch (19904 / 60000 train. data). Loss: 1.2393008470535278\n",
      "Training log: 14 epoch (20544 / 60000 train. data). Loss: 1.3336944580078125\n",
      "Training log: 14 epoch (21184 / 60000 train. data). Loss: 1.2892693281173706\n",
      "Training log: 14 epoch (21824 / 60000 train. data). Loss: 1.6822090148925781\n",
      "Training log: 14 epoch (22464 / 60000 train. data). Loss: 1.393855333328247\n",
      "Training log: 14 epoch (23104 / 60000 train. data). Loss: 1.4528100490570068\n",
      "Training log: 14 epoch (23744 / 60000 train. data). Loss: 1.4422370195388794\n",
      "Training log: 14 epoch (24384 / 60000 train. data). Loss: 1.3153603076934814\n",
      "Training log: 14 epoch (25024 / 60000 train. data). Loss: 1.2395700216293335\n",
      "Training log: 14 epoch (25664 / 60000 train. data). Loss: 1.3404277563095093\n",
      "Training log: 14 epoch (26304 / 60000 train. data). Loss: 1.4798870086669922\n",
      "Training log: 14 epoch (26944 / 60000 train. data). Loss: 1.4855421781539917\n",
      "Training log: 14 epoch (27584 / 60000 train. data). Loss: 1.5238118171691895\n",
      "Training log: 14 epoch (28224 / 60000 train. data). Loss: 1.354782223701477\n",
      "Training log: 14 epoch (28864 / 60000 train. data). Loss: 1.4350128173828125\n",
      "Training log: 14 epoch (29504 / 60000 train. data). Loss: 1.327679991722107\n",
      "Training log: 14 epoch (30144 / 60000 train. data). Loss: 1.5775920152664185\n",
      "Training log: 14 epoch (30784 / 60000 train. data). Loss: 1.58799409866333\n",
      "Training log: 14 epoch (31424 / 60000 train. data). Loss: 1.3962687253952026\n",
      "Training log: 14 epoch (32064 / 60000 train. data). Loss: 1.6319658756256104\n",
      "Training log: 14 epoch (32704 / 60000 train. data). Loss: 1.4544585943222046\n",
      "Training log: 14 epoch (33344 / 60000 train. data). Loss: 1.5855761766433716\n",
      "Training log: 14 epoch (33984 / 60000 train. data). Loss: 1.4089255332946777\n",
      "Training log: 14 epoch (34624 / 60000 train. data). Loss: 1.378605842590332\n",
      "Training log: 14 epoch (35264 / 60000 train. data). Loss: 1.5170791149139404\n",
      "Training log: 14 epoch (35904 / 60000 train. data). Loss: 1.470933437347412\n",
      "Training log: 14 epoch (36544 / 60000 train. data). Loss: 1.3275104761123657\n",
      "Training log: 14 epoch (37184 / 60000 train. data). Loss: 1.2932926416397095\n",
      "Training log: 14 epoch (37824 / 60000 train. data). Loss: 1.2760205268859863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 14 epoch (38464 / 60000 train. data). Loss: 1.4578726291656494\n",
      "Training log: 14 epoch (39104 / 60000 train. data). Loss: 1.3553924560546875\n",
      "Training log: 14 epoch (39744 / 60000 train. data). Loss: 1.6668078899383545\n",
      "Training log: 14 epoch (40384 / 60000 train. data). Loss: 1.5615439414978027\n",
      "Training log: 14 epoch (41024 / 60000 train. data). Loss: 1.2956291437149048\n",
      "Training log: 14 epoch (41664 / 60000 train. data). Loss: 1.451356053352356\n",
      "Training log: 14 epoch (42304 / 60000 train. data). Loss: 1.3776627779006958\n",
      "Training log: 14 epoch (42944 / 60000 train. data). Loss: 1.6569234132766724\n",
      "Training log: 14 epoch (43584 / 60000 train. data). Loss: 1.375567078590393\n",
      "Training log: 14 epoch (44224 / 60000 train. data). Loss: 1.2444674968719482\n",
      "Training log: 14 epoch (44864 / 60000 train. data). Loss: 1.390974521636963\n",
      "Training log: 14 epoch (45504 / 60000 train. data). Loss: 1.5835117101669312\n",
      "Training log: 14 epoch (46144 / 60000 train. data). Loss: 1.284099817276001\n",
      "Training log: 14 epoch (46784 / 60000 train. data). Loss: 1.6337507963180542\n",
      "Training log: 14 epoch (47424 / 60000 train. data). Loss: 1.4285011291503906\n",
      "Training log: 14 epoch (48064 / 60000 train. data). Loss: 1.4035141468048096\n",
      "Training log: 14 epoch (48704 / 60000 train. data). Loss: 1.375786542892456\n",
      "Training log: 14 epoch (49344 / 60000 train. data). Loss: 1.4616676568984985\n",
      "Training log: 14 epoch (49984 / 60000 train. data). Loss: 1.4172497987747192\n",
      "Test loss (avg): 1.566954943784483, Accuracy: 0.461\n",
      "Training log: 15 epoch (64 / 60000 train. data). Loss: 1.2688997983932495\n",
      "Training log: 15 epoch (704 / 60000 train. data). Loss: 1.372484564781189\n",
      "Training log: 15 epoch (1344 / 60000 train. data). Loss: 1.3511327505111694\n",
      "Training log: 15 epoch (1984 / 60000 train. data). Loss: 1.1738638877868652\n",
      "Training log: 15 epoch (2624 / 60000 train. data). Loss: 1.2346088886260986\n",
      "Training log: 15 epoch (3264 / 60000 train. data). Loss: 1.1534245014190674\n",
      "Training log: 15 epoch (3904 / 60000 train. data). Loss: 1.3070002794265747\n",
      "Training log: 15 epoch (4544 / 60000 train. data). Loss: 1.4283725023269653\n",
      "Training log: 15 epoch (5184 / 60000 train. data). Loss: 1.0762344598770142\n",
      "Training log: 15 epoch (5824 / 60000 train. data). Loss: 1.4185551404953003\n",
      "Training log: 15 epoch (6464 / 60000 train. data). Loss: 1.2031432390213013\n",
      "Training log: 15 epoch (7104 / 60000 train. data). Loss: 1.2733900547027588\n",
      "Training log: 15 epoch (7744 / 60000 train. data). Loss: 1.2638733386993408\n",
      "Training log: 15 epoch (8384 / 60000 train. data). Loss: 1.2287806272506714\n",
      "Training log: 15 epoch (9024 / 60000 train. data). Loss: 1.2622567415237427\n",
      "Training log: 15 epoch (9664 / 60000 train. data). Loss: 1.239650845527649\n",
      "Training log: 15 epoch (10304 / 60000 train. data). Loss: 1.1117833852767944\n",
      "Training log: 15 epoch (10944 / 60000 train. data). Loss: 1.3443387746810913\n",
      "Training log: 15 epoch (11584 / 60000 train. data). Loss: 1.1841953992843628\n",
      "Training log: 15 epoch (12224 / 60000 train. data). Loss: 1.2945716381072998\n",
      "Training log: 15 epoch (12864 / 60000 train. data). Loss: 1.4298608303070068\n",
      "Training log: 15 epoch (13504 / 60000 train. data). Loss: 1.3484609127044678\n",
      "Training log: 15 epoch (14144 / 60000 train. data). Loss: 1.5850391387939453\n",
      "Training log: 15 epoch (14784 / 60000 train. data). Loss: 1.3294233083724976\n",
      "Training log: 15 epoch (15424 / 60000 train. data). Loss: 1.3069790601730347\n",
      "Training log: 15 epoch (16064 / 60000 train. data). Loss: 1.366888403892517\n",
      "Training log: 15 epoch (16704 / 60000 train. data). Loss: 1.3614020347595215\n",
      "Training log: 15 epoch (17344 / 60000 train. data). Loss: 1.2650303840637207\n",
      "Training log: 15 epoch (17984 / 60000 train. data). Loss: 1.164959192276001\n",
      "Training log: 15 epoch (18624 / 60000 train. data). Loss: 1.3260334730148315\n",
      "Training log: 15 epoch (19264 / 60000 train. data). Loss: 1.2545303106307983\n",
      "Training log: 15 epoch (19904 / 60000 train. data). Loss: 1.6197295188903809\n",
      "Training log: 15 epoch (20544 / 60000 train. data). Loss: 1.3328121900558472\n",
      "Training log: 15 epoch (21184 / 60000 train. data). Loss: 1.3614763021469116\n",
      "Training log: 15 epoch (21824 / 60000 train. data). Loss: 1.185183048248291\n",
      "Training log: 15 epoch (22464 / 60000 train. data). Loss: 1.3157776594161987\n",
      "Training log: 15 epoch (23104 / 60000 train. data). Loss: 1.4776315689086914\n",
      "Training log: 15 epoch (23744 / 60000 train. data). Loss: 1.518912672996521\n",
      "Training log: 15 epoch (24384 / 60000 train. data). Loss: 1.362386703491211\n",
      "Training log: 15 epoch (25024 / 60000 train. data). Loss: 1.5994495153427124\n",
      "Training log: 15 epoch (25664 / 60000 train. data). Loss: 1.5462321043014526\n",
      "Training log: 15 epoch (26304 / 60000 train. data). Loss: 1.220665693283081\n",
      "Training log: 15 epoch (26944 / 60000 train. data). Loss: 1.4172767400741577\n",
      "Training log: 15 epoch (27584 / 60000 train. data). Loss: 1.3161052465438843\n",
      "Training log: 15 epoch (28224 / 60000 train. data). Loss: 1.515236496925354\n",
      "Training log: 15 epoch (28864 / 60000 train. data). Loss: 1.4423022270202637\n",
      "Training log: 15 epoch (29504 / 60000 train. data). Loss: 1.3976750373840332\n",
      "Training log: 15 epoch (30144 / 60000 train. data). Loss: 1.3560177087783813\n",
      "Training log: 15 epoch (30784 / 60000 train. data). Loss: 1.4774601459503174\n",
      "Training log: 15 epoch (31424 / 60000 train. data). Loss: 1.2667726278305054\n",
      "Training log: 15 epoch (32064 / 60000 train. data). Loss: 1.598828911781311\n",
      "Training log: 15 epoch (32704 / 60000 train. data). Loss: 1.424034595489502\n",
      "Training log: 15 epoch (33344 / 60000 train. data). Loss: 1.133217453956604\n",
      "Training log: 15 epoch (33984 / 60000 train. data). Loss: 1.476265788078308\n",
      "Training log: 15 epoch (34624 / 60000 train. data). Loss: 1.3661776781082153\n",
      "Training log: 15 epoch (35264 / 60000 train. data). Loss: 1.4485058784484863\n",
      "Training log: 15 epoch (35904 / 60000 train. data). Loss: 1.583414077758789\n",
      "Training log: 15 epoch (36544 / 60000 train. data). Loss: 1.4267560243606567\n",
      "Training log: 15 epoch (37184 / 60000 train. data). Loss: 1.6249561309814453\n",
      "Training log: 15 epoch (37824 / 60000 train. data). Loss: 1.5003658533096313\n",
      "Training log: 15 epoch (38464 / 60000 train. data). Loss: 1.3380897045135498\n",
      "Training log: 15 epoch (39104 / 60000 train. data). Loss: 1.6083875894546509\n",
      "Training log: 15 epoch (39744 / 60000 train. data). Loss: 1.3313229084014893\n",
      "Training log: 15 epoch (40384 / 60000 train. data). Loss: 1.4154123067855835\n",
      "Training log: 15 epoch (41024 / 60000 train. data). Loss: 1.4337762594223022\n",
      "Training log: 15 epoch (41664 / 60000 train. data). Loss: 1.2165194749832153\n",
      "Training log: 15 epoch (42304 / 60000 train. data). Loss: 1.544015645980835\n",
      "Training log: 15 epoch (42944 / 60000 train. data). Loss: 1.4891775846481323\n",
      "Training log: 15 epoch (43584 / 60000 train. data). Loss: 1.484445571899414\n",
      "Training log: 15 epoch (44224 / 60000 train. data). Loss: 1.3558841943740845\n",
      "Training log: 15 epoch (44864 / 60000 train. data). Loss: 1.4999542236328125\n",
      "Training log: 15 epoch (45504 / 60000 train. data). Loss: 1.2657445669174194\n",
      "Training log: 15 epoch (46144 / 60000 train. data). Loss: 1.298803448677063\n",
      "Training log: 15 epoch (46784 / 60000 train. data). Loss: 1.4531711339950562\n",
      "Training log: 15 epoch (47424 / 60000 train. data). Loss: 1.2101200819015503\n",
      "Training log: 15 epoch (48064 / 60000 train. data). Loss: 1.504791498184204\n",
      "Training log: 15 epoch (48704 / 60000 train. data). Loss: 1.1045289039611816\n",
      "Training log: 15 epoch (49344 / 60000 train. data). Loss: 1.3784101009368896\n",
      "Training log: 15 epoch (49984 / 60000 train. data). Loss: 1.2362680435180664\n",
      "Test loss (avg): 1.5181242295890858, Accuracy: 0.4734\n",
      "Training log: 16 epoch (64 / 60000 train. data). Loss: 1.2828209400177002\n",
      "Training log: 16 epoch (704 / 60000 train. data). Loss: 1.4840342998504639\n",
      "Training log: 16 epoch (1344 / 60000 train. data). Loss: 1.1820776462554932\n",
      "Training log: 16 epoch (1984 / 60000 train. data). Loss: 1.3615909814834595\n",
      "Training log: 16 epoch (2624 / 60000 train. data). Loss: 1.296019196510315\n",
      "Training log: 16 epoch (3264 / 60000 train. data). Loss: 1.4622122049331665\n",
      "Training log: 16 epoch (3904 / 60000 train. data). Loss: 1.1351144313812256\n",
      "Training log: 16 epoch (4544 / 60000 train. data). Loss: 1.4823918342590332\n",
      "Training log: 16 epoch (5184 / 60000 train. data). Loss: 1.3529492616653442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 16 epoch (5824 / 60000 train. data). Loss: 1.2138855457305908\n",
      "Training log: 16 epoch (6464 / 60000 train. data). Loss: 1.310174822807312\n",
      "Training log: 16 epoch (7104 / 60000 train. data). Loss: 1.1059696674346924\n",
      "Training log: 16 epoch (7744 / 60000 train. data). Loss: 1.2804168462753296\n",
      "Training log: 16 epoch (8384 / 60000 train. data). Loss: 1.5858237743377686\n",
      "Training log: 16 epoch (9024 / 60000 train. data). Loss: 1.302053451538086\n",
      "Training log: 16 epoch (9664 / 60000 train. data). Loss: 1.4017529487609863\n",
      "Training log: 16 epoch (10304 / 60000 train. data). Loss: 1.1831177473068237\n",
      "Training log: 16 epoch (10944 / 60000 train. data). Loss: 1.2163852453231812\n",
      "Training log: 16 epoch (11584 / 60000 train. data). Loss: 1.3761608600616455\n",
      "Training log: 16 epoch (12224 / 60000 train. data). Loss: 1.5701524019241333\n",
      "Training log: 16 epoch (12864 / 60000 train. data). Loss: 1.047572374343872\n",
      "Training log: 16 epoch (13504 / 60000 train. data). Loss: 1.269863247871399\n",
      "Training log: 16 epoch (14144 / 60000 train. data). Loss: 1.142717957496643\n",
      "Training log: 16 epoch (14784 / 60000 train. data). Loss: 1.326075553894043\n",
      "Training log: 16 epoch (15424 / 60000 train. data). Loss: 1.4056174755096436\n",
      "Training log: 16 epoch (16064 / 60000 train. data). Loss: 1.542525291442871\n",
      "Training log: 16 epoch (16704 / 60000 train. data). Loss: 1.388084888458252\n",
      "Training log: 16 epoch (17344 / 60000 train. data). Loss: 1.3898996114730835\n",
      "Training log: 16 epoch (17984 / 60000 train. data). Loss: 1.1572750806808472\n",
      "Training log: 16 epoch (18624 / 60000 train. data). Loss: 1.1692827939987183\n",
      "Training log: 16 epoch (19264 / 60000 train. data). Loss: 1.0679835081100464\n",
      "Training log: 16 epoch (19904 / 60000 train. data). Loss: 1.1705518960952759\n",
      "Training log: 16 epoch (20544 / 60000 train. data). Loss: 1.323378324508667\n",
      "Training log: 16 epoch (21184 / 60000 train. data). Loss: 1.31486177444458\n",
      "Training log: 16 epoch (21824 / 60000 train. data). Loss: 1.5180914402008057\n",
      "Training log: 16 epoch (22464 / 60000 train. data). Loss: 1.1340157985687256\n",
      "Training log: 16 epoch (23104 / 60000 train. data). Loss: 1.5561710596084595\n",
      "Training log: 16 epoch (23744 / 60000 train. data). Loss: 1.7453577518463135\n",
      "Training log: 16 epoch (24384 / 60000 train. data). Loss: 1.6261824369430542\n",
      "Training log: 16 epoch (25024 / 60000 train. data). Loss: 1.1662465333938599\n",
      "Training log: 16 epoch (25664 / 60000 train. data). Loss: 1.2217823266983032\n",
      "Training log: 16 epoch (26304 / 60000 train. data). Loss: 1.360503911972046\n",
      "Training log: 16 epoch (26944 / 60000 train. data). Loss: 1.097886323928833\n",
      "Training log: 16 epoch (27584 / 60000 train. data). Loss: 1.5167251825332642\n",
      "Training log: 16 epoch (28224 / 60000 train. data). Loss: 1.1220389604568481\n",
      "Training log: 16 epoch (28864 / 60000 train. data). Loss: 1.2270524501800537\n",
      "Training log: 16 epoch (29504 / 60000 train. data). Loss: 1.3216458559036255\n",
      "Training log: 16 epoch (30144 / 60000 train. data). Loss: 1.4082701206207275\n",
      "Training log: 16 epoch (30784 / 60000 train. data). Loss: 1.2200877666473389\n",
      "Training log: 16 epoch (31424 / 60000 train. data). Loss: 1.5300902128219604\n",
      "Training log: 16 epoch (32064 / 60000 train. data). Loss: 1.156660556793213\n",
      "Training log: 16 epoch (32704 / 60000 train. data). Loss: 1.428520917892456\n",
      "Training log: 16 epoch (33344 / 60000 train. data). Loss: 1.267703890800476\n",
      "Training log: 16 epoch (33984 / 60000 train. data). Loss: 1.278806209564209\n",
      "Training log: 16 epoch (34624 / 60000 train. data). Loss: 1.3371182680130005\n",
      "Training log: 16 epoch (35264 / 60000 train. data). Loss: 1.4821538925170898\n",
      "Training log: 16 epoch (35904 / 60000 train. data). Loss: 1.5564417839050293\n",
      "Training log: 16 epoch (36544 / 60000 train. data). Loss: 1.0902098417282104\n",
      "Training log: 16 epoch (37184 / 60000 train. data). Loss: 1.2421274185180664\n",
      "Training log: 16 epoch (37824 / 60000 train. data). Loss: 1.5916550159454346\n",
      "Training log: 16 epoch (38464 / 60000 train. data). Loss: 1.2184878587722778\n",
      "Training log: 16 epoch (39104 / 60000 train. data). Loss: 1.5058379173278809\n",
      "Training log: 16 epoch (39744 / 60000 train. data). Loss: 1.4006116390228271\n",
      "Training log: 16 epoch (40384 / 60000 train. data). Loss: 1.3521276712417603\n",
      "Training log: 16 epoch (41024 / 60000 train. data). Loss: 1.4366670846939087\n",
      "Training log: 16 epoch (41664 / 60000 train. data). Loss: 1.1793179512023926\n",
      "Training log: 16 epoch (42304 / 60000 train. data). Loss: 1.2919455766677856\n",
      "Training log: 16 epoch (42944 / 60000 train. data). Loss: 1.6848483085632324\n",
      "Training log: 16 epoch (43584 / 60000 train. data). Loss: 1.185687780380249\n",
      "Training log: 16 epoch (44224 / 60000 train. data). Loss: 1.2593950033187866\n",
      "Training log: 16 epoch (44864 / 60000 train. data). Loss: 1.4199854135513306\n",
      "Training log: 16 epoch (45504 / 60000 train. data). Loss: 1.602860927581787\n",
      "Training log: 16 epoch (46144 / 60000 train. data). Loss: 1.35423743724823\n",
      "Training log: 16 epoch (46784 / 60000 train. data). Loss: 1.406211495399475\n",
      "Training log: 16 epoch (47424 / 60000 train. data). Loss: 1.3714921474456787\n",
      "Training log: 16 epoch (48064 / 60000 train. data). Loss: 1.4291579723358154\n",
      "Training log: 16 epoch (48704 / 60000 train. data). Loss: 1.3472340106964111\n",
      "Training log: 16 epoch (49344 / 60000 train. data). Loss: 1.3228226900100708\n",
      "Training log: 16 epoch (49984 / 60000 train. data). Loss: 1.307668685913086\n",
      "Test loss (avg): 1.4817003026889388, Accuracy: 0.4896\n",
      "Training log: 17 epoch (64 / 60000 train. data). Loss: 1.3547693490982056\n",
      "Training log: 17 epoch (704 / 60000 train. data). Loss: 1.501886248588562\n",
      "Training log: 17 epoch (1344 / 60000 train. data). Loss: 1.1531627178192139\n",
      "Training log: 17 epoch (1984 / 60000 train. data). Loss: 1.5702781677246094\n",
      "Training log: 17 epoch (2624 / 60000 train. data). Loss: 1.0610275268554688\n",
      "Training log: 17 epoch (3264 / 60000 train. data). Loss: 1.3816534280776978\n",
      "Training log: 17 epoch (3904 / 60000 train. data). Loss: 1.652905821800232\n",
      "Training log: 17 epoch (4544 / 60000 train. data). Loss: 1.3204052448272705\n",
      "Training log: 17 epoch (5184 / 60000 train. data). Loss: 1.2254399061203003\n",
      "Training log: 17 epoch (5824 / 60000 train. data). Loss: 1.1837546825408936\n",
      "Training log: 17 epoch (6464 / 60000 train. data). Loss: 1.3838876485824585\n",
      "Training log: 17 epoch (7104 / 60000 train. data). Loss: 1.3574488162994385\n",
      "Training log: 17 epoch (7744 / 60000 train. data). Loss: 1.4359091520309448\n",
      "Training log: 17 epoch (8384 / 60000 train. data). Loss: 1.3484485149383545\n",
      "Training log: 17 epoch (9024 / 60000 train. data). Loss: 1.5253325700759888\n",
      "Training log: 17 epoch (9664 / 60000 train. data). Loss: 1.6076587438583374\n",
      "Training log: 17 epoch (10304 / 60000 train. data). Loss: 1.3306599855422974\n",
      "Training log: 17 epoch (10944 / 60000 train. data). Loss: 1.3472989797592163\n",
      "Training log: 17 epoch (11584 / 60000 train. data). Loss: 1.0494725704193115\n",
      "Training log: 17 epoch (12224 / 60000 train. data). Loss: 1.3090736865997314\n",
      "Training log: 17 epoch (12864 / 60000 train. data). Loss: 1.2514770030975342\n",
      "Training log: 17 epoch (13504 / 60000 train. data). Loss: 1.459721565246582\n",
      "Training log: 17 epoch (14144 / 60000 train. data). Loss: 1.278016209602356\n",
      "Training log: 17 epoch (14784 / 60000 train. data). Loss: 1.0712205171585083\n",
      "Training log: 17 epoch (15424 / 60000 train. data). Loss: 1.112204909324646\n",
      "Training log: 17 epoch (16064 / 60000 train. data). Loss: 1.525675892829895\n",
      "Training log: 17 epoch (16704 / 60000 train. data). Loss: 1.4272160530090332\n",
      "Training log: 17 epoch (17344 / 60000 train. data). Loss: 1.226646065711975\n",
      "Training log: 17 epoch (17984 / 60000 train. data). Loss: 1.4003336429595947\n",
      "Training log: 17 epoch (18624 / 60000 train. data). Loss: 1.4833065271377563\n",
      "Training log: 17 epoch (19264 / 60000 train. data). Loss: 1.313883900642395\n",
      "Training log: 17 epoch (19904 / 60000 train. data). Loss: 1.3194513320922852\n",
      "Training log: 17 epoch (20544 / 60000 train. data). Loss: 1.2953461408615112\n",
      "Training log: 17 epoch (21184 / 60000 train. data). Loss: 1.3038921356201172\n",
      "Training log: 17 epoch (21824 / 60000 train. data). Loss: 1.2801345586776733\n",
      "Training log: 17 epoch (22464 / 60000 train. data). Loss: 1.357075572013855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 17 epoch (23104 / 60000 train. data). Loss: 1.2679563760757446\n",
      "Training log: 17 epoch (23744 / 60000 train. data). Loss: 1.2760531902313232\n",
      "Training log: 17 epoch (24384 / 60000 train. data). Loss: 1.4256821870803833\n",
      "Training log: 17 epoch (25024 / 60000 train. data). Loss: 1.3447856903076172\n",
      "Training log: 17 epoch (25664 / 60000 train. data). Loss: 1.3406720161437988\n",
      "Training log: 17 epoch (26304 / 60000 train. data). Loss: 1.6474888324737549\n",
      "Training log: 17 epoch (26944 / 60000 train. data). Loss: 1.2300524711608887\n",
      "Training log: 17 epoch (27584 / 60000 train. data). Loss: 1.3244091272354126\n",
      "Training log: 17 epoch (28224 / 60000 train. data). Loss: 1.170741081237793\n",
      "Training log: 17 epoch (28864 / 60000 train. data). Loss: 1.4002536535263062\n",
      "Training log: 17 epoch (29504 / 60000 train. data). Loss: 1.208756685256958\n",
      "Training log: 17 epoch (30144 / 60000 train. data). Loss: 1.3632540702819824\n",
      "Training log: 17 epoch (30784 / 60000 train. data). Loss: 1.4622819423675537\n",
      "Training log: 17 epoch (31424 / 60000 train. data). Loss: 1.2966530323028564\n",
      "Training log: 17 epoch (32064 / 60000 train. data). Loss: 1.4078551530838013\n",
      "Training log: 17 epoch (32704 / 60000 train. data). Loss: 1.461681842803955\n",
      "Training log: 17 epoch (33344 / 60000 train. data). Loss: 1.0686115026474\n",
      "Training log: 17 epoch (33984 / 60000 train. data). Loss: 1.403806209564209\n",
      "Training log: 17 epoch (34624 / 60000 train. data). Loss: 1.453896403312683\n",
      "Training log: 17 epoch (35264 / 60000 train. data). Loss: 1.506626009941101\n",
      "Training log: 17 epoch (35904 / 60000 train. data). Loss: 1.2277926206588745\n",
      "Training log: 17 epoch (36544 / 60000 train. data). Loss: 1.3361165523529053\n",
      "Training log: 17 epoch (37184 / 60000 train. data). Loss: 1.2065402269363403\n",
      "Training log: 17 epoch (37824 / 60000 train. data). Loss: 1.514154314994812\n",
      "Training log: 17 epoch (38464 / 60000 train. data). Loss: 1.2971961498260498\n",
      "Training log: 17 epoch (39104 / 60000 train. data). Loss: 1.1248234510421753\n",
      "Training log: 17 epoch (39744 / 60000 train. data). Loss: 1.1922837495803833\n",
      "Training log: 17 epoch (40384 / 60000 train. data). Loss: 1.3591338396072388\n",
      "Training log: 17 epoch (41024 / 60000 train. data). Loss: 1.313723087310791\n",
      "Training log: 17 epoch (41664 / 60000 train. data). Loss: 1.0870367288589478\n",
      "Training log: 17 epoch (42304 / 60000 train. data). Loss: 1.406847357749939\n",
      "Training log: 17 epoch (42944 / 60000 train. data). Loss: 1.4876810312271118\n",
      "Training log: 17 epoch (43584 / 60000 train. data). Loss: 1.2715293169021606\n",
      "Training log: 17 epoch (44224 / 60000 train. data). Loss: 1.0868327617645264\n",
      "Training log: 17 epoch (44864 / 60000 train. data). Loss: 1.4241985082626343\n",
      "Training log: 17 epoch (45504 / 60000 train. data). Loss: 1.3426967859268188\n",
      "Training log: 17 epoch (46144 / 60000 train. data). Loss: 1.2897748947143555\n",
      "Training log: 17 epoch (46784 / 60000 train. data). Loss: 1.3363823890686035\n",
      "Training log: 17 epoch (47424 / 60000 train. data). Loss: 1.4042284488677979\n",
      "Training log: 17 epoch (48064 / 60000 train. data). Loss: 1.351813554763794\n",
      "Training log: 17 epoch (48704 / 60000 train. data). Loss: 1.0671188831329346\n",
      "Training log: 17 epoch (49344 / 60000 train. data). Loss: 1.465389370918274\n",
      "Training log: 17 epoch (49984 / 60000 train. data). Loss: 1.2910354137420654\n",
      "Test loss (avg): 1.5018098946589573, Accuracy: 0.4823\n",
      "Training log: 18 epoch (64 / 60000 train. data). Loss: 1.1684837341308594\n",
      "Training log: 18 epoch (704 / 60000 train. data). Loss: 1.1081795692443848\n",
      "Training log: 18 epoch (1344 / 60000 train. data). Loss: 1.4169986248016357\n",
      "Training log: 18 epoch (1984 / 60000 train. data). Loss: 1.2014890909194946\n",
      "Training log: 18 epoch (2624 / 60000 train. data). Loss: 1.2105063199996948\n",
      "Training log: 18 epoch (3264 / 60000 train. data). Loss: 1.5048667192459106\n",
      "Training log: 18 epoch (3904 / 60000 train. data). Loss: 1.370902419090271\n",
      "Training log: 18 epoch (4544 / 60000 train. data). Loss: 1.1016178131103516\n",
      "Training log: 18 epoch (5184 / 60000 train. data). Loss: 1.4108573198318481\n",
      "Training log: 18 epoch (5824 / 60000 train. data). Loss: 1.3518379926681519\n",
      "Training log: 18 epoch (6464 / 60000 train. data). Loss: 1.0888466835021973\n",
      "Training log: 18 epoch (7104 / 60000 train. data). Loss: 1.2724155187606812\n",
      "Training log: 18 epoch (7744 / 60000 train. data). Loss: 1.109514832496643\n",
      "Training log: 18 epoch (8384 / 60000 train. data). Loss: 1.4690860509872437\n",
      "Training log: 18 epoch (9024 / 60000 train. data). Loss: 1.2804348468780518\n",
      "Training log: 18 epoch (9664 / 60000 train. data). Loss: 1.3115583658218384\n",
      "Training log: 18 epoch (10304 / 60000 train. data). Loss: 1.2368463277816772\n",
      "Training log: 18 epoch (10944 / 60000 train. data). Loss: 1.0914993286132812\n",
      "Training log: 18 epoch (11584 / 60000 train. data). Loss: 1.1302073001861572\n",
      "Training log: 18 epoch (12224 / 60000 train. data). Loss: 1.5280431509017944\n",
      "Training log: 18 epoch (12864 / 60000 train. data). Loss: 1.3332035541534424\n",
      "Training log: 18 epoch (13504 / 60000 train. data). Loss: 1.2568598985671997\n",
      "Training log: 18 epoch (14144 / 60000 train. data). Loss: 1.101624608039856\n",
      "Training log: 18 epoch (14784 / 60000 train. data). Loss: 1.5745633840560913\n",
      "Training log: 18 epoch (15424 / 60000 train. data). Loss: 1.3097167015075684\n",
      "Training log: 18 epoch (16064 / 60000 train. data). Loss: 1.3277589082717896\n",
      "Training log: 18 epoch (16704 / 60000 train. data). Loss: 1.1662321090698242\n",
      "Training log: 18 epoch (17344 / 60000 train. data). Loss: 1.3643571138381958\n",
      "Training log: 18 epoch (17984 / 60000 train. data). Loss: 1.521401047706604\n",
      "Training log: 18 epoch (18624 / 60000 train. data). Loss: 1.192206859588623\n",
      "Training log: 18 epoch (19264 / 60000 train. data). Loss: 1.5114306211471558\n",
      "Training log: 18 epoch (19904 / 60000 train. data). Loss: 1.1611055135726929\n",
      "Training log: 18 epoch (20544 / 60000 train. data). Loss: 1.478459119796753\n",
      "Training log: 18 epoch (21184 / 60000 train. data). Loss: 1.3189057111740112\n",
      "Training log: 18 epoch (21824 / 60000 train. data). Loss: 1.5494325160980225\n",
      "Training log: 18 epoch (22464 / 60000 train. data). Loss: 1.4652694463729858\n",
      "Training log: 18 epoch (23104 / 60000 train. data). Loss: 1.2389321327209473\n",
      "Training log: 18 epoch (23744 / 60000 train. data). Loss: 1.457202434539795\n",
      "Training log: 18 epoch (24384 / 60000 train. data). Loss: 1.4635426998138428\n",
      "Training log: 18 epoch (25024 / 60000 train. data). Loss: 1.3561522960662842\n",
      "Training log: 18 epoch (25664 / 60000 train. data). Loss: 0.9927689433097839\n",
      "Training log: 18 epoch (26304 / 60000 train. data). Loss: 1.1468853950500488\n",
      "Training log: 18 epoch (26944 / 60000 train. data). Loss: 1.4137543439865112\n",
      "Training log: 18 epoch (27584 / 60000 train. data). Loss: 1.294163703918457\n",
      "Training log: 18 epoch (28224 / 60000 train. data). Loss: 1.3204154968261719\n",
      "Training log: 18 epoch (28864 / 60000 train. data). Loss: 1.4911446571350098\n",
      "Training log: 18 epoch (29504 / 60000 train. data). Loss: 1.3386640548706055\n",
      "Training log: 18 epoch (30144 / 60000 train. data). Loss: 1.2436347007751465\n",
      "Training log: 18 epoch (30784 / 60000 train. data). Loss: 1.1349565982818604\n",
      "Training log: 18 epoch (31424 / 60000 train. data). Loss: 1.3986473083496094\n",
      "Training log: 18 epoch (32064 / 60000 train. data). Loss: 1.356398582458496\n",
      "Training log: 18 epoch (32704 / 60000 train. data). Loss: 1.1163150072097778\n",
      "Training log: 18 epoch (33344 / 60000 train. data). Loss: 1.3364324569702148\n",
      "Training log: 18 epoch (33984 / 60000 train. data). Loss: 1.319429874420166\n",
      "Training log: 18 epoch (34624 / 60000 train. data). Loss: 1.2921245098114014\n",
      "Training log: 18 epoch (35264 / 60000 train. data). Loss: 1.45940363407135\n",
      "Training log: 18 epoch (35904 / 60000 train. data). Loss: 1.2756457328796387\n",
      "Training log: 18 epoch (36544 / 60000 train. data). Loss: 1.1912381649017334\n",
      "Training log: 18 epoch (37184 / 60000 train. data). Loss: 1.2921382188796997\n",
      "Training log: 18 epoch (37824 / 60000 train. data). Loss: 1.2450956106185913\n",
      "Training log: 18 epoch (38464 / 60000 train. data). Loss: 1.3080437183380127\n",
      "Training log: 18 epoch (39104 / 60000 train. data). Loss: 1.4119724035263062\n",
      "Training log: 18 epoch (39744 / 60000 train. data). Loss: 1.2636473178863525\n",
      "Training log: 18 epoch (40384 / 60000 train. data). Loss: 1.367902159690857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 18 epoch (41024 / 60000 train. data). Loss: 1.3224022388458252\n",
      "Training log: 18 epoch (41664 / 60000 train. data). Loss: 1.3017326593399048\n",
      "Training log: 18 epoch (42304 / 60000 train. data). Loss: 1.2670273780822754\n",
      "Training log: 18 epoch (42944 / 60000 train. data). Loss: 1.1980571746826172\n",
      "Training log: 18 epoch (43584 / 60000 train. data). Loss: 1.5463451147079468\n",
      "Training log: 18 epoch (44224 / 60000 train. data). Loss: 1.1417748928070068\n",
      "Training log: 18 epoch (44864 / 60000 train. data). Loss: 1.3168379068374634\n",
      "Training log: 18 epoch (45504 / 60000 train. data). Loss: 1.53428316116333\n",
      "Training log: 18 epoch (46144 / 60000 train. data). Loss: 1.090967059135437\n",
      "Training log: 18 epoch (46784 / 60000 train. data). Loss: 1.406651496887207\n",
      "Training log: 18 epoch (47424 / 60000 train. data). Loss: 1.3354530334472656\n",
      "Training log: 18 epoch (48064 / 60000 train. data). Loss: 1.5125395059585571\n",
      "Training log: 18 epoch (48704 / 60000 train. data). Loss: 1.1911253929138184\n",
      "Training log: 18 epoch (49344 / 60000 train. data). Loss: 1.5839285850524902\n",
      "Training log: 18 epoch (49984 / 60000 train. data). Loss: 1.138804316520691\n",
      "Test loss (avg): 1.516284231926985, Accuracy: 0.4744\n",
      "Training log: 19 epoch (64 / 60000 train. data). Loss: 1.2449837923049927\n",
      "Training log: 19 epoch (704 / 60000 train. data). Loss: 1.0816982984542847\n",
      "Training log: 19 epoch (1344 / 60000 train. data). Loss: 1.089954137802124\n",
      "Training log: 19 epoch (1984 / 60000 train. data). Loss: 1.1543697118759155\n",
      "Training log: 19 epoch (2624 / 60000 train. data). Loss: 1.1099989414215088\n",
      "Training log: 19 epoch (3264 / 60000 train. data). Loss: 1.151221752166748\n",
      "Training log: 19 epoch (3904 / 60000 train. data). Loss: 1.2459166049957275\n",
      "Training log: 19 epoch (4544 / 60000 train. data). Loss: 1.2429218292236328\n",
      "Training log: 19 epoch (5184 / 60000 train. data). Loss: 1.2185909748077393\n",
      "Training log: 19 epoch (5824 / 60000 train. data). Loss: 1.2315422296524048\n",
      "Training log: 19 epoch (6464 / 60000 train. data). Loss: 1.3874285221099854\n",
      "Training log: 19 epoch (7104 / 60000 train. data). Loss: 1.2269511222839355\n",
      "Training log: 19 epoch (7744 / 60000 train. data). Loss: 1.1250779628753662\n",
      "Training log: 19 epoch (8384 / 60000 train. data). Loss: 1.2562901973724365\n",
      "Training log: 19 epoch (9024 / 60000 train. data). Loss: 1.1404212713241577\n",
      "Training log: 19 epoch (9664 / 60000 train. data). Loss: 1.1238489151000977\n",
      "Training log: 19 epoch (10304 / 60000 train. data). Loss: 1.2354652881622314\n",
      "Training log: 19 epoch (10944 / 60000 train. data). Loss: 1.5833326578140259\n",
      "Training log: 19 epoch (11584 / 60000 train. data). Loss: 1.255779504776001\n",
      "Training log: 19 epoch (12224 / 60000 train. data). Loss: 1.4633021354675293\n",
      "Training log: 19 epoch (12864 / 60000 train. data). Loss: 1.137587308883667\n",
      "Training log: 19 epoch (13504 / 60000 train. data). Loss: 1.2713806629180908\n",
      "Training log: 19 epoch (14144 / 60000 train. data). Loss: 1.2017484903335571\n",
      "Training log: 19 epoch (14784 / 60000 train. data). Loss: 1.2766865491867065\n",
      "Training log: 19 epoch (15424 / 60000 train. data). Loss: 1.4286811351776123\n",
      "Training log: 19 epoch (16064 / 60000 train. data). Loss: 1.278682827949524\n",
      "Training log: 19 epoch (16704 / 60000 train. data). Loss: 1.3095777034759521\n",
      "Training log: 19 epoch (17344 / 60000 train. data). Loss: 1.4979170560836792\n",
      "Training log: 19 epoch (17984 / 60000 train. data). Loss: 1.2420201301574707\n",
      "Training log: 19 epoch (18624 / 60000 train. data). Loss: 1.2168899774551392\n",
      "Training log: 19 epoch (19264 / 60000 train. data). Loss: 1.6923823356628418\n",
      "Training log: 19 epoch (19904 / 60000 train. data). Loss: 1.0705451965332031\n",
      "Training log: 19 epoch (20544 / 60000 train. data). Loss: 1.3297598361968994\n",
      "Training log: 19 epoch (21184 / 60000 train. data). Loss: 1.414590835571289\n",
      "Training log: 19 epoch (21824 / 60000 train. data). Loss: 1.1366347074508667\n",
      "Training log: 19 epoch (22464 / 60000 train. data). Loss: 1.2711677551269531\n",
      "Training log: 19 epoch (23104 / 60000 train. data). Loss: 1.2301996946334839\n",
      "Training log: 19 epoch (23744 / 60000 train. data). Loss: 1.3150405883789062\n",
      "Training log: 19 epoch (24384 / 60000 train. data). Loss: 1.1514047384262085\n",
      "Training log: 19 epoch (25024 / 60000 train. data). Loss: 1.2226516008377075\n",
      "Training log: 19 epoch (25664 / 60000 train. data). Loss: 1.2873518466949463\n",
      "Training log: 19 epoch (26304 / 60000 train. data). Loss: 1.3629915714263916\n",
      "Training log: 19 epoch (26944 / 60000 train. data). Loss: 1.0972379446029663\n",
      "Training log: 19 epoch (27584 / 60000 train. data). Loss: 1.2692490816116333\n",
      "Training log: 19 epoch (28224 / 60000 train. data). Loss: 1.1340855360031128\n",
      "Training log: 19 epoch (28864 / 60000 train. data). Loss: 1.4555238485336304\n",
      "Training log: 19 epoch (29504 / 60000 train. data). Loss: 1.4366910457611084\n",
      "Training log: 19 epoch (30144 / 60000 train. data). Loss: 1.1487383842468262\n",
      "Training log: 19 epoch (30784 / 60000 train. data). Loss: 1.1520863771438599\n",
      "Training log: 19 epoch (31424 / 60000 train. data). Loss: 1.2001526355743408\n",
      "Training log: 19 epoch (32064 / 60000 train. data). Loss: 1.4301822185516357\n",
      "Training log: 19 epoch (32704 / 60000 train. data). Loss: 1.38669753074646\n",
      "Training log: 19 epoch (33344 / 60000 train. data). Loss: 1.3790223598480225\n",
      "Training log: 19 epoch (33984 / 60000 train. data). Loss: 1.335166573524475\n",
      "Training log: 19 epoch (34624 / 60000 train. data). Loss: 1.0049861669540405\n",
      "Training log: 19 epoch (35264 / 60000 train. data). Loss: 1.447859525680542\n",
      "Training log: 19 epoch (35904 / 60000 train. data). Loss: 1.1976454257965088\n",
      "Training log: 19 epoch (36544 / 60000 train. data). Loss: 1.2114683389663696\n",
      "Training log: 19 epoch (37184 / 60000 train. data). Loss: 1.2395716905593872\n",
      "Training log: 19 epoch (37824 / 60000 train. data). Loss: 1.2043383121490479\n",
      "Training log: 19 epoch (38464 / 60000 train. data). Loss: 1.5196611881256104\n",
      "Training log: 19 epoch (39104 / 60000 train. data). Loss: 1.2125616073608398\n",
      "Training log: 19 epoch (39744 / 60000 train. data). Loss: 1.4435430765151978\n",
      "Training log: 19 epoch (40384 / 60000 train. data). Loss: 1.4040565490722656\n",
      "Training log: 19 epoch (41024 / 60000 train. data). Loss: 1.2333513498306274\n",
      "Training log: 19 epoch (41664 / 60000 train. data). Loss: 1.204291820526123\n",
      "Training log: 19 epoch (42304 / 60000 train. data). Loss: 1.2311683893203735\n",
      "Training log: 19 epoch (42944 / 60000 train. data). Loss: 1.4333257675170898\n",
      "Training log: 19 epoch (43584 / 60000 train. data). Loss: 1.1642699241638184\n",
      "Training log: 19 epoch (44224 / 60000 train. data). Loss: 1.28462815284729\n",
      "Training log: 19 epoch (44864 / 60000 train. data). Loss: 1.1152050495147705\n",
      "Training log: 19 epoch (45504 / 60000 train. data). Loss: 1.11827552318573\n",
      "Training log: 19 epoch (46144 / 60000 train. data). Loss: 1.09929358959198\n",
      "Training log: 19 epoch (46784 / 60000 train. data). Loss: 1.2857842445373535\n",
      "Training log: 19 epoch (47424 / 60000 train. data). Loss: 1.3244144916534424\n",
      "Training log: 19 epoch (48064 / 60000 train. data). Loss: 1.077089786529541\n",
      "Training log: 19 epoch (48704 / 60000 train. data). Loss: 1.301785945892334\n",
      "Training log: 19 epoch (49344 / 60000 train. data). Loss: 1.177724003791809\n",
      "Training log: 19 epoch (49984 / 60000 train. data). Loss: 1.2644779682159424\n",
      "Test loss (avg): 1.4796353510230968, Accuracy: 0.484\n",
      "Training log: 20 epoch (64 / 60000 train. data). Loss: 1.2734425067901611\n",
      "Training log: 20 epoch (704 / 60000 train. data). Loss: 1.262865662574768\n",
      "Training log: 20 epoch (1344 / 60000 train. data). Loss: 1.0437238216400146\n",
      "Training log: 20 epoch (1984 / 60000 train. data). Loss: 1.2956418991088867\n",
      "Training log: 20 epoch (2624 / 60000 train. data). Loss: 1.2340385913848877\n",
      "Training log: 20 epoch (3264 / 60000 train. data). Loss: 1.4583613872528076\n",
      "Training log: 20 epoch (3904 / 60000 train. data). Loss: 1.1842715740203857\n",
      "Training log: 20 epoch (4544 / 60000 train. data). Loss: 1.130666971206665\n",
      "Training log: 20 epoch (5184 / 60000 train. data). Loss: 1.182564616203308\n",
      "Training log: 20 epoch (5824 / 60000 train. data). Loss: 1.2073252201080322\n",
      "Training log: 20 epoch (6464 / 60000 train. data). Loss: 1.1731384992599487\n",
      "Training log: 20 epoch (7104 / 60000 train. data). Loss: 1.3783355951309204\n",
      "Training log: 20 epoch (7744 / 60000 train. data). Loss: 1.1625632047653198\n",
      "Training log: 20 epoch (8384 / 60000 train. data). Loss: 1.0196633338928223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 20 epoch (9024 / 60000 train. data). Loss: 1.1837414503097534\n",
      "Training log: 20 epoch (9664 / 60000 train. data). Loss: 1.2619192600250244\n",
      "Training log: 20 epoch (10304 / 60000 train. data). Loss: 1.3458259105682373\n",
      "Training log: 20 epoch (10944 / 60000 train. data). Loss: 1.2636770009994507\n",
      "Training log: 20 epoch (11584 / 60000 train. data). Loss: 1.495055079460144\n",
      "Training log: 20 epoch (12224 / 60000 train. data). Loss: 1.1803152561187744\n",
      "Training log: 20 epoch (12864 / 60000 train. data). Loss: 1.350661277770996\n",
      "Training log: 20 epoch (13504 / 60000 train. data). Loss: 1.525702953338623\n",
      "Training log: 20 epoch (14144 / 60000 train. data). Loss: 0.9943737387657166\n",
      "Training log: 20 epoch (14784 / 60000 train. data). Loss: 1.2564873695373535\n",
      "Training log: 20 epoch (15424 / 60000 train. data). Loss: 1.3450700044631958\n",
      "Training log: 20 epoch (16064 / 60000 train. data). Loss: 1.4644629955291748\n",
      "Training log: 20 epoch (16704 / 60000 train. data). Loss: 1.5931963920593262\n",
      "Training log: 20 epoch (17344 / 60000 train. data). Loss: 1.2152715921401978\n",
      "Training log: 20 epoch (17984 / 60000 train. data). Loss: 1.3841382265090942\n",
      "Training log: 20 epoch (18624 / 60000 train. data). Loss: 1.1816643476486206\n",
      "Training log: 20 epoch (19264 / 60000 train. data). Loss: 1.3348575830459595\n",
      "Training log: 20 epoch (19904 / 60000 train. data). Loss: 1.2913134098052979\n",
      "Training log: 20 epoch (20544 / 60000 train. data). Loss: 1.4392985105514526\n",
      "Training log: 20 epoch (21184 / 60000 train. data). Loss: 1.363345980644226\n",
      "Training log: 20 epoch (21824 / 60000 train. data). Loss: 1.5209431648254395\n",
      "Training log: 20 epoch (22464 / 60000 train. data). Loss: 1.276526927947998\n",
      "Training log: 20 epoch (23104 / 60000 train. data). Loss: 1.2450155019760132\n",
      "Training log: 20 epoch (23744 / 60000 train. data). Loss: 1.2994757890701294\n",
      "Training log: 20 epoch (24384 / 60000 train. data). Loss: 1.392967939376831\n",
      "Training log: 20 epoch (25024 / 60000 train. data). Loss: 1.3768773078918457\n",
      "Training log: 20 epoch (25664 / 60000 train. data). Loss: 1.4710514545440674\n",
      "Training log: 20 epoch (26304 / 60000 train. data). Loss: 1.1130026578903198\n",
      "Training log: 20 epoch (26944 / 60000 train. data). Loss: 1.1857978105545044\n",
      "Training log: 20 epoch (27584 / 60000 train. data). Loss: 1.3764222860336304\n",
      "Training log: 20 epoch (28224 / 60000 train. data). Loss: 1.1741359233856201\n",
      "Training log: 20 epoch (28864 / 60000 train. data). Loss: 1.3119361400604248\n",
      "Training log: 20 epoch (29504 / 60000 train. data). Loss: 1.0879508256912231\n",
      "Training log: 20 epoch (30144 / 60000 train. data). Loss: 1.166021704673767\n",
      "Training log: 20 epoch (30784 / 60000 train. data). Loss: 1.1646873950958252\n",
      "Training log: 20 epoch (31424 / 60000 train. data). Loss: 1.2302203178405762\n",
      "Training log: 20 epoch (32064 / 60000 train. data). Loss: 1.1561360359191895\n",
      "Training log: 20 epoch (32704 / 60000 train. data). Loss: 1.2045847177505493\n",
      "Training log: 20 epoch (33344 / 60000 train. data). Loss: 1.2789225578308105\n",
      "Training log: 20 epoch (33984 / 60000 train. data). Loss: 1.1832256317138672\n",
      "Training log: 20 epoch (34624 / 60000 train. data). Loss: 1.4421920776367188\n",
      "Training log: 20 epoch (35264 / 60000 train. data). Loss: 1.3240251541137695\n",
      "Training log: 20 epoch (35904 / 60000 train. data). Loss: 1.1288201808929443\n",
      "Training log: 20 epoch (36544 / 60000 train. data). Loss: 1.3764548301696777\n",
      "Training log: 20 epoch (37184 / 60000 train. data). Loss: 1.0995067358016968\n",
      "Training log: 20 epoch (37824 / 60000 train. data). Loss: 1.3349648714065552\n",
      "Training log: 20 epoch (38464 / 60000 train. data). Loss: 1.3315292596817017\n",
      "Training log: 20 epoch (39104 / 60000 train. data). Loss: 1.0404118299484253\n",
      "Training log: 20 epoch (39744 / 60000 train. data). Loss: 1.3036549091339111\n",
      "Training log: 20 epoch (40384 / 60000 train. data). Loss: 1.4048523902893066\n",
      "Training log: 20 epoch (41024 / 60000 train. data). Loss: 1.0542824268341064\n",
      "Training log: 20 epoch (41664 / 60000 train. data). Loss: 1.4342864751815796\n",
      "Training log: 20 epoch (42304 / 60000 train. data). Loss: 1.2596514225006104\n",
      "Training log: 20 epoch (42944 / 60000 train. data). Loss: 1.3078569173812866\n",
      "Training log: 20 epoch (43584 / 60000 train. data). Loss: 1.1932108402252197\n",
      "Training log: 20 epoch (44224 / 60000 train. data). Loss: 1.193549633026123\n",
      "Training log: 20 epoch (44864 / 60000 train. data). Loss: 1.3998726606369019\n",
      "Training log: 20 epoch (45504 / 60000 train. data). Loss: 1.2219793796539307\n",
      "Training log: 20 epoch (46144 / 60000 train. data). Loss: 1.3616012334823608\n",
      "Training log: 20 epoch (46784 / 60000 train. data). Loss: 1.2123233079910278\n",
      "Training log: 20 epoch (47424 / 60000 train. data). Loss: 1.3075854778289795\n",
      "Training log: 20 epoch (48064 / 60000 train. data). Loss: 1.3746334314346313\n",
      "Training log: 20 epoch (48704 / 60000 train. data). Loss: 1.1596778631210327\n",
      "Training log: 20 epoch (49344 / 60000 train. data). Loss: 1.2022966146469116\n",
      "Training log: 20 epoch (49984 / 60000 train. data). Loss: 1.3291484117507935\n",
      "Test loss (avg): 1.5035954493625907, Accuracy: 0.4812\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 3*32*32).to(device)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "                print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(epoch+1, (i+1)*batch_size, loss.item()))\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_acc = train_acc / len(train_loader.dataset)\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 3*32*32).to(device)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    avg_val_acc = val_acc / len(test_loader.dataset)\n",
    "    \n",
    "    print('Test loss (avg): {}, Accuracy: {}'.format(avg_val_loss, avg_val_acc))\n",
    "    \n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    train_acc_list.append(avg_train_acc)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    val_acc_list.append(avg_val_acc)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[0.3644, 0.3653, 0.3828, 0.4001, 0.3911, 0.4134, 0.4403, 0.4282, 0.4546, 0.4619, 0.4593, 0.46, 0.4595, 0.461, 0.4734, 0.4896, 0.4823, 0.4744, 0.484, 0.4812]\n",
      "[1.9186365654706345, 1.7977016208421848, 1.748704845795546, 1.7126100856020017, 1.6710413342241741, 1.630115689371553, 1.600355274994355, 1.5694480568856535, 1.5337557144787, 1.5103644271335943, 1.4773764143819395, 1.4546486227713582, 1.4234473662608116, 1.4025335049690189, 1.3782090677324768, 1.3501522852026897, 1.3240302757686362, 1.3048373933338449, 1.28334709460778, 1.2543975737546107]\n",
      "[1.8065056337672434, 1.7981510792568232, 1.8153648558695605, 1.7161058741769972, 1.7270270548049051, 1.6955419247317467, 1.6210983482895382, 1.642423692022919, 1.5768233856577782, 1.568482499213735, 1.5727734011449632, 1.572849355685483, 1.5405113871689815, 1.566954943784483, 1.5181242295890858, 1.4817003026889388, 1.5018098946589573, 1.516284231926985, 1.4796353510230968, 1.5035954493625907]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(num_epochs)\n",
    "# print(history)\n",
    "plt.figure()\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, label='train_loss')\n",
    "plt.plot(range(len(val_loss_list)), val_loss_list, label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('linear.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(val_acc_list)), val_acc_list)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('linear.png')\n",
    "print(val_acc_list)\n",
    "print(train_loss_list)\n",
    "print(val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
